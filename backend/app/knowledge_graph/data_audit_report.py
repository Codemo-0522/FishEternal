"""
çŸ¥è¯†å›¾è°±æ•°æ®ä¸€è‡´æ€§å®¡æŸ¥è„šæœ¬

ç”¨äºæ£€æµ‹JSONæ•°æ®å’ŒçŸ¥è¯†å›¾è°±åˆ›å»º/æ£€ç´¢ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜
"""
import json
import sys
from pathlib import Path
from typing import Dict, List, Any
from collections import defaultdict


def audit_json_file(json_path: str) -> Dict[str, Any]:
    """å®¡æŸ¥å•ä¸ªJSONæ–‡ä»¶çš„æ•°æ®å®Œæ•´æ€§"""
    with open(json_path, 'r', encoding='utf-8') as f:
        papers = json.load(f)
    
    stats = {
        "total_papers": len(papers),
        "field_presence": defaultdict(int),
        "empty_fields": defaultdict(int),
        "field_types": defaultdict(set),
        "issues": []
    }
    
    for paper in papers:
        # ç»Ÿè®¡å­—æ®µå­˜åœ¨æƒ…å†µ
        for key in paper.keys():
            stats["field_presence"][key] += 1
            
            # æ£€æŸ¥ç©ºå€¼
            value = paper.get(key)
            if value is None or value == "" or (isinstance(value, list) and len(value) == 0):
                stats["empty_fields"][key] += 1
            
            # è®°å½•å­—æ®µç±»å‹
            stats["field_types"][key].add(type(value).__name__)
        
        # æ£€æŸ¥ç‰¹å®šé—®é¢˜
        # 1. Keywordsä¸ºç©º
        if not paper.get("Keywords") or paper.get("Keywords").strip() == "":
            stats["issues"].append({
                "paper_id": paper.get("ArticleId"),
                "issue": "Keywordsä¸ºç©º",
                "impact": "æ— æ³•åˆ›å»ºFieldOfStudyèŠ‚ç‚¹å’Œå…³ç³»"
            })
        
        # 2. Referencesä¸ºç©º
        if not paper.get("References") or len(paper.get("References", [])) == 0:
            stats["issues"].append({
                "paper_id": paper.get("ArticleId"),
                "issue": "Referencesä¸ºç©º",
                "impact": "æ— æ³•åˆ›å»ºCITEDå…³ç³»"
            })
        
        # 3. Authorsä¸ºç©ºï¼ˆä¸¥é‡é—®é¢˜ï¼‰
        if not paper.get("Authors") or len(paper.get("Authors", [])) == 0:
            stats["issues"].append({
                "paper_id": paper.get("ArticleId"),
                "issue": "Authorsä¸ºç©ºï¼ˆä¸¥é‡ï¼‰",
                "impact": "æ— æ³•åˆ›å»ºAuthorèŠ‚ç‚¹å’ŒAUTHOREDå…³ç³»"
            })
        
        # 4. æ£€æŸ¥Referencesç»“æ„
        for ref in paper.get("References", []):
            if isinstance(ref, dict):
                # æ£€æŸ¥å¼•ç”¨å­—æ®µå®Œæ•´æ€§
                if not ref.get("Title"):
                    stats["issues"].append({
                        "paper_id": paper.get("ArticleId"),
                        "issue": "Referenceç¼ºå°‘Title",
                        "impact": "å¼•ç”¨èŠ‚ç‚¹ä¿¡æ¯ä¸å®Œæ•´"
                    })
                
                # æ£€æŸ¥Authorså­—æ®µï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
                if not ref.get("Authors"):
                    # è¿™æ˜¯å¸¸è§æƒ…å†µï¼Œä¸ç®—ä¸¥é‡é—®é¢˜
                    pass
    
    return stats


def check_graph_builder_compatibility(stats: Dict[str, Any]) -> List[Dict[str, str]]:
    """æ£€æŸ¥graph_builder.pyçš„å…¼å®¹æ€§"""
    issues = []
    
    # æ£€æŸ¥normalize_paper_dataå‡½æ•°æ˜¯å¦å¤„ç†äº†æ‰€æœ‰å­—æ®µ
    required_mappings = [
        ("ArticleId", "id"),
        ("Title", "title"),
        ("Abstract", "abstract"),
        ("PubYear", "year"),
        ("DOI", "doi"),
        ("Keywords", "fos"),
        ("Authors", "authors"),
        ("References", "references"),
        ("JournalTitle", "venue")
    ]
    
    for json_field, normalized_field in required_mappings:
        if json_field not in stats["field_presence"]:
            issues.append({
                "component": "normalize_paper_data",
                "issue": f"JSONå­—æ®µ '{json_field}' ä¸å­˜åœ¨",
                "severity": "é«˜"
            })
    
    # æ£€æŸ¥ç©ºå€¼å¤„ç†
    if stats["empty_fields"].get("Keywords", 0) > 0:
        issues.append({
            "component": "_create_fields_and_relationships",
            "issue": f"{stats['empty_fields']['Keywords']} ç¯‡è®ºæ–‡Keywordsä¸ºç©º",
            "severity": "ä¸­",
            "recommendation": "å·²å¤„ç†ï¼šnormalize_paper_dataä¼šè¿”å›ç©ºåˆ—è¡¨ï¼Œ_create_fields_and_relationshipsä¼šè·³è¿‡"
        })
    
    if stats["empty_fields"].get("References", 0) > 0:
        issues.append({
            "component": "_create_references_and_relationships",
            "issue": f"{stats['empty_fields']['References']} ç¯‡è®ºæ–‡Referencesä¸ºç©º",
            "severity": "ä¸­",
            "recommendation": "å·²å¤„ç†ï¼šå‡½æ•°ä¼šè¿”å›0ï¼Œä¸ä¼šåˆ›å»ºå¼•ç”¨å…³ç³»"
        })
    
    if stats["empty_fields"].get("Authors", 0) > 0:
        issues.append({
            "component": "_create_authors_and_relationships",
            "issue": f"{stats['empty_fields']['Authors']} ç¯‡è®ºæ–‡Authorsä¸ºç©º",
            "severity": "é«˜",
            "recommendation": "å·²å¤„ç†ï¼šå‡½æ•°ä¼šè¿”å›0ï¼Œä½†è®ºæ–‡èŠ‚ç‚¹ä¼šå­¤ç«‹"
        })
    
    return issues


def check_query_compatibility(stats: Dict[str, Any]) -> List[Dict[str, str]]:
    """æ£€æŸ¥graph_queries.pyçš„æŸ¥è¯¢å…¼å®¹æ€§"""
    issues = []
    
    # æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦å‡è®¾å­—æ®µæ€»æ˜¯å­˜åœ¨
    query_assumptions = [
        {
            "function": "get_author_papers",
            "assumes": ["p.year", "p.venue", "p.n_citation"],
            "issue": "å¦‚æœè¿™äº›å­—æ®µä¸ºNULLï¼Œæ’åºå¯èƒ½å‡ºé—®é¢˜"
        },
        {
            "function": "get_paper_details",
            "assumes": ["OPTIONAL MATCH"],
            "issue": "ä½¿ç”¨äº†OPTIONAL MATCHï¼Œå…¼å®¹æ€§å¥½"
        },
        {
            "function": "search_papers",
            "assumes": ["p.title CONTAINS", "p.abstract CONTAINS"],
            "issue": "å¦‚æœabstractä¸ºç©ºï¼ŒCONTAINSæŸ¥è¯¢ä»ç„¶å®‰å…¨"
        }
    ]
    
    # æ£€æŸ¥å¯èƒ½çš„NULLå€¼é—®é¢˜
    if stats["empty_fields"].get("Abstract", 0) > 0:
        issues.append({
            "component": "search_papers",
            "issue": f"{stats['empty_fields']['Abstract']} ç¯‡è®ºæ–‡Abstractä¸ºç©º",
            "severity": "ä½",
            "recommendation": "CONTAINSæŸ¥è¯¢å¯¹ç©ºå­—ç¬¦ä¸²å®‰å…¨ï¼Œä½†å¯èƒ½å½±å“æœç´¢ç»“æœ"
        })
    
    if stats["empty_fields"].get("Keywords", 0) > 0:
        issues.append({
            "component": "get_author_research_fields",
            "issue": f"{stats['empty_fields']['Keywords']} ç¯‡è®ºæ–‡æ— ç ”ç©¶é¢†åŸŸ",
            "severity": "ä¸­",
            "recommendation": "è¿™äº›è®ºæ–‡ä¸ä¼šå‡ºç°åœ¨é¢†åŸŸæŸ¥è¯¢ç»“æœä¸­"
        })
    
    return issues


def check_retrieval_tools_compatibility(stats: Dict[str, Any]) -> List[Dict[str, str]]:
    """æ£€æŸ¥MCPæ£€ç´¢å·¥å…·çš„å…¼å®¹æ€§"""
    issues = []
    
    # graph_retrieval.py æ£€æŸ¥
    issues.append({
        "component": "graph_retrieval._expand_by_citation",
        "issue": "æŸ¥è¯¢å‡è®¾ cited.n_citation å’Œ citing.n_citation å­˜åœ¨",
        "severity": "ä½",
        "recommendation": "åº”æ·»åŠ  IS NOT NULL æ£€æŸ¥æˆ–ä½¿ç”¨ COALESCE(p.n_citation, 0)"
    })
    
    issues.append({
        "component": "graph_retrieval._expand_by_field",
        "issue": f"{stats['empty_fields'].get('Keywords', 0)} ç¯‡è®ºæ–‡æ— é¢†åŸŸä¿¡æ¯",
        "severity": "ä¸­",
        "recommendation": "è¿™äº›è®ºæ–‡ä¸ä¼šè¢«é¢†åŸŸæ‰©å±•æ£€ç´¢åˆ°"
    })
    
    # flexible_graph_query.py æ£€æŸ¥
    issues.append({
        "component": "flexible_graph_query",
        "issue": "LLMç”Ÿæˆçš„æŸ¥è¯¢å¯èƒ½ä¸å¤„ç†NULLå€¼",
        "severity": "ä¸­",
        "recommendation": "å»ºè®®åœ¨å·¥å…·æè¿°ä¸­æç¤ºLLMä½¿ç”¨ IS NOT NULL æˆ– COALESCE"
    })
    
    return issues


def main():
    """ä¸»å®¡æŸ¥æµç¨‹"""
    print("=" * 80)
    print("çŸ¥è¯†å›¾è°±æ•°æ®ä¸€è‡´æ€§å®¡æŸ¥æŠ¥å‘Š")
    print("=" * 80)
    print()
    
    # å®¡æŸ¥JSONæ•°æ®
    json_file = Path(__file__).parent.parent.parent.parent / "è®ºæ–‡æ•°æ®" / "0a2bd635e05d4d768ee42968cb759011.json"
    
    if not json_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
        return
    
    print(f"ğŸ“ å®¡æŸ¥æ–‡ä»¶: {json_file.name}")
    print()
    
    stats = audit_json_file(str(json_file))
    
    # 1. æ•°æ®ç»Ÿè®¡
    print("ğŸ“Š æ•°æ®ç»Ÿè®¡")
    print("-" * 80)
    print(f"æ€»è®ºæ–‡æ•°: {stats['total_papers']}")
    print()
    
    print("å­—æ®µå­˜åœ¨æƒ…å†µ:")
    for field, count in sorted(stats["field_presence"].items()):
        percentage = (count / stats['total_papers']) * 100
        print(f"  {field:20s}: {count:3d}/{stats['total_papers']} ({percentage:5.1f}%)")
    print()
    
    print("ç©ºå­—æ®µç»Ÿè®¡:")
    for field, count in sorted(stats["empty_fields"].items()):
        if count > 0:
            percentage = (count / stats['total_papers']) * 100
            print(f"  {field:20s}: {count:3d} ç¯‡ä¸ºç©º ({percentage:5.1f}%)")
    print()
    
    print("å­—æ®µç±»å‹:")
    for field, types in sorted(stats["field_types"].items()):
        print(f"  {field:20s}: {', '.join(sorted(types))}")
    print()
    
    # 2. å…¼å®¹æ€§æ£€æŸ¥
    print("ğŸ” å…¼å®¹æ€§æ£€æŸ¥")
    print("-" * 80)
    
    print("\nã€graph_builder.py å…¼å®¹æ€§ã€‘")
    builder_issues = check_graph_builder_compatibility(stats)
    if builder_issues:
        for issue in builder_issues:
            severity_icon = "ğŸ”´" if issue.get("severity") == "é«˜" else "ğŸŸ¡" if issue.get("severity") == "ä¸­" else "ğŸŸ¢"
            print(f"{severity_icon} {issue['component']}")
            print(f"   é—®é¢˜: {issue['issue']}")
            if "recommendation" in issue:
                print(f"   å»ºè®®: {issue['recommendation']}")
            print()
    else:
        print("âœ… æ— å…¼å®¹æ€§é—®é¢˜")
    
    print("\nã€graph_queries.py å…¼å®¹æ€§ã€‘")
    query_issues = check_query_compatibility(stats)
    if query_issues:
        for issue in query_issues:
            severity_icon = "ğŸ”´" if issue.get("severity") == "é«˜" else "ğŸŸ¡" if issue.get("severity") == "ä¸­" else "ğŸŸ¢"
            print(f"{severity_icon} {issue['component']}")
            print(f"   é—®é¢˜: {issue['issue']}")
            if "recommendation" in issue:
                print(f"   å»ºè®®: {issue['recommendation']}")
            print()
    else:
        print("âœ… æ— å…¼å®¹æ€§é—®é¢˜")
    
    print("\nã€MCPæ£€ç´¢å·¥å…·å…¼å®¹æ€§ã€‘")
    retrieval_issues = check_retrieval_tools_compatibility(stats)
    if retrieval_issues:
        for issue in retrieval_issues:
            severity_icon = "ğŸ”´" if issue.get("severity") == "é«˜" else "ğŸŸ¡" if issue.get("severity") == "ä¸­" else "ğŸŸ¢"
            print(f"{severity_icon} {issue['component']}")
            print(f"   é—®é¢˜: {issue['issue']}")
            if "recommendation" in issue:
                print(f"   å»ºè®®: {issue['recommendation']}")
            print()
    else:
        print("âœ… æ— å…¼å®¹æ€§é—®é¢˜")
    
    # 3. å…³é”®é—®é¢˜æ±‡æ€»
    print("\n" + "=" * 80)
    print("âš ï¸  å…³é”®é—®é¢˜æ±‡æ€»")
    print("=" * 80)
    
    critical_issues = []
    
    # ç©ºKeywordsé—®é¢˜
    if stats["empty_fields"].get("Keywords", 0) > 0:
        critical_issues.append({
            "issue": f"{stats['empty_fields']['Keywords']} ç¯‡è®ºæ–‡æ— Keywords",
            "impact": "æ— æ³•åˆ›å»ºç ”ç©¶é¢†åŸŸèŠ‚ç‚¹ï¼Œé¢†åŸŸæ£€ç´¢ä¼šé—æ¼è¿™äº›è®ºæ–‡",
            "severity": "ä¸­"
        })
    
    # ç©ºReferencesé—®é¢˜
    if stats["empty_fields"].get("References", 0) > 0:
        critical_issues.append({
            "issue": f"{stats['empty_fields']['References']} ç¯‡è®ºæ–‡æ— References",
            "impact": "æ— æ³•å»ºç«‹å¼•ç”¨å…³ç³»ï¼Œå¼•ç”¨é“¾æ£€ç´¢ä¼šä¸­æ–­",
            "severity": "ä¸­"
        })
    
    # ç©ºAbstracté—®é¢˜
    if stats["empty_fields"].get("Abstract", 0) > 0:
        critical_issues.append({
            "issue": f"{stats['empty_fields']['Abstract']} ç¯‡è®ºæ–‡æ— Abstract",
            "impact": "å‘é‡æ£€ç´¢å’Œå…³é”®è¯æœç´¢æ•ˆæœä¸‹é™",
            "severity": "ä½"
        })
    
    # ç©ºAuthorsé—®é¢˜ï¼ˆä¸¥é‡ï¼‰
    if stats["empty_fields"].get("Authors", 0) > 0:
        critical_issues.append({
            "issue": f"{stats['empty_fields']['Authors']} ç¯‡è®ºæ–‡æ— Authors",
            "impact": "è®ºæ–‡èŠ‚ç‚¹å­¤ç«‹ï¼Œæ— æ³•é€šè¿‡ä½œè€…æ£€ç´¢",
            "severity": "é«˜"
        })
    
    for idx, issue in enumerate(critical_issues, 1):
        severity_icon = "ğŸ”´" if issue["severity"] == "é«˜" else "ğŸŸ¡" if issue["severity"] == "ä¸­" else "ğŸŸ¢"
        print(f"\n{idx}. {severity_icon} {issue['issue']}")
        print(f"   å½±å“: {issue['impact']}")
    
    if not critical_issues:
        print("\nâœ… æœªå‘ç°å…³é”®é—®é¢˜")
    
    # 4. å»ºè®®
    print("\n" + "=" * 80)
    print("ğŸ’¡ æ”¹è¿›å»ºè®®")
    print("=" * 80)
    
    recommendations = [
        {
            "component": "graph_builder.py",
            "recommendation": "âœ… å·²æ­£ç¡®å¤„ç†ç©ºå€¼ï¼šnormalize_paper_dataè¿”å›ç©ºåˆ—è¡¨ï¼Œå„createå‡½æ•°ä¼šè·³è¿‡"
        },
        {
            "component": "graph_queries.py",
            "recommendation": "å»ºè®®åœ¨æ’åºå­—æ®µä¸Šä½¿ç”¨ COALESCE(p.n_citation, 0) ç¡®ä¿NULLå€¼å®‰å…¨"
        },
        {
            "component": "graph_retrieval.py",
            "recommendation": "åœ¨è¿‡æ»¤æ¡ä»¶ä¸­æ·»åŠ  IS NOT NULL æ£€æŸ¥ï¼Œé¿å…NULLå€¼å¯¼è‡´æŸ¥è¯¢å¼‚å¸¸"
        },
        {
            "component": "flexible_graph_query.py",
            "recommendation": "åœ¨å·¥å…·æè¿°ä¸­æç¤ºLLMå¤„ç†NULLå€¼ï¼Œå»ºè®®ä½¿ç”¨ COALESCE æˆ– IS NOT NULL"
        },
        {
            "component": "æ•°æ®è´¨é‡",
            "recommendation": "è€ƒè™‘åœ¨å¯¼å…¥å‰å¯¹JSONæ•°æ®è¿›è¡Œæ¸…æ´—ï¼Œè¡¥å……ç¼ºå¤±çš„Keywordså’ŒReferences"
        }
    ]
    
    for idx, rec in enumerate(recommendations, 1):
        print(f"\n{idx}. ã€{rec['component']}ã€‘")
        print(f"   {rec['recommendation']}")
    
    print("\n" + "=" * 80)
    print("å®¡æŸ¥å®Œæˆ")
    print("=" * 80)


if __name__ == "__main__":
    main()

