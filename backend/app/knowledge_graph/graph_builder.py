"""
çŸ¥è¯†å›¾è°±æ„å»ºå™¨

è´Ÿè´£ä»JSONè®ºæ–‡æ•°æ®æ„å»ºçŸ¥è¯†å›¾è°±
æ”¯æŒå¹¶å‘å¤„ç†ã€æ‰¹é‡å¯¼å…¥ã€å¢é‡æ›´æ–°
"""

import json
import logging
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
import time
import random

from .neo4j_client import get_client
from .schema import get_cypher_create_constraints, get_cypher_create_indexes, validate_paper_data

logger = logging.getLogger(__name__)


# è£…é¥°å™¨å·²ç§»é™¤ï¼Œæ­»é”é‡è¯•é€»è¾‘ç°åœ¨ç›´æ¥åœ¨ _process_single_paper ä¸­å®ç°


def normalize_paper_data(paper: Dict) -> Dict:
    """
    æ ‡å‡†åŒ–è®ºæ–‡æ•°æ®å­—æ®µ
    
    å°†JSONæ•°æ®çš„å¤§å†™å­—æ®µåè½¬æ¢ä¸ºä»£ç æœŸæœ›çš„å°å†™å­—æ®µå
    
    Args:
        paper: åŸå§‹è®ºæ–‡æ•°æ®
        
    Returns:
        æ ‡å‡†åŒ–åçš„è®ºæ–‡æ•°æ®
    """
    # å¦‚æœå·²ç»æ˜¯å°å†™æ ¼å¼ï¼Œç›´æ¥è¿”å›
    if "id" in paper and "title" in paper:
        return paper
    
    # è½¬æ¢å­—æ®µæ˜ å°„
    normalized = {
        "id": paper.get("ArticleId"),
        "title": paper.get("Title", ""),
        "abstract": paper.get("Abstract", ""),
        "year": paper.get("PubYear"),
        "doi": paper.get("DOI", ""),
        "volume": paper.get("Volume", ""),
        "issue": paper.get("Issue", ""),
        "keywords": paper.get("Keywords", ""),
    }
    
    # è½¬æ¢ä¼šè®®/æœŸåˆŠä¿¡æ¯
    if "JournalTitle" in paper:
        normalized["venue"] = {
            "raw": paper.get("JournalTitle", ""),
            "id": paper.get("JournalId"),
            "type": "journal"
        }
    
    # è½¬æ¢ä½œè€…ä¿¡æ¯
    if "Authors" in paper:
        normalized["authors"] = []
        for author in paper.get("Authors", []):
            normalized["authors"].append({
                "id": author.get("AuthorId"),
                "name": author.get("Name", ""),
                "org": author.get("Affiliation", "")
            })
    
    # è½¬æ¢å¼•ç”¨ä¿¡æ¯
    # Referenceså¯èƒ½æ˜¯å­—å…¸åˆ—è¡¨ï¼Œéœ€è¦æå–æ ‡é¢˜æˆ–ç”ŸæˆID
    if "References" in paper:
        refs = paper.get("References", [])
        normalized["references"] = []
        for ref in refs:
            if isinstance(ref, dict):
                # å¦‚æœå¼•ç”¨æ˜¯å­—å…¸ï¼Œæå–æ ‡é¢˜ä½œä¸ºå¼•ç”¨ä¿¡æ¯
                ref_title = ref.get("Title", "")
                if ref_title:
                    # ä½¿ç”¨æ ‡é¢˜ç”Ÿæˆå”¯ä¸€ID
                    ref_id = hashlib.md5(ref_title.encode()).hexdigest()[:16]
                    normalized["references"].append({
                        "ref_id": ref_id,
                        "title": ref_title,
                        "authors": ref.get("Authors", ""),
                        "year": ref.get("PubYear"),
                        "venue": ref.get("JournalTitle", "")
                    })
            elif isinstance(ref, (str, int)):
                # å¦‚æœå¼•ç”¨æ˜¯ç®€å•çš„ID
                normalized["references"].append({"ref_id": str(ref)})
    else:
        normalized["references"] = []
    
    # è½¬æ¢ç ”ç©¶é¢†åŸŸï¼ˆä»Keywordsæå–ï¼‰
    keywords_str = paper.get("Keywords", "")
    if keywords_str:
        # æ™ºèƒ½æ£€æµ‹åˆ†éš”ç¬¦ï¼ˆä¼˜å…ˆä½¿ç”¨åˆ†å·ï¼Œå…¶æ¬¡é€—å·ï¼‰
        # æ³¨æ„ï¼šåˆ†éš”ç¬¦å¯èƒ½æ˜¯ " ; "ï¼ˆåˆ†å·+ç©ºæ ¼ï¼‰æˆ– ";"
        if ";" in keywords_str:
            separator = ";"
        elif "," in keywords_str:
            separator = ","
        else:
            separator = None
        
        # åˆ†å‰²å¹¶æ¸…ç†å…³é”®è¯ï¼ˆè‡ªåŠ¨å»é™¤é¦–å°¾ç©ºç™½ï¼‰
        if separator:
            keywords = [k.strip() for k in keywords_str.split(separator) if k.strip()]
        else:
            keywords = [keywords_str.strip()] if keywords_str.strip() else []
        
        # ğŸ”§ æ¸…ç†HTMLæ ‡ç­¾ï¼ˆå¦‚ <sub>2.5</sub>ï¼‰
        import re
        keywords = [re.sub(r'<[^>]+>', '', k) for k in keywords]
        
        # å¢åŠ åˆ°æœ€å¤š10ä¸ªå…³é”®è¯ï¼ˆè¦†ç›–æ›´å¤šç ”ç©¶é¢†åŸŸï¼‰
        normalized["fos"] = [{"name": keyword} for keyword in keywords[:10] if keyword]
    else:
        normalized["fos"] = []
    
    # å…¶ä»–å¯èƒ½çš„å­—æ®µ
    normalized["n_citation"] = paper.get("CitationCount", 0)
    normalized["doc_type"] = "journal-article"  # æ ¹æ®JSONç»“æ„æ¨æ–­
    normalized["publisher"] = paper.get("Publisher", "")
    normalized["page_start"] = paper.get("PageStart", "")
    normalized["page_end"] = paper.get("PageEnd", "")
    
    return normalized


class KnowledgeGraphBuilder:
    """
    çŸ¥è¯†å›¾è°±æ„å»ºå™¨
    
    åŠŸèƒ½:
    - ä»JSONæ–‡ä»¶æ‰¹é‡å¯¼å…¥è®ºæ–‡æ•°æ®
    - è‡ªåŠ¨æå–å®ä½“ï¼ˆè®ºæ–‡ã€ä½œè€…ã€é¢†åŸŸã€ä¼šè®®/æœŸåˆŠï¼‰
    - è‡ªåŠ¨åˆ›å»ºå…³ç³»ï¼ˆä½œè€…-è®ºæ–‡ã€å¼•ç”¨ã€åˆä½œã€å‘è¡¨ï¼‰
    - å¹¶å‘å¤„ç†æå‡å¯¼å…¥é€Ÿåº¦
    - æ”¯æŒå¢é‡æ›´æ–°
    """
    
    def __init__(self, batch_size: int = 100, max_workers: int = 2):
        """
        åˆå§‹åŒ–æ„å»ºå™¨
        
        Args:
            batch_size: æ‰¹é‡å¤„ç†å¤§å°
            max_workers: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
        """
        self.client = get_client()
        self.batch_size = batch_size
        self.max_workers = max_workers
        
        logger.info(f"çŸ¥è¯†å›¾è°±æ„å»ºå™¨åˆå§‹åŒ–: batch_size={batch_size}, max_workers={max_workers}")
    
    def initialize_schema(self) -> None:
        """åˆå§‹åŒ–æ•°æ®åº“Schemaï¼ˆçº¦æŸå’Œç´¢å¼•ï¼‰"""
        constraints = get_cypher_create_constraints()
        indexes = get_cypher_create_indexes()
        self.client.create_constraints_and_indexes(constraints, indexes)
    
    async def build_from_json(
        self,
        json_path: str,
        clear_existing: bool = False
    ) -> Dict[str, Any]:
        """
        ä»JSONæ–‡ä»¶æ„å»ºçŸ¥è¯†å›¾è°±
        
        Args:
            json_path: JSONæ–‡ä»¶è·¯å¾„
            clear_existing: æ˜¯å¦æ¸…ç©ºç°æœ‰æ•°æ®
            
        Returns:
            æ„å»ºç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±: {json_path}")
        start_time = datetime.now()
        
        # æ¸…ç©ºç°æœ‰æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if clear_existing:
            logger.warning("æ¸…ç©ºç°æœ‰æ•°æ®...")
            self.client.clear_database()
        
        # åˆå§‹åŒ–Schema
        self.initialize_schema()
        
        # åŠ è½½JSONæ•°æ®
        logger.info("åŠ è½½JSONæ•°æ®...")
        with open(json_path, 'r', encoding='utf-8') as f:
            papers = json.load(f)
        
        logger.info(f"å…±åŠ è½½ {len(papers)} ç¯‡è®ºæ–‡")
        
        # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
        logger.info("æ ‡å‡†åŒ–æ•°æ®æ ¼å¼...")
        papers = [normalize_paper_data(p) for p in papers]
        
        # éªŒè¯æ•°æ®
        valid_papers = [p for p in papers if validate_paper_data(p)]
        logger.info(f"éªŒè¯é€šè¿‡: {len(valid_papers)} ç¯‡è®ºæ–‡")
        
        # åˆ†æ‰¹å¹¶å‘å¤„ç†
        stats = await self._process_papers_concurrent(valid_papers)
        
        # æ„å»ºåˆä½œå…³ç³»
        logger.info("æ„å»ºä½œè€…åˆä½œå…³ç³»...")
        await self._build_collaboration_relationships()
        
        # ç»Ÿè®¡ä¿¡æ¯
        elapsed_time = (datetime.now() - start_time).total_seconds()
        db_stats = self.client.get_statistics()
        
        result = {
            "success": True,
            "papers_processed": len(valid_papers),
            "elapsed_time_seconds": elapsed_time,
            "database_stats": db_stats,
            "details": stats
        }
        
        logger.info(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼è€—æ—¶: {elapsed_time:.2f}ç§’")
        logger.info(f"ğŸ“Š èŠ‚ç‚¹æ€»æ•°: {db_stats['total_nodes']}, å…³ç³»æ€»æ•°: {db_stats['total_relationships']}")
        
        return result
    
    async def _process_papers_concurrent(self, papers: List[Dict]) -> Dict[str, int]:
        """
        å¹¶å‘å¤„ç†è®ºæ–‡æ•°æ®
        
        Args:
            papers: è®ºæ–‡æ•°æ®åˆ—è¡¨
            
        Returns:
            å¤„ç†ç»Ÿè®¡
        """
        # åˆ†æ‰¹
        batches = [papers[i:i + self.batch_size] for i in range(0, len(papers), self.batch_size)]
        logger.info(f"åˆ†ä¸º {len(batches)} æ‰¹å¤„ç†")
        
        stats = {
            "papers_created": 0,
            "authors_created": 0,
            "fields_created": 0,
            "venues_created": 0,
            "references_created": 0,
            "reference_authors_created": 0,  # ğŸ†• å¼•ç”¨æ–‡çŒ®ä½œè€…
            "reference_venues_created": 0,   # ğŸ†• å¼•ç”¨æ–‡çŒ®æœŸåˆŠ
            "relationships_created": 0
        }
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                loop.run_in_executor(executor, self._process_batch, batch_idx, batch)
                for batch_idx, batch in enumerate(batches)
            ]
            
            for future in asyncio.as_completed(futures):
                batch_stats = await future
                for key in stats:
                    stats[key] += batch_stats.get(key, 0)
        
        return stats
    
    def _process_single_paper(self, paper: Dict) -> Dict[str, int]:
        """
        å¤„ç†å•ç¯‡è®ºæ–‡ï¼ˆç‹¬ç«‹äº‹åŠ¡ï¼Œå¸¦æ­»é”é‡è¯•ï¼‰
        
        Args:
            paper: è®ºæ–‡æ•°æ®
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            from neo4j.exceptions import TransientError, TransactionError
        except ImportError:
            # å¦‚æœ neo4j æœªå®‰è£…ï¼Œå®šä¹‰å ä½ç¬¦å¼‚å¸¸
            TransientError = Exception
            TransactionError = Exception
        
        stats = {
            "papers_created": 0,
            "authors_created": 0,
            "fields_created": 0,
            "venues_created": 0,
            "references_created": 0,
            "reference_authors_created": 0,  # ğŸ†• å¼•ç”¨æ–‡çŒ®ä½œè€…
            "reference_venues_created": 0,   # ğŸ†• å¼•ç”¨æ–‡çŒ®æœŸåˆŠ
            "relationships_created": 0
        }
        
        max_retries = 5
        for attempt in range(max_retries):
            try:
                with self.client.get_session() as session:
                    with session.begin_transaction() as tx:
                        # åˆ›å»ºè®ºæ–‡èŠ‚ç‚¹
                        self._create_paper_node(tx, paper)
                        stats["papers_created"] = 1
                        
                        # åˆ›å»ºä½œè€…åŠå…³ç³»
                        authors_count = self._create_authors_and_relationships(tx, paper)
                        stats["authors_created"] = authors_count
                        stats["relationships_created"] += authors_count
                        
                        # åˆ›å»ºç ”ç©¶é¢†åŸŸåŠå…³ç³»
                        fields_count = self._create_fields_and_relationships(tx, paper)
                        stats["fields_created"] = fields_count
                        stats["relationships_created"] += fields_count
                        
                        # åˆ›å»ºä¼šè®®/æœŸåˆŠåŠå…³ç³»
                        if self._create_venue_and_relationship(tx, paper):
                            stats["venues_created"] = 1
                            stats["relationships_created"] += 1
                        
                        # åˆ›å»ºå¼•ç”¨å…³ç³»ï¼ˆåŒ…æ‹¬å¼•ç”¨æ–‡çŒ®çš„ä½œè€…å’ŒæœŸåˆŠï¼‰
                        refs_stats = self._create_references_and_relationships(tx, paper)
                        stats["references_created"] = refs_stats["references"]
                        stats["reference_authors_created"] = refs_stats["ref_authors"]
                        stats["reference_venues_created"] = refs_stats["ref_venues"]
                        stats["relationships_created"] += refs_stats["references"]
                
                # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                return stats
                
            except (TransientError, TransactionError) as e:
                error_msg = str(e)
                if "DeadlockDetected" in error_msg or "Deadlock" in error_msg:
                    if attempt < max_retries - 1:
                        delay = 0.2 * (2 ** attempt) + random.uniform(0, 0.1)
                        logger.warning(
                            f"è®ºæ–‡ {paper.get('id', 'unknown')} é‡åˆ°æ­»é”ï¼Œ"
                            f"{delay:.2f}ç§’åé‡è¯• (å°è¯• {attempt + 1}/{max_retries})"
                        )
                        time.sleep(delay)
                        continue
                    else:
                        logger.error(f"è®ºæ–‡ {paper.get('id', 'unknown')} æ­»é”é‡è¯•å¤±è´¥: {e}")
                        raise
                else:
                    # éæ­»é”é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    raise
            except Exception as e:
                logger.error(f"å¤„ç†è®ºæ–‡å¤±è´¥ {paper.get('id', 'unknown')}: {e}")
                raise
        
        return stats
    
    def _process_batch(self, batch_idx: int, papers: List[Dict]) -> Dict[str, int]:
        """
        å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼ˆåœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
        æ¯ç¯‡è®ºæ–‡ä½¿ç”¨ç‹¬ç«‹äº‹åŠ¡ï¼Œé¿å…æ­»é”
        
        Args:
            batch_idx: æ‰¹æ¬¡ç´¢å¼•
            papers: è®ºæ–‡åˆ—è¡¨
            
        Returns:
            æ‰¹æ¬¡ç»Ÿè®¡
        """
        logger.info(f"å¤„ç†æ‰¹æ¬¡ #{batch_idx + 1}: {len(papers)} ç¯‡è®ºæ–‡")
        
        total_stats = {
            "papers_created": 0,
            "authors_created": 0,
            "fields_created": 0,
            "venues_created": 0,
            "references_created": 0,
            "reference_authors_created": 0,  # ğŸ†• å¼•ç”¨æ–‡çŒ®ä½œè€…
            "reference_venues_created": 0,   # ğŸ†• å¼•ç”¨æ–‡çŒ®æœŸåˆŠ
            "relationships_created": 0
        }
        
        # é€ä¸ªå¤„ç†æ¯ç¯‡è®ºæ–‡ï¼Œä½¿ç”¨ç‹¬ç«‹äº‹åŠ¡
        for paper in papers:
            try:
                stats = self._process_single_paper(paper)
                # ç´¯åŠ ç»Ÿè®¡
                for key in total_stats:
                    total_stats[key] += stats[key]
            except Exception as e:
                logger.error(f"è®ºæ–‡ {paper.get('id', 'unknown')} å¤„ç†å¤±è´¥: {e}")
                continue
        
        logger.info(f"æ‰¹æ¬¡ #{batch_idx + 1} å®Œæˆ")
        return total_stats
    
    def _create_paper_node(self, tx, paper: Dict) -> None:
        """åˆ›å»ºè®ºæ–‡èŠ‚ç‚¹"""
        query = """
        MERGE (p:Paper {paper_id: $paper_id})
        SET p.title = $title,
            p.abstract = $abstract,
            p.year = $year,
            p.venue = $venue,
            p.n_citation = $n_citation,
            p.page_start = $page_start,
            p.page_end = $page_end,
            p.doc_type = $doc_type,
            p.publisher = $publisher,
            p.volume = $volume,
            p.issue = $issue,
            p.doi = $doi,
            p.created_at = datetime()
        """
        tx.run(query, {
            "paper_id": paper.get("id"),
            "title": paper.get("title", ""),
            "abstract": paper.get("abstract", ""),
            "year": paper.get("year"),
            "venue": paper.get("venue", {}).get("raw", ""),
            "n_citation": paper.get("n_citation", 0),
            "page_start": paper.get("page_start", ""),
            "page_end": paper.get("page_end", ""),
            "doc_type": paper.get("doc_type", ""),
            "publisher": paper.get("publisher", ""),
            "volume": paper.get("volume", ""),
            "issue": paper.get("issue", ""),
            "doi": paper.get("doi", "")
        })
    
    def _create_authors_and_relationships(self, tx, paper: Dict) -> int:
        """åˆ›å»ºä½œè€…èŠ‚ç‚¹åŠAUTHOREDå…³ç³»"""
        authors = paper.get("authors", [])
        if not authors:
            return 0
        
        for idx, author in enumerate(authors):
            # âš ï¸ æ³¨æ„ï¼šJSONä¸­çš„AuthorIdåªæ˜¯è®ºæ–‡å†…åºå·ï¼Œä¸æ˜¯å…¨å±€å”¯ä¸€ID
            # å¿…é¡»ä½¿ç”¨å§“å+æœºæ„ç”Ÿæˆå”¯ä¸€ID
            author_name = author.get("name", "Unknown")
            author_org = author.get("org", "")
            author_id = self._generate_author_id_from_name_org(author_name, author_org)
            
            # åˆ¤æ–­ä½œè€…ä½ç½®
            position = "first" if idx == 0 else ("last" if idx == len(authors) - 1 else "middle")
            
            query = """
            MERGE (a:Author {author_id: $author_id})
            ON CREATE SET 
                a.name = $name, 
                a.org = $org, 
                a.total_papers = 0
            ON MATCH SET 
                a.name = $name,
                // ğŸ”¥ æ™ºèƒ½æœºæ„æ›´æ–°ï¼šå¦‚æœç°æœ‰æœºæ„ä¸ºç©ºæˆ–æ–°æœºæ„æ›´è¯¦ç»†ï¼ˆæ›´é•¿ï¼‰ï¼Œåˆ™æ›´æ–°
                a.org = CASE 
                    WHEN a.org IS NULL OR a.org = '' THEN $org
                    WHEN size($org) > size(a.org) THEN $org
                    ELSE a.org 
                END
            
            WITH a
            MATCH (p:Paper {paper_id: $paper_id})
            MERGE (a)-[r:AUTHORED]->(p)
            SET r.position = $position
            """
            tx.run(query, {
                "author_id": author_id,
                "name": author_name,
                "org": author_org,
                "paper_id": paper.get("id"),
                "position": position
            })
        
        return len(authors)
    
    def _create_fields_and_relationships(self, tx, paper: Dict) -> int:
        """åˆ›å»ºç ”ç©¶é¢†åŸŸèŠ‚ç‚¹åŠBELONGS_TO_FIELDå…³ç³»"""
        fields = paper.get("fos", [])
        if not fields:
            return 0
        
        for field in fields:
            field_name = field.get("name", "")
            if not field_name:
                continue
            
            field_id = self._generate_field_id(field_name)
            
            query = """
            MERGE (f:FieldOfStudy {field_id: $field_id})
            ON CREATE SET f.name = $name, f.paper_count = 1
            ON MATCH SET f.paper_count = f.paper_count + 1
            
            WITH f
            MATCH (p:Paper {paper_id: $paper_id})
            MERGE (p)-[:BELONGS_TO_FIELD]->(f)
            """
            tx.run(query, {
                "field_id": field_id,
                "name": field_name,
                "paper_id": paper.get("id")
            })
        
        return len(fields)
    
    def _create_venue_and_relationship(self, tx, paper: Dict) -> bool:
        """åˆ›å»ºä¼šè®®/æœŸåˆŠèŠ‚ç‚¹åŠPUBLISHED_INå…³ç³»"""
        venue_info = paper.get("venue", {})
        venue_name = venue_info.get("raw", "")
        
        if not venue_name:
            return False
        
        venue_id = self._generate_venue_id(venue_name)
        
        query = """
        MERGE (v:Venue {venue_id: $venue_id})
        ON CREATE SET v.name = $name, v.type = $type, v.paper_count = 1
        ON MATCH SET v.paper_count = v.paper_count + 1
        
        WITH v
        MATCH (p:Paper {paper_id: $paper_id})
        MERGE (p)-[r:PUBLISHED_IN]->(v)
        SET r.year = $year
        """
        tx.run(query, {
            "venue_id": venue_id,
            "name": venue_name,
            "type": "conference",  # é»˜è®¤ç±»å‹
            "paper_id": paper.get("id"),
            "year": paper.get("year")
        })
        
        return True
    
    def _create_references_and_relationships(self, tx, paper: Dict) -> Dict[str, int]:
        """
        åˆ›å»ºå‚è€ƒæ–‡çŒ®åŠCITEDå…³ç³»ï¼ˆå¢å¼ºç‰ˆï¼šå¤„ç†å¼•ç”¨ä¸­çš„ä½œè€…å’ŒæœŸåˆŠï¼‰
        
        Returns:
            åŒ…å«å„ç±»ç»Ÿè®¡çš„å­—å…¸
        """
        references = paper.get("references", [])
        if not references:
            return {"references": 0, "ref_authors": 0, "ref_venues": 0}
        
        ref_count = 0
        ref_author_count = 0
        ref_venue_count = 0
        
        for ref in references:
            # æå–å¼•ç”¨IDå’Œå…¶ä»–ä¿¡æ¯
            if isinstance(ref, dict):
                ref_id = ref.get("ref_id")
                ref_title = ref.get("title", "")
                ref_authors = ref.get("authors", "")
                ref_year = ref.get("year")
                ref_venue = ref.get("venue", "")
            else:
                # å…¼å®¹æ—§æ ¼å¼ï¼ˆç®€å•çš„IDï¼‰
                ref_id = str(ref)
                ref_title = ""
                ref_authors = ""
                ref_year = None
                ref_venue = ""
            
            if not ref_id:
                continue
            
            # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯å·²å­˜åœ¨çš„è®ºæ–‡
            check_query = "MATCH (p:Paper {paper_id: $ref_id}) RETURN p"
            result = tx.run(check_query, {"ref_id": ref_id}).single()
            
            if result:
                # å¼•ç”¨çš„æ˜¯å·²å­˜åœ¨çš„è®ºæ–‡
                cite_query = """
                MATCH (p1:Paper {paper_id: $paper_id})
                MATCH (p2:Paper {paper_id: $ref_id})
                MERGE (p1)-[:CITED]->(p2)
                """
                tx.run(cite_query, {"paper_id": paper.get("id"), "ref_id": ref_id})
            else:
                # åˆ›å»ºReferenceèŠ‚ç‚¹ï¼ŒåŒ…å«æ›´å¤šå…ƒæ•°æ®
                ref_query = """
                MERGE (r:Reference {ref_id: $ref_id})
                ON CREATE SET 
                    r.title = $title,
                    r.authors = $authors,
                    r.year = $year,
                    r.venue = $venue
                ON MATCH SET
                    r.title = CASE WHEN r.title = '' AND $title <> '' THEN $title ELSE r.title END,
                    r.authors = CASE WHEN r.authors = '' AND $authors <> '' THEN $authors ELSE r.authors END,
                    r.year = CASE WHEN r.year IS NULL AND $year IS NOT NULL THEN $year ELSE r.year END,
                    r.venue = CASE WHEN r.venue = '' AND $venue <> '' THEN $venue ELSE r.venue END
                
                WITH r
                MATCH (p:Paper {paper_id: $paper_id})
                MERGE (p)-[:CITED]->(r)
                """
                tx.run(ref_query, {
                    "ref_id": ref_id,
                    "title": ref_title,
                    "authors": ref_authors,
                    "year": ref_year,
                    "venue": ref_venue,
                    "paper_id": paper.get("id")
                })
                
                # ğŸ†• è§£æå¹¶åˆ›å»ºå¼•ç”¨æ–‡çŒ®çš„ä½œè€…èŠ‚ç‚¹å’Œå…³ç³»
                if ref_authors:
                    author_count = self._create_reference_authors(tx, ref_id, ref_authors)
                    ref_author_count += author_count
                
                # ğŸ†• åˆ›å»ºå¼•ç”¨æ–‡çŒ®çš„æœŸåˆŠ/ä¼šè®®èŠ‚ç‚¹å’Œå…³ç³»
                if ref_venue:
                    if self._create_reference_venue(tx, ref_id, ref_venue, ref_year):
                        ref_venue_count += 1
            
            ref_count += 1
        
        return {
            "references": ref_count,
            "ref_authors": ref_author_count,
            "ref_venues": ref_venue_count
        }
    
    def _create_reference_authors(self, tx, ref_id: str, authors_str: str) -> int:
        """
        è§£æå¼•ç”¨æ–‡çŒ®çš„ä½œè€…å­—ç¬¦ä¸²å¹¶åˆ›å»ºä½œè€…èŠ‚ç‚¹å’Œå…³ç³»
        
        Args:
            tx: Neo4jäº‹åŠ¡
            ref_id: å¼•ç”¨æ–‡çŒ®ID
            authors_str: ä½œè€…å­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ "Jinyin Chen; Keke Hu; Yitao Yang"
            
        Returns:
            åˆ›å»ºçš„ä½œè€…æ•°é‡
        """
        # æ£€æµ‹åˆ†éš”ç¬¦ï¼ˆå¯èƒ½æ˜¯åˆ†å·æˆ–é€—å·ï¼‰
        if "; " in authors_str:
            separator = "; "
        elif ";" in authors_str:
            separator = ";"
        elif ", " in authors_str:
            separator = ", "
        else:
            # å•ä¸ªä½œè€…
            separator = None
        
        # åˆ†å‰²ä½œè€…å
        if separator:
            author_names = [name.strip() for name in authors_str.split(separator) if name.strip()]
        else:
            author_names = [authors_str.strip()] if authors_str.strip() else []
        
        if not author_names:
            return 0
        
        count = 0
        for author_name in author_names:
            if not author_name:
                continue
            
            # ä½¿ç”¨å§“åç”ŸæˆIDï¼ˆå¼•ç”¨æ–‡çŒ®é€šå¸¸æ²¡æœ‰æœºæ„ä¿¡æ¯ï¼‰
            author_id = self._generate_author_id_from_name_org(author_name, "")
            
            query = """
            MERGE (a:Author {author_id: $author_id})
            ON CREATE SET a.name = $name, a.org = '', a.total_papers = 0
            ON MATCH SET a.name = $name
            
            WITH a
            MATCH (r:Reference {ref_id: $ref_id})
            MERGE (a)-[rel:AUTHORED]->(r)
            """
            
            tx.run(query, {
                "author_id": author_id,
                "name": author_name,
                "ref_id": ref_id
            })
            count += 1
        
        return count
    
    def _create_reference_venue(self, tx, ref_id: str, venue_name: str, year: Optional[int] = None) -> bool:
        """
        åˆ›å»ºå¼•ç”¨æ–‡çŒ®çš„æœŸåˆŠ/ä¼šè®®èŠ‚ç‚¹å’Œå…³ç³»
        
        Args:
            tx: Neo4jäº‹åŠ¡
            ref_id: å¼•ç”¨æ–‡çŒ®ID
            venue_name: æœŸåˆŠ/ä¼šè®®åç§°
            year: å‘è¡¨å¹´ä»½ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸåˆ›å»º
        """
        if not venue_name:
            return False
        
        venue_id = self._generate_venue_id(venue_name)
        
        query = """
        MERGE (v:Venue {venue_id: $venue_id})
        ON CREATE SET v.name = $name, v.type = 'journal', v.paper_count = 0
        ON MATCH SET v.name = $name
        
        WITH v
        MATCH (r:Reference {ref_id: $ref_id})
        MERGE (r)-[rel:PUBLISHED_IN]->(v)
        SET rel.year = $year
        """
        
        tx.run(query, {
            "venue_id": venue_id,
            "name": venue_name,
            "ref_id": ref_id,
            "year": year
        })
        
        return True
    
    async def _build_collaboration_relationships(self) -> None:
        """æ„å»ºä½œè€…åˆä½œå…³ç³»ï¼ˆåŸºäºå…±åŒä½œè€…çš„è®ºæ–‡ï¼‰"""
        query = """
        MATCH (a1:Author)-[:AUTHORED]->(p:Paper)<-[:AUTHORED]-(a2:Author)
        WHERE a1.author_id < a2.author_id
        WITH a1, a2, collect(p) as papers, min(p.year) as first_year, max(p.year) as last_year
        MERGE (a1)-[c:COLLABORATED]-(a2)
        SET c.paper_count = size(papers),
            c.first_collab_year = first_year,
            c.last_collab_year = last_year
        """
        
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, self.client.execute_write, query)
        logger.info("âœ… ä½œè€…åˆä½œå…³ç³»æ„å»ºå®Œæˆ")
    
    # ======================== è¾…åŠ©æ–¹æ³• ========================
    
    @staticmethod
    def _generate_author_id_from_name_org(name: str, org: str = "") -> str:
        """
        ç”Ÿæˆä½œè€…å”¯ä¸€IDï¼ˆä»…åŸºäºå§“åå“ˆå¸Œï¼‰
        
        Args:
            name: ä½œè€…å§“å
            org: ä½œè€…æœºæ„ï¼ˆå¯é€‰ï¼Œä»…ç”¨äºæ›´æ–°èŠ‚ç‚¹å±æ€§ï¼Œä¸å‚ä¸IDç”Ÿæˆï¼‰
            
        Returns:
            16ä½å“ˆå¸ŒID
            
        Note:
            - âœ… åªä½¿ç”¨å§“åç”ŸæˆIDï¼Œç¡®ä¿åŒåä½œè€…è¢«åˆå¹¶ä¸ºä¸€ä¸ªèŠ‚ç‚¹
            - âš ï¸ å¯èƒ½å­˜åœ¨åŒåä¸åŒäººçš„æƒ…å†µï¼ˆæå°‘è§ï¼‰ï¼Œä½†ä¼˜å…ˆä¿è¯å»é‡
            - ğŸ“ æœºæ„ä¿¡æ¯åœ¨MERGEæ—¶è‡ªåŠ¨é€‰æ‹©æœ€å®Œæ•´çš„ç‰ˆæœ¬ï¼ˆON MATCHé€»è¾‘å¤„ç†ï¼‰
        """
        # æ ‡å‡†åŒ–ï¼šå»é™¤é¦–å°¾ç©ºç™½ï¼Œè½¬å°å†™
        name = name.strip().lower()
        
        # åªä½¿ç”¨å§“åç”ŸæˆIDï¼ˆç¡®ä¿å»é‡ï¼‰
        return hashlib.md5(name.encode()).hexdigest()[:16]
    
    @staticmethod
    def _generate_author_id(name: str) -> str:
        """ç”Ÿæˆä½œè€…IDï¼ˆåŸºäºå§“åå“ˆå¸Œï¼‰- å·²åºŸå¼ƒï¼Œä½¿ç”¨ _generate_author_id_from_name_org"""
        return hashlib.md5(name.encode()).hexdigest()[:16]
    
    @staticmethod
    def _generate_field_id(name: str) -> str:
        """ç”Ÿæˆé¢†åŸŸIDï¼ˆåŸºäºé¢†åŸŸåå“ˆå¸Œï¼‰"""
        return hashlib.md5(name.encode()).hexdigest()[:16]
    
    @staticmethod
    def _generate_venue_id(name: str) -> str:
        """ç”Ÿæˆä¼šè®®/æœŸåˆŠIDï¼ˆåŸºäºåç§°å“ˆå¸Œï¼‰"""
        return hashlib.md5(name.encode()).hexdigest()[:16]

