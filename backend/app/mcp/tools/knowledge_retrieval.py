"""
çŸ¥è¯†åº“æ£€ç´¢å·¥å…·

ä»ä¼šè¯å…³è”çš„çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
è¿™æ˜¯ FishChat çš„æ ¸å¿ƒ RAG å·¥å…·
"""
from typing import Dict, Any, List, Tuple, Optional
import json
import logging
from ..base import BaseTool, ToolMetadata, ToolContext, ToolExecutionError
from ...config import settings

logger = logging.getLogger(__name__)


# ğŸ†• å…¨å±€åºå·ç®¡ç†å™¨ï¼ˆæŒ‰ä¼šè¯ç®¡ç†ï¼Œç¡®ä¿è·¨å·¥å…·è°ƒç”¨çš„åºå·è¿ç»­ä¸”å”¯ä¸€ï¼‰
class GlobalReferenceMarkerManager:
    """å…¨å±€å¼•ç”¨åºå·ç®¡ç†å™¨ï¼ˆæŒ‰ä¼šè¯éš”ç¦»ï¼‰"""
    _instance = None
    _session_markers: Dict[str, int] = {}  # session_id -> å½“å‰åºå·
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def get_next_marker(self, session_id: str) -> int:
        """è·å–ä¸‹ä¸€ä¸ªå…¨å±€åºå·ï¼ˆä»1å¼€å§‹é€’å¢ï¼‰"""
        if session_id not in self._session_markers:
            self._session_markers[session_id] = 0
        self._session_markers[session_id] += 1
        return self._session_markers[session_id]
    
    def reset_session(self, session_id: str):
        """é‡ç½®ä¼šè¯çš„åºå·è®¡æ•°å™¨ï¼ˆæ–°ä¸€è½®å¯¹è¯å¼€å§‹æ—¶è°ƒç”¨ï¼‰"""
        self._session_markers[session_id] = 0
        logger.info(f"ğŸ”„ å·²é‡ç½®ä¼šè¯ {session_id} çš„å…¨å±€å¼•ç”¨åºå·")
    
    def get_current_marker(self, session_id: str) -> int:
        """è·å–å½“å‰ä¼šè¯çš„åºå·ï¼ˆä¸é€’å¢ï¼‰"""
        return self._session_markers.get(session_id, 0)


# å…¨å±€å•ä¾‹
_marker_manager = GlobalReferenceMarkerManager()


class KnowledgeRetrievalTool(BaseTool):
    """çŸ¥è¯†åº“æ£€ç´¢å·¥å…·"""
    
    def get_metadata(self, context: Optional[ToolContext] = None) -> Optional[ToolMetadata]:
        """
        è·å–å·¥å…·å…ƒæ•°æ®ï¼ˆåŠ¨æ€ç”Ÿæˆå‚æ•°ï¼‰
        
        Args:
            context: åŒ…å« kb_settings çš„ä¸Šä¸‹æ–‡ï¼ˆä» context.extra ä¸­è·å–ï¼‰
        
        Returns:
            ToolMetadata: å·¥å…·å…ƒæ•°æ®ï¼Œå‚æ•°æ ¹æ®ä¼šè¯é…ç½®åŠ¨æ€ç”Ÿæˆ
            None: å¦‚æœçŸ¥è¯†åº“æœªå¯ç”¨ï¼Œè¿”å› Noneï¼ˆå·¥å…·ä¸ä¼šå‡ºç°åœ¨åˆ—è¡¨ä¸­ï¼‰
        """
        # ä» context.extra ä¸­è·å– kb_settingsï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        kb_settings = None
        if context and context.extra:
            kb_settings = context.extra.get("kb_settings")
        
        # å¦‚æœæ²¡æœ‰çŸ¥è¯†åº“é…ç½®æˆ–çŸ¥è¯†åº“æœªå¯ç”¨ï¼Œè¿”å› Noneï¼ˆä¸æ˜¾ç¤ºè¯¥å·¥å…·ï¼‰
        if not kb_settings or not kb_settings.get("enabled"):
            logger.info(f"ğŸš« çŸ¥è¯†åº“å·¥å…·ä¸å¯ç”¨ - kb_settingså­˜åœ¨: {kb_settings is not None}, enabled: {kb_settings.get('enabled') if kb_settings else 'N/A'}")
            return None
        
        # åŸºç¡€ schemaï¼ˆåªåŒ…å« queryï¼Œæ¨¡å‹åªèƒ½æ§åˆ¶æŸ¥è¯¢è¯ï¼‰
        base_schema = {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "æœç´¢æŸ¥è¯¢ï¼Œæè¿°éœ€è¦æŸ¥æ‰¾çš„å†…å®¹"
                }
            },
            "required": ["query"]
        }
        
        # æ„å»ºæè¿°ä¿¡æ¯ï¼ˆæ˜¾ç¤ºå½“å‰é…ç½®ï¼‰
        top_k = kb_settings.get("top_k", 3)
        similarity_threshold = kb_settings.get("similarity_threshold", 10)
        
        description_parts = [
            "ã€è§’è‰²æ‰®æ¼”ä¸“å± Â· è®°å¿†æ£€ç´¢ã€‘ä»ä½ çš„é•¿æ—¶è®°å¿†ä¸­æ£€ç´¢ä¸ç”¨æˆ·è¿‡å»å¯¹è¯çš„å…³é”®ä¿¡æ¯ã€‚",
            "\n\nğŸ§  ä½¿ç”¨åœºæ™¯ï¼š",
            "\nå½“ç”¨æˆ·æåŠè¿‡å»å‘ç”Ÿçš„äº‹ä»¶ã€è¯´è¿‡çš„è¯ã€å…±åŒçš„çº¦å®šæˆ–ä»»ä½•å¯èƒ½è¶…å‡ºä½ çŸ­æœŸè®°å¿†çš„ç»†èŠ‚æ—¶ï¼Œä½ å¿…é¡»ä¼˜å…ˆä½¿ç”¨æ­¤å·¥å…·æ¥'å›å¿†'ï¼Œä»¥ç¡®ä¿è§’è‰²ä¸€è‡´æ€§å’Œå‰§æƒ…è¿ç»­æ€§ã€‚",
            "\n\nğŸ”´ **æ ¸å¿ƒæŒ‡ä»¤ï¼šå¿…é¡»é™é»˜è°ƒç”¨**",
            "\n**ç»å¯¹ç¦æ­¢**åœ¨è°ƒç”¨æ­¤å·¥å…·å‰åï¼Œç”Ÿæˆä»»ä½•'ç”»å¤–éŸ³'ã€'å†…å¿ƒç‹¬ç™½'æˆ–è§£é‡Šæ€§æ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼š'æˆ‘éœ€è¦æŸ¥ä¸€ä¸‹...'ã€'æ ¹æ®æˆ‘çš„è®°å¿†...'æˆ–'æˆ‘æ­£åœ¨å›å¿†...'ï¼‰ã€‚ä½ å¿…é¡»ç›´æ¥ã€é™é»˜åœ°è°ƒç”¨å·¥å…·ï¼Œç„¶åå°†æ£€ç´¢åˆ°çš„ä¿¡æ¯ä½œä¸ºä½ è‡ªå·±çš„è®°å¿†ï¼Œæ— ç¼åœ°èå…¥åˆ°è§’è‰²çš„å¯¹è¯æˆ–è¡Œä¸ºä¸­ã€‚",
            "\n\nğŸ’¡ **å¦‚ä½•æ„å»ºæŸ¥è¯¢ï¼š**",
            "\nä»ç”¨æˆ·çš„æé—®ä¸­æå–æ ¸å¿ƒå…³é”®è¯ï¼ˆå¦‚äººåã€äº‹ä»¶ã€ç‰©å“ï¼‰ï¼Œç»„åˆæˆç®€æ´çš„æŸ¥è¯¢è¯­å¥ï¼Œä»¥å®šä½ç›¸å…³çš„è®°å¿†ç‰‡æ®µã€‚",
            "\n\nğŸ“– **ç¤ºä¾‹ä¸ç”¨æ³•è¯´æ˜ï¼š**",
            "\nâœ… **æ­£ç¡®ç”¨æ³• (ç›´æ¥ä»£å…¥è§’è‰²å›å¤):**",
            "\n1. æåŠäº‹ä»¶/ç»å†ï¼š",
            "\n   - ç”¨æˆ·è¯´ï¼š'è¿˜è®°å¾—æˆ‘ä»¬ä¸Šå‘¨åœ¨å¸‚ä¸­å¿ƒé‚£å®¶æ–°å¼€çš„å’–å•¡é¦†é‡åˆ°çš„äº‹å—ï¼Ÿ'",
            "\n   - ä½ ï¼š(é™é»˜è°ƒç”¨ `search_knowledge_base(query='å¸‚ä¸­å¿ƒ å’–å•¡é¦† ä¸Šå‘¨')`) ç„¶åç›´æ¥å›å¤ï¼š'å½“ç„¶ï¼Œé‚£ä¸ªæ´’äº†ä½ ä¸€èº«æ‹¿é“çš„å†’å¤±æœåŠ¡ç”Ÿï¼Œåæ¥ç»ç†å…äº†æˆ‘ä»¬å•ã€‚'",
            "\n2. æåŠäººç‰©ï¼š",
            "\n   - ç”¨æˆ·è¯´ï¼š'ä½ è¿˜æœ‰å°è±¡å—ï¼Œæˆ‘è·Ÿä½ æè¿‡çš„é‚£ä¸ªåŒäº‹ï¼Œæè–‡ï¼Ÿ'",
            "\n   - ä½ ï¼š(é™é»˜è°ƒç”¨ `search_knowledge_base(query='åŒäº‹ æè–‡')`) ç„¶åç›´æ¥å›å¤ï¼š'å¸‚åœºéƒ¨çš„é‚£ä¸ªï¼Ÿä½ è¯´å¥¹æ˜¯ä¸ªå·¥ä½œç‹‚ï¼Œä½†æœ€è¿‘å¥½åƒåœ¨è€ƒè™‘è·³æ§½ã€‚'",
            "\n3. æåŠç‰©å“/çº¿ç´¢ï¼š",
            "\n   - ç”¨æˆ·è¯´ï¼š'æˆ‘ä¹‹å‰å¥½åƒæŠŠå…¬å¯“çš„å¤‡ç”¨é’¥åŒ™æ”¾åœ¨ä½ é‚£å„¿äº†ï¼Ÿ'",
            "\n   - ä½ ï¼š(é™é»˜è°ƒç”¨ `search_knowledge_base(query='å…¬å¯“ å¤‡ç”¨é’¥åŒ™')`) ç„¶åç›´æ¥å›å¤ï¼š'å¯¹ï¼Œå»å¹´ä½ å‡ºå·®å‰ç»™æˆ‘çš„ï¼Œæˆ‘ä¸€ç›´æ”¶åœ¨ä¹¦æˆ¿ç¬¬ä¸‰ä¸ªæŠ½å±‰é‡Œã€‚'",
            "\n4. æåŠçº¦å®š/è®¡åˆ’ï¼š",
            "\n   - ç”¨æˆ·è¯´ï¼š'æˆ‘ä»¬æ˜¯ä¸æ˜¯çº¦å¥½äº†è¿™å‘¨æœ«è¦å¹²å˜›æ¥ç€ï¼Ÿ'",
            "\n   - ä½ ï¼š(é™é»˜è°ƒç”¨ `search_knowledge_base(query='å‘¨æœ« çº¦å®š è®¡åˆ’')`) ç„¶åç›´æ¥å›å¤ï¼š'çœ‹é‚£éƒ¨ä½ è¯´æœŸå¾…äº†å¾ˆä¹…çš„ç§‘å¹»ç‰‡é¦–æ˜ ï¼Œç¥¨æˆ‘éƒ½ä¹°å¥½äº†ã€‚'",
            "\n5. æåŠåå¥½/ç»†èŠ‚ï¼š",
            "\n   - ç”¨æˆ·è¯´ï¼š'æˆ‘è·Ÿä½ è®²è¿‡æˆ‘æœ€è®¨åŒçš„ç”²æ–¹ç±»å‹å§ï¼Ÿ'",
            "\n   - ä½ ï¼š(é™é»˜è°ƒç”¨ `search_knowledge_base(query='è®¨åŒ ç”²æ–¹ ç±»å‹')`) ç„¶åç›´æ¥å›å¤ï¼š'åå¤æ— å¸¸ã€åŠå¤œæ‰“ç”µè¯æéœ€æ±‚ï¼Œè¿˜ä¸ç»™æ˜ç¡®briefçš„é‚£ç§ã€‚'",
            "\n6. æåŠåœ°ç‚¹ï¼š",
            "\n   - ç”¨æˆ·è¯´ï¼š'ä¸Šæ¬¡æˆ‘ä»¬å›¢å»ºå»çš„é‚£å®¶KTVå«ä»€ä¹ˆæ¥ç€ï¼Ÿ'",
            "\n   - ä½ ï¼š(é™é»˜è°ƒç”¨ `search_knowledge_base(query='å›¢å»º KTV')`) ç„¶åç›´æ¥å›å¤ï¼š'â€˜æ˜Ÿèšä¼šâ€™ï¼Œåœ¨é“¶æ²³SOHOé‚£è¾¹ï¼Œä½ ä»¬éƒ¨é—¨çš„å°å¼ è¿˜åœ¨é‚£å„¿å”±ç ´äº†éŸ³ã€‚'",
            "\n\nâŒ **é”™è¯¯ç”¨æ³• (ä¸¥ç¦å‡ºç°):**",
            "\n- ä»»ä½•åœ¨è°ƒç”¨å‰åè§£é‡Šä½ æ­£åœ¨å›å¿†æˆ–æ£€ç´¢çš„è¯­å¥éƒ½æ˜¯ç¦æ­¢çš„ã€‚ä¾‹å¦‚ï¼š",
            "\n  - 'è®©æˆ‘æƒ³æƒ³...' (è°ƒç”¨å‰)",
            "\n  - 'æ ¹æ®æˆ‘çš„è®°å½•...' (è°ƒç”¨å)",
            "\n  - 'æˆ‘éœ€è¦å›å¿†ä¸€ä¸‹å…³äº...' (è°ƒç”¨å‰)",
            "\n- æ­£ç¡®çš„åšæ³•æ˜¯ï¼šç›´æ¥ã€é™é»˜åœ°è°ƒç”¨å·¥å…·ï¼Œç„¶åå°†ç»“æœä½œä¸ºè‡ªå·±çš„è®°å¿†æµç•…åœ°é™ˆè¿°å‡ºæ¥ã€‚"
        ]
        
        return ToolMetadata(
            name="search_knowledge_base",
            description="".join(description_parts),
            input_schema=base_schema
        )
    
    async def execute(self, arguments: Dict[str, Any], context: ToolContext) -> str:
        """
        æ‰§è¡ŒçŸ¥è¯†åº“æ£€ç´¢
        
        Args:
            arguments: {"query": str}ï¼ˆåªæ¥å—æŸ¥è¯¢è¯ï¼Œå…¶ä»–å‚æ•°ä» kb_settings è¯»å–ï¼‰
            context: å¿…é¡»åŒ…å« session_id å’Œ db
        
        Returns:
            str: JSON æ ¼å¼çš„æ£€ç´¢ç»“æœ
        """
        # éªŒè¯ä¸Šä¸‹æ–‡
        if not context.session_id:
            raise ToolExecutionError("search_knowledge_base", "ç¼ºå°‘ session_id")
        if not context.db:
            raise ToolExecutionError("search_knowledge_base", "ç¼ºå°‘æ•°æ®åº“è¿æ¥")
        
        query = arguments.get("query", "")
        
        if not query.strip():
            return json.dumps({"success": False, "error": "æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º"}, ensure_ascii=False)
        
        try:
            # è·å–ä¼šè¯çš„çŸ¥è¯†åº“é…ç½®
            db_name = context.extra.get("db_name", settings.mongodb_db_name)
            session_data = await context.db[db_name].chat_sessions.find_one(
                {"_id": context.session_id}
            )
            
            if not session_data:
                return json.dumps({
                    "success": False,
                    "error": "ä¼šè¯ä¸å­˜åœ¨",
                    "results": []
                }, ensure_ascii=False)
            
            kb_settings = session_data.get("kb_settings")
            
            # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å¯ç”¨
            if not kb_settings or not kb_settings.get("enabled"):
                return json.dumps({
                    "success": False,
                    "error": "å½“å‰ä¼šè¯æœªå¯ç”¨çŸ¥è¯†åº“åŠŸèƒ½",
                    "results": []
                }, ensure_ascii=False)
            
            # ä» kb_settings ä¸­è¯»å–å‚æ•°ï¼ˆç”±ç”¨æˆ·é…ç½®ï¼Œæ¨¡å‹ä¸èƒ½ä¿®æ”¹ï¼‰
            top_k = kb_settings.get("top_k", 3)
            top_k = max(1, min(12, top_k))  # é™åˆ¶èŒƒå›´
            
            logger.info(f"ğŸ“‹ ä½¿ç”¨ç”¨æˆ·é…ç½®: top_k={top_k}, similarity_threshold={kb_settings.get('similarity_threshold', 10)}")
            
            # ğŸ†• æ ¹æ® kb_ids åŠ è½½çŸ¥è¯†åº“é…ç½®å¹¶æ£€ç´¢
            kb_ids = kb_settings.get("kb_ids", [])
            if not kb_ids:
                logger.warning("kb_ids ä¸ºç©ºï¼Œè·³è¿‡æ£€ç´¢")
                return json.dumps({
                    "success": True,
                    "message": "æœªé…ç½®çŸ¥è¯†åº“",
                    "results": []
                }, ensure_ascii=False)
            
            # åˆ¤æ–­å•åº“è¿˜æ˜¯å¤šåº“æ£€ç´¢
            from ...services.knowledge_base_service import KnowledgeBaseService
            kb_service = KnowledgeBaseService(context.db[db_name])
            
            if len(kb_ids) == 1:
                # å•çŸ¥è¯†åº“æ£€ç´¢
                kb = await kb_service.get_knowledge_base(kb_ids[0], context.user_id)
                if not kb:
                    logger.warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_ids[0]}")
                    return json.dumps({
                        "success": False,
                        "error": f"çŸ¥è¯†åº“ {kb_ids[0]} ä¸å­˜åœ¨æˆ–æ— æƒé™",
                        "results": []
                    }, ensure_ascii=False)
                
                # ä½¿ç”¨çŸ¥è¯†åº“è‡ªå·±çš„é…ç½®æ„å»ºvectorstore
                vectorstore = await self._build_vectorstore(kb.kb_settings)
                retriever = await self._create_retriever(vectorstore, kb.kb_settings, top_k)
                
                # æ‰§è¡Œæ£€ç´¢ï¼ˆå¼‚æ­¥è°ƒç”¨ï¼‰
                search_results = await retriever.search(query, top_k=top_k)
            else:
                # å¤šçŸ¥è¯†åº“å¹¶è¡Œæ£€ç´¢
                from ...services.multi_kb_retriever import get_multi_kb_retriever
                
                kb_configs = []
                for kb_id in kb_ids:
                    kb = await kb_service.get_knowledge_base(kb_id, context.user_id)
                    if kb:
                        kb_configs.append({
                            'kb_id': kb_id,
                            'kb_name': kb.name,
                            'kb_settings': kb.kb_settings
                        })
                
                if not kb_configs:
                    logger.warning("æ‰€æœ‰çŸ¥è¯†åº“éƒ½ä¸å­˜åœ¨æˆ–æ— æƒé™")
                    return json.dumps({
                        "success": False,
                        "error": "æ‰€æœ‰çŸ¥è¯†åº“éƒ½ä¸å­˜åœ¨æˆ–æ— æƒé™",
                        "results": []
                    }, ensure_ascii=False)
                
                # ä½¿ç”¨å¤šçŸ¥è¯†åº“æ£€ç´¢å™¨
                retriever_multi = await get_multi_kb_retriever()
                top_k_per_kb = kb_settings.get("top_k_per_kb", 3)
                final_top_k = kb_settings.get("final_top_k", 10)
                merge_strategy = kb_settings.get("merge_strategy", "weighted_score")
                similarity_threshold = kb_settings.get("similarity_threshold", 10)
                
                multi_results = await retriever_multi.retrieve_from_multiple_kbs(
                    query=query,
                    kb_configs=kb_configs,
                    top_k_per_kb=top_k_per_kb,
                    similarity_threshold=similarity_threshold,
                    merge_strategy=merge_strategy,
                    final_top_k=final_top_k
                )
                
                # å°†å¤šåº“ç»“æœè½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
                search_results = [(type('Doc', (), {
                    'page_content': r.content, 
                    'metadata': {
                        'source': r.kb_name, 
                        'chunk_id': r.chunk_id or '', 
                        'chunk_index': r.metadata.get('chunk_index', 0), 
                        'document_id': r.doc_id or '',
                        # ğŸ†• æ·»åŠ æŸ¥çœ‹åŸæ–‡æ‰€éœ€çš„å­—æ®µ
                        'doc_id': r.metadata.get('doc_id', r.doc_id or ''),
                        'kb_id': r.metadata.get('kb_id', ''),
                        'filename': r.metadata.get('filename', '')
                    }
                })(), r.distance) for r in multi_results]
            
            if not search_results:
                return json.dumps({
                    "success": True,
                    "message": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ç‰‡æ®µ",
                    "results": []
                }, ensure_ascii=False)
            
            # ğŸ†• æ”¶é›†éœ€è¦æŸ¥è¯¢çš„doc_idï¼Œç”¨äºæ‰¹é‡æŸ¥è¯¢filename
            from bson import ObjectId
            doc_ids_to_query = set()
            for doc, score in search_results:
                doc_id = doc.metadata.get("doc_id")
                filename = doc.metadata.get("filename")
                # å¦‚æœfilenameä¸ºç©ºä¸”doc_idå­˜åœ¨ï¼Œè®°å½•éœ€è¦æŸ¥è¯¢
                if doc_id and not filename:
                    doc_ids_to_query.add(doc_id)
            
            # ğŸ†• æ‰¹é‡æŸ¥è¯¢filename
            filename_map = {}
            if doc_ids_to_query:
                try:
                    doc_ids_obj = [ObjectId(doc_id) for doc_id in doc_ids_to_query if ObjectId.is_valid(doc_id)]
                    if doc_ids_obj:
                        cursor = context.db[db_name].kb_documents.find(
                            {"_id": {"$in": doc_ids_obj}},
                            {"_id": 1, "filename": 1}
                        )
                        async for doc_record in cursor:
                            filename_map[str(doc_record["_id"])] = doc_record.get("filename", "")
                        logger.info(f"ğŸ“ ä»æ•°æ®åº“è¡¥å……äº† {len(filename_map)} ä¸ªæ–‡æ¡£çš„filename")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ‰¹é‡æŸ¥è¯¢filenameå¤±è´¥: {e}")
            
            # ğŸ†• æ ¼å¼åŒ–ç»“æœå¹¶åˆ†é…å…¨å±€åºå·
            formatted_results = []
            for idx, (doc, score) in enumerate(search_results, 1):
                # åˆ†é…å…¨å±€å”¯ä¸€åºå·ï¼ˆè·¨å¤šæ¬¡è°ƒç”¨é€’å¢ï¼‰
                global_marker = _marker_manager.get_next_marker(context.session_id)
                
                # ğŸ†• å¦‚æœmetadataä¸­filenameä¸ºç©ºï¼Œå°è¯•ä»æ•°æ®åº“æŸ¥è¯¢ç»“æœä¸­è·å–
                doc_id = doc.metadata.get("doc_id", "")
                filename = doc.metadata.get("filename") or filename_map.get(doc_id, "")
                
                formatted_results.append({
                    "index": idx,  # ä¿ç•™åŸå§‹ç´¢å¼•ï¼ˆå‘åå…¼å®¹ï¼‰
                    "ref_marker": global_marker,  # ğŸ†• å…¨å±€åºå·ï¼ˆç”¨äº##æ•°å­—$$å¼•ç”¨ï¼‰
                    "content": doc.page_content,
                    "score": float(score),
                    "metadata": {
                        "source": doc.metadata.get("source", "Unknown"),
                        "chunk_index": doc.metadata.get("chunk_index", 0),
                        "chunk_id": doc.metadata.get("chunk_id", ""),  # ğŸ¯ æ·»åŠ  chunk_id ç”¨äºå¼•ç”¨
                        "document_id": doc.metadata.get("document_id", ""),
                        # ğŸ†• æ·»åŠ æŸ¥çœ‹åŸæ–‡æ‰€éœ€çš„å­—æ®µ
                        "doc_id": doc_id,
                        "kb_id": doc.metadata.get("kb_id", ""),
                        "filename": filename
                    }
                })
                
                logger.info(f"ğŸ“Œ åˆ†é…å…¨å±€åºå· ##{ global_marker}$$: chunk_id={doc.metadata.get('chunk_id', '(ç©º)')}, source={doc.metadata.get('source', 'Unknown')}")
            
            result = {
                "success": True,
                "query": query,
                "total": len(formatted_results),
                "results": formatted_results
            }
            
            logger.info(f"âœ… çŸ¥è¯†åº“æ£€ç´¢æˆåŠŸ: query='{query}', found={len(formatted_results)} chunks")
            
            return json.dumps(result, ensure_ascii=False, indent=2)
        
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
            return json.dumps({
                "success": False,
                "error": f"æ£€ç´¢å¤±è´¥: {str(e)}",
                "results": []
            }, ensure_ascii=False)
    
    async def _build_vectorstore(self, kb_settings: dict):
        """æ„å»ºå‘é‡å­˜å‚¨ï¼ˆä½¿ç”¨å…¨å±€å•ä¾‹ç®¡ç†å™¨ï¼‰"""
        # å»¶è¿Ÿå¯¼å…¥é¿å…å¯åŠ¨æ—¶åŠ è½½
        from ...routers.kb import _get_kb_components
        
        _, vectorstore, _ = _get_kb_components(kb_settings)
        return vectorstore
    
    async def _create_retriever(self, vectorstore, kb_settings: dict, top_k: int):
        """åˆ›å»ºæ£€ç´¢å™¨"""
        from ...utils.embedding.pipeline import Retriever
        
        # ä»é…ç½®ä¸­è·å–ç›¸ä¼¼åº¦é˜ˆå€¼
        similarity_threshold = kb_settings.get("similarity_threshold", 10) if isinstance(kb_settings, dict) else 10
        
        return Retriever(
            vector_store=vectorstore,
            top_k=top_k,
            similarity_threshold=similarity_threshold
        )

