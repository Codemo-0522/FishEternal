"""
çŸ¥è¯†åº“æ£€ç´¢å·¥å…·

ä»ä¼šè¯å…³è”çš„çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
è¿™æ˜¯ FishChat çš„æ ¸å¿ƒ RAG å·¥å…·
"""
from typing import Dict, Any, List, Tuple, Optional
import json
import logging
from ..base import BaseTool, ToolMetadata, ToolContext, ToolExecutionError
from ...config import settings

logger = logging.getLogger(__name__)


# ğŸ†• å…¨å±€åºå·ç®¡ç†å™¨ï¼ˆæŒ‰ä¼šè¯ç®¡ç†ï¼Œç¡®ä¿è·¨å·¥å…·è°ƒç”¨çš„åºå·è¿ç»­ä¸”å”¯ä¸€ï¼‰
class GlobalReferenceMarkerManager:
    """å…¨å±€å¼•ç”¨åºå·ç®¡ç†å™¨ï¼ˆæŒ‰ä¼šè¯éš”ç¦»ï¼‰"""
    _instance = None
    _session_markers: Dict[str, int] = {}  # session_id -> å½“å‰åºå·
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def get_next_marker(self, session_id: str) -> int:
        """è·å–ä¸‹ä¸€ä¸ªå…¨å±€åºå·ï¼ˆä»1å¼€å§‹é€’å¢ï¼‰"""
        if session_id not in self._session_markers:
            self._session_markers[session_id] = 0
        self._session_markers[session_id] += 1
        return self._session_markers[session_id]
    
    def reset_session(self, session_id: str):
        """é‡ç½®ä¼šè¯çš„åºå·è®¡æ•°å™¨ï¼ˆæ–°ä¸€è½®å¯¹è¯å¼€å§‹æ—¶è°ƒç”¨ï¼‰"""
        self._session_markers[session_id] = 0
        logger.info(f"ğŸ”„ å·²é‡ç½®ä¼šè¯ {session_id} çš„å…¨å±€å¼•ç”¨åºå·")
    
    def get_current_marker(self, session_id: str) -> int:
        """è·å–å½“å‰ä¼šè¯çš„åºå·ï¼ˆä¸é€’å¢ï¼‰"""
        return self._session_markers.get(session_id, 0)


# å…¨å±€å•ä¾‹
_marker_manager = GlobalReferenceMarkerManager()


class KnowledgeRetrievalTool(BaseTool):
    """çŸ¥è¯†åº“æ£€ç´¢å·¥å…·"""
    
    def get_metadata(self, context: Optional[ToolContext] = None) -> Optional[ToolMetadata]:
        """
        è·å–å·¥å…·å…ƒæ•°æ®ï¼ˆåŠ¨æ€ç”Ÿæˆå‚æ•°ï¼‰
        
        Args:
            context: åŒ…å« kb_settings çš„ä¸Šä¸‹æ–‡ï¼ˆä» context.extra ä¸­è·å–ï¼‰
        
        Returns:
            ToolMetadata: å·¥å…·å…ƒæ•°æ®ï¼Œå‚æ•°æ ¹æ®ä¼šè¯é…ç½®åŠ¨æ€ç”Ÿæˆ
            None: å¦‚æœçŸ¥è¯†åº“æœªå¯ç”¨ï¼Œè¿”å› Noneï¼ˆå·¥å…·ä¸ä¼šå‡ºç°åœ¨åˆ—è¡¨ä¸­ï¼‰
        """
        # ä» context.extra ä¸­è·å– kb_settingsï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        kb_settings = None
        if context and context.extra:
            kb_settings = context.extra.get("kb_settings")
        
        # å¦‚æœæ²¡æœ‰çŸ¥è¯†åº“é…ç½®æˆ–çŸ¥è¯†åº“æœªå¯ç”¨ï¼Œè¿”å› Noneï¼ˆä¸æ˜¾ç¤ºè¯¥å·¥å…·ï¼‰
        if not kb_settings or not kb_settings.get("enabled"):
            logger.info(f"ğŸš« çŸ¥è¯†åº“å·¥å…·ä¸å¯ç”¨ - kb_settingså­˜åœ¨: {kb_settings is not None}, enabled: {kb_settings.get('enabled') if kb_settings else 'N/A'}")
            return None
        
        # åŸºç¡€ schemaï¼ˆåªåŒ…å« queryï¼Œæ¨¡å‹åªèƒ½æ§åˆ¶æŸ¥è¯¢è¯ï¼‰
        base_schema = {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "æœç´¢æŸ¥è¯¢ï¼Œæè¿°éœ€è¦æŸ¥æ‰¾çš„å†…å®¹"
                }
            },
            "required": ["query"]
        }
        
        # æ„å»ºæè¿°ä¿¡æ¯ï¼ˆæ˜¾ç¤ºå½“å‰é…ç½®ï¼‰
        top_k = kb_settings.get("top_k", 3)
        similarity_threshold = kb_settings.get("similarity_threshold", 10)
        
        description_parts = ["""
            "ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µã€‚",
            "\n\nğŸŒ å¤šè¯­è¨€æ£€ç´¢ç­–ç•¥ï¼š",
            "\nå½“æœç´¢æŠ€æœ¯æ–‡æ¡£ã€ç¼–ç¨‹æ¦‚å¿µã€é¡¹ç›®é…ç½®ã€APIæ–‡æ¡£ç­‰ä¸“ä¸šå†…å®¹æ—¶ï¼Œå¿…é¡»åŒæ—¶ä½¿ç”¨å¤šè¯­è¨€å¹¶è¡Œæ£€ç´¢ä»¥æé«˜å‡†ç¡®æ€§ï¼š",
            "\n- è¯­è¨€ä¼˜å…ˆçº§ï¼šä¸­æ–‡ â†’ è‹±æ–‡ â†’ å…¶ä»–è¯­è¨€",
            "\n- æŠ€æœ¯æœ¯è¯­ï¼ˆé«˜å¹¶å‘ã€æ•°æ®åº“ä¼˜åŒ–ç­‰ï¼‰å¿…é¡»ä¸­è‹±æ–‡å¹¶è¡Œæœç´¢",
            "\n- åŒ…å«åŒä¹‰è¯å’Œç›¸å…³æ¦‚å¿µï¼ˆå¦‚"å¹¶å‘"ä¸"å¼‚æ­¥"ï¼‰",
            "\n- æ‰€æœ‰å¹¶è¡Œæ£€ç´¢å¿…é¡»åœ¨åŒä¸€è½®æ¬¡è°ƒç”¨",
            "\n\nç¤ºä¾‹ï¼š",
            "\n- ç”¨æˆ·é—®"é«˜å¹¶å‘ã€çº¿ç¨‹é˜»å¡" â†’ å¹¶è¡Œè°ƒç”¨:",
            "\n  1. search_knowledge_base(query=\"é«˜å¹¶å‘ çº¿ç¨‹é˜»å¡ æ€§èƒ½ä¼˜åŒ–\")",
            "\n  2. search_knowledge_base(query=\"concurrency thread blocking performance\")",
            "\n  3. search_knowledge_base(query=\"å¹¶å‘ç¼–ç¨‹ å¼‚æ­¥å¤„ç† å¤šçº¿ç¨‹\")",
            "\n\nä»…æ—¥å¸¸å¯¹è¯ï¼ˆå¦‚"ä½ å¥½"ã€"å¤©æ°”"ï¼‰å¯å•è¯­è¨€æ£€ç´¢ã€‚"
            """
        ]
        
        return ToolMetadata(
            name="search_knowledge_base",
            description="".join(description_parts),
            input_schema=base_schema
        )
    
    async def execute(self, arguments: Dict[str, Any], context: ToolContext) -> str:
        """
        æ‰§è¡ŒçŸ¥è¯†åº“æ£€ç´¢
        
        Args:
            arguments: {"query": str}ï¼ˆåªæ¥å—æŸ¥è¯¢è¯ï¼Œå…¶ä»–å‚æ•°ä» kb_settings è¯»å–ï¼‰
            context: å¿…é¡»åŒ…å« session_id å’Œ db
        
        Returns:
            str: JSON æ ¼å¼çš„æ£€ç´¢ç»“æœ
        """
        # éªŒè¯ä¸Šä¸‹æ–‡
        if not context.session_id:
            raise ToolExecutionError("search_knowledge_base", "ç¼ºå°‘ session_id")
        if not context.db:
            raise ToolExecutionError("search_knowledge_base", "ç¼ºå°‘æ•°æ®åº“è¿æ¥")
        
        query = arguments.get("query", "")
        
        if not query.strip():
            return json.dumps({"success": False, "error": "æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º"}, ensure_ascii=False)
        
        try:
            # è·å–ä¼šè¯çš„çŸ¥è¯†åº“é…ç½®
            db_name = context.extra.get("db_name", settings.mongodb_db_name)
            session_data = await context.db[db_name].chat_sessions.find_one(
                {"_id": context.session_id}
            )
            
            if not session_data:
                return json.dumps({
                    "success": False,
                    "error": "ä¼šè¯ä¸å­˜åœ¨",
                    "results": []
                }, ensure_ascii=False)
            
            kb_settings = session_data.get("kb_settings")
            
            # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å¯ç”¨
            if not kb_settings or not kb_settings.get("enabled"):
                return json.dumps({
                    "success": False,
                    "error": "å½“å‰ä¼šè¯æœªå¯ç”¨çŸ¥è¯†åº“åŠŸèƒ½",
                    "results": []
                }, ensure_ascii=False)
            
            # ä» kb_settings ä¸­è¯»å–å‚æ•°ï¼ˆç”±ç”¨æˆ·é…ç½®ï¼Œæ¨¡å‹ä¸èƒ½ä¿®æ”¹ï¼‰
            top_k = kb_settings.get("top_k", 3)
            top_k = max(1, min(12, top_k))  # é™åˆ¶èŒƒå›´
            
            logger.info(f"ğŸ“‹ ä½¿ç”¨ç”¨æˆ·é…ç½®: top_k={top_k}, similarity_threshold={kb_settings.get('similarity_threshold', 10)}")
            
            # ğŸ†• æ ¹æ® kb_ids åŠ è½½çŸ¥è¯†åº“é…ç½®å¹¶æ£€ç´¢
            kb_ids = kb_settings.get("kb_ids", [])
            if not kb_ids:
                logger.warning("kb_ids ä¸ºç©ºï¼Œè·³è¿‡æ£€ç´¢")
                return json.dumps({
                    "success": True,
                    "message": "æœªé…ç½®çŸ¥è¯†åº“",
                    "results": []
                }, ensure_ascii=False)
            
            # åˆ¤æ–­å•åº“è¿˜æ˜¯å¤šåº“æ£€ç´¢
            from ...services.knowledge_base_service import KnowledgeBaseService
            kb_service = KnowledgeBaseService(context.db[db_name])
            
            if len(kb_ids) == 1:
                # å•çŸ¥è¯†åº“æ£€ç´¢
                kb = await kb_service.get_knowledge_base(kb_ids[0], context.user_id)
                if not kb:
                    logger.warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_ids[0]}")
                    return json.dumps({
                        "success": False,
                        "error": f"çŸ¥è¯†åº“ {kb_ids[0]} ä¸å­˜åœ¨æˆ–æ— æƒé™",
                        "results": []
                    }, ensure_ascii=False)
                
                # ä½¿ç”¨çŸ¥è¯†åº“è‡ªå·±çš„é…ç½®æ„å»ºvectorstore
                vectorstore = await self._build_vectorstore(kb.kb_settings)
                retriever = await self._create_retriever(vectorstore, kb.kb_settings, top_k)
                
                # æ‰§è¡Œæ£€ç´¢ï¼ˆå¼‚æ­¥è°ƒç”¨ï¼‰
                search_results = await retriever.search(query, top_k=top_k)
            else:
                # å¤šçŸ¥è¯†åº“å¹¶è¡Œæ£€ç´¢
                from ...services.multi_kb_retriever import get_multi_kb_retriever
                
                kb_configs = []
                for kb_id in kb_ids:
                    kb = await kb_service.get_knowledge_base(kb_id, context.user_id)
                    if kb:
                        kb_configs.append({
                            'kb_id': kb_id,
                            'kb_name': kb.name,
                            'kb_settings': kb.kb_settings
                        })
                
                if not kb_configs:
                    logger.warning("æ‰€æœ‰çŸ¥è¯†åº“éƒ½ä¸å­˜åœ¨æˆ–æ— æƒé™")
                    return json.dumps({
                        "success": False,
                        "error": "æ‰€æœ‰çŸ¥è¯†åº“éƒ½ä¸å­˜åœ¨æˆ–æ— æƒé™",
                        "results": []
                    }, ensure_ascii=False)
                
                # ä½¿ç”¨å¤šçŸ¥è¯†åº“æ£€ç´¢å™¨
                retriever_multi = await get_multi_kb_retriever()
                top_k_per_kb = kb_settings.get("top_k_per_kb", 3)
                final_top_k = kb_settings.get("final_top_k", 10)
                merge_strategy = kb_settings.get("merge_strategy", "weighted_score")
                similarity_threshold = kb_settings.get("similarity_threshold", 10)
                
                multi_results = await retriever_multi.retrieve_from_multiple_kbs(
                    query=query,
                    kb_configs=kb_configs,
                    top_k_per_kb=top_k_per_kb,
                    similarity_threshold=similarity_threshold,
                    merge_strategy=merge_strategy,
                    final_top_k=final_top_k
                )
                
                # å°†å¤šåº“ç»“æœè½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
                search_results = [(type('Doc', (), {
                    'page_content': r.content, 
                    'metadata': {
                        'source': r.kb_name, 
                        'chunk_id': r.chunk_id or '', 
                        'chunk_index': r.metadata.get('chunk_index', 0), 
                        'document_id': r.doc_id or '',
                        # ğŸ†• æ·»åŠ æŸ¥çœ‹åŸæ–‡æ‰€éœ€çš„å­—æ®µ
                        'doc_id': r.metadata.get('doc_id', r.doc_id or ''),
                        'kb_id': r.metadata.get('kb_id', ''),
                        'filename': r.metadata.get('filename', '')
                    }
                })(), r.distance) for r in multi_results]
            
            if not search_results:
                return json.dumps({
                    "success": True,
                    "message": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ç‰‡æ®µ",
                    "results": []
                }, ensure_ascii=False)
            
            # ğŸ†• æ”¶é›†éœ€è¦æŸ¥è¯¢çš„doc_idï¼Œç”¨äºæ‰¹é‡æŸ¥è¯¢filename
            from bson import ObjectId
            doc_ids_to_query = set()
            for doc, score in search_results:
                doc_id = doc.metadata.get("doc_id")
                filename = doc.metadata.get("filename")
                # å¦‚æœfilenameä¸ºç©ºä¸”doc_idå­˜åœ¨ï¼Œè®°å½•éœ€è¦æŸ¥è¯¢
                if doc_id and not filename:
                    doc_ids_to_query.add(doc_id)
            
            # ğŸ†• æ‰¹é‡æŸ¥è¯¢filename
            filename_map = {}
            if doc_ids_to_query:
                try:
                    doc_ids_obj = [ObjectId(doc_id) for doc_id in doc_ids_to_query if ObjectId.is_valid(doc_id)]
                    if doc_ids_obj:
                        cursor = context.db[db_name].kb_documents.find(
                            {"_id": {"$in": doc_ids_obj}},
                            {"_id": 1, "filename": 1}
                        )
                        async for doc_record in cursor:
                            filename_map[str(doc_record["_id"])] = doc_record.get("filename", "")
                        logger.info(f"ğŸ“ ä»æ•°æ®åº“è¡¥å……äº† {len(filename_map)} ä¸ªæ–‡æ¡£çš„filename")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ‰¹é‡æŸ¥è¯¢filenameå¤±è´¥: {e}")
            
            # ğŸ†• æ ¼å¼åŒ–ç»“æœå¹¶åˆ†é…å…¨å±€åºå·
            formatted_results = []
            for idx, (doc, score) in enumerate(search_results, 1):
                # åˆ†é…å…¨å±€å”¯ä¸€åºå·ï¼ˆè·¨å¤šæ¬¡è°ƒç”¨é€’å¢ï¼‰
                global_marker = _marker_manager.get_next_marker(context.session_id)
                
                # ğŸ†• å¦‚æœmetadataä¸­filenameä¸ºç©ºï¼Œå°è¯•ä»æ•°æ®åº“æŸ¥è¯¢ç»“æœä¸­è·å–
                doc_id = doc.metadata.get("doc_id", "")
                filename = doc.metadata.get("filename") or filename_map.get(doc_id, "")
                
                formatted_results.append({
                    "index": idx,  # ä¿ç•™åŸå§‹ç´¢å¼•ï¼ˆå‘åå…¼å®¹ï¼‰
                    "ref_marker": global_marker,  # ğŸ†• å…¨å±€åºå·ï¼ˆç”¨äº##æ•°å­—$$å¼•ç”¨ï¼‰
                    "content": doc.page_content,
                    "score": float(score),
                    "metadata": {
                        "source": doc.metadata.get("source", "Unknown"),
                        "chunk_index": doc.metadata.get("chunk_index", 0),
                        "chunk_id": doc.metadata.get("chunk_id", ""),  # ğŸ¯ æ·»åŠ  chunk_id ç”¨äºå¼•ç”¨
                        "document_id": doc.metadata.get("document_id", ""),
                        # ğŸ†• æ·»åŠ æŸ¥çœ‹åŸæ–‡æ‰€éœ€çš„å­—æ®µ
                        "doc_id": doc_id,
                        "kb_id": doc.metadata.get("kb_id", ""),
                        "filename": filename
                    }
                })
                
                logger.info(f"ğŸ“Œ åˆ†é…å…¨å±€åºå· ##{ global_marker}$$: chunk_id={doc.metadata.get('chunk_id', '(ç©º)')}, source={doc.metadata.get('source', 'Unknown')}")
            
            result = {
                "success": True,
                "query": query,
                "total": len(formatted_results),
                "results": formatted_results
            }
            
            logger.info(f"âœ… çŸ¥è¯†åº“æ£€ç´¢æˆåŠŸ: query='{query}', found={len(formatted_results)} chunks")
            
            return json.dumps(result, ensure_ascii=False, indent=2)
        
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
            return json.dumps({
                "success": False,
                "error": f"æ£€ç´¢å¤±è´¥: {str(e)}",
                "results": []
            }, ensure_ascii=False)
    
    async def _build_vectorstore(self, kb_settings: dict):
        """æ„å»ºå‘é‡å­˜å‚¨ï¼ˆä½¿ç”¨å…¨å±€å•ä¾‹ç®¡ç†å™¨ï¼‰"""
        # å»¶è¿Ÿå¯¼å…¥é¿å…å¯åŠ¨æ—¶åŠ è½½
        from ...routers.kb import _get_kb_components
        
        _, vectorstore, _ = _get_kb_components(kb_settings)
        return vectorstore
    
    async def _create_retriever(self, vectorstore, kb_settings: dict, top_k: int):
        """åˆ›å»ºæ£€ç´¢å™¨"""
        from ...utils.embedding.pipeline import Retriever
        
        # ä»é…ç½®ä¸­è·å–ç›¸ä¼¼åº¦é˜ˆå€¼
        similarity_threshold = kb_settings.get("similarity_threshold", 10) if isinstance(kb_settings, dict) else 10
        
        return Retriever(
            vector_store=vectorstore,
            top_k=top_k,
            similarity_threshold=similarity_threshold
        )

