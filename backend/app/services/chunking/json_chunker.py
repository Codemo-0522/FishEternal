"""
JSONåˆ†ç‰‡å™¨
ä¿æŒJSONç»“æ„å®Œæ•´æ€§çš„æ™ºèƒ½åˆ†ç‰‡
"""

from typing import List, Dict, Any, Optional
import json
import logging
from .base_chunker import BaseChunker, ChunkResult, ChunkingConfig
from .registry import register_chunker

logger = logging.getLogger(__name__)


@register_chunker("json")
class JSONChunker(BaseChunker):
    """JSONä¸“ç”¨åˆ†ç‰‡å™¨"""
    
    def can_handle(self, file_type: str, content: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºJSONæ–‡ä»¶"""
        if file_type.lower() == 'json':
            try:
                json.loads(content)
                return True
            except json.JSONDecodeError:
                return False
        return False
    
    def get_priority(self) -> int:
        """é«˜ä¼˜å…ˆçº§ï¼ˆä¸“ç”¨åˆ†ç‰‡å™¨ï¼‰"""
        return 90
    
    def chunk(self, content: str, metadata: Optional[Dict[str, Any]] = None) -> List[ChunkResult]:
        """
        æ™ºèƒ½åˆ†ç‰‡JSONæ–‡æ¡£
        
        Args:
            content: JSONå†…å®¹
            metadata: æ–‡æ¡£å…ƒæ•°æ®
            
        Returns:
            åˆ†ç‰‡ç»“æœåˆ—è¡¨
        """
        try:
            data = json.loads(content)
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing failed: {e}")
            return self.fallback_chunk(content, metadata)
        
        logger.info(f"ğŸ”ª JSONChunker å¼€å§‹åˆ†ç‰‡: chunk_size={self.config.chunk_size}, æ•°æ®ç±»å‹={type(data).__name__}")
        if isinstance(data, list):
            logger.info(f"  - JSONæ•°ç»„ï¼Œå…ƒç´ æ•°é‡: {len(data)}")
        elif isinstance(data, dict):
            logger.info(f"  - JSONå¯¹è±¡ï¼Œå­—æ®µæ•°é‡: {len(data)}")
        
        chunks = []
        
        if isinstance(data, list):
            chunks = self._chunk_array(data, metadata)
        elif isinstance(data, dict):
            chunks = self._chunk_object(data, metadata)
        else:
            # åŸºæœ¬ç±»å‹ï¼Œç›´æ¥è¿”å›
            chunk = ChunkResult(
                content=json.dumps(data, ensure_ascii=False, indent=2),
                metadata={
                    'chunker': 'json',
                    'json_type': type(data).__name__,
                    **(metadata or {})
                },
                chunk_index=0,
                completeness=True
            )
            chunks = [self.enrich_metadata(chunk, metadata)]
        
        logger.info(f"âœ… JSONChunker åˆ†ç‰‡å®Œæˆ: ç”Ÿæˆ {len(chunks)} ä¸ªåˆ†ç‰‡")
        
        return chunks
    
    def _chunk_array(self, array: list, doc_metadata: Optional[Dict[str, Any]] = None) -> List[ChunkResult]:
        """
        åˆ†ç‰‡JSONæ•°ç»„ï¼Œä¿æŒå…ƒç´ å®Œæ•´æ€§
        
        **æ ¸å¿ƒåŸåˆ™ï¼šchunk_size æ˜¯ç¡¬æ€§ä¸Šé™**
        - å¯¹è±¡ < chunk_sizeï¼šä¿æŒå¯¹è±¡å®Œæ•´æ€§ï¼ˆä¼˜å…ˆçº§ç¬¬ä¸€ï¼‰
        - å¯¹è±¡ > chunk_sizeï¼šåœ¨åˆé€‚çš„è¾¹ç•Œæˆªæ–­ï¼ˆå­—æ®µè¾¹ç•Œï¼‰
        
        Args:
            array: JSONæ•°ç»„
            doc_metadata: æ–‡æ¡£å…ƒæ•°æ®
            
        Returns:
            åˆ†ç‰‡ç»“æœåˆ—è¡¨
        """
        chunks = []
        chunk_index = 0
        
        for array_index, item in enumerate(array):
            item_str = json.dumps(item, ensure_ascii=False, indent=2)
            item_size = len(item_str)
            
            # æƒ…å†µ1: å…ƒç´ åœ¨ chunk_size èŒƒå›´å†…ï¼Œä¿æŒå®Œæ•´æ€§
            if item_size <= self.config.chunk_size:
                chunk = ChunkResult(
                    content=item_str,
                    metadata={
                        'chunker': 'json',
                        'json_type': 'array_item',
                        'array_index': array_index,
                        'oversized': False,
                        **(doc_metadata or {})
                    },
                    chunk_index=chunk_index,
                    completeness=True
                )
                chunks.append(self.enrich_metadata(chunk, doc_metadata))
                chunk_index += 1
            
            # æƒ…å†µ2: å…ƒç´ è¶…è¿‡ chunk_sizeï¼Œéœ€è¦åœ¨å­—æ®µè¾¹ç•Œæˆªæ–­
            else:
                logger.info(f"ğŸ“¦ æ•°ç»„å…ƒç´ è¶…è¿‡ chunk_size ({item_size} > {self.config.chunk_size})ï¼Œåœ¨å­—æ®µè¾¹ç•Œæˆªæ–­")
                
                if isinstance(item, dict):
                    # å¯¹è±¡ç±»å‹ï¼šæŒ‰å­—æ®µåˆ†ç‰‡
                    sub_chunks = self._chunk_large_object(item, doc_metadata, chunk_index, array_index)
                    chunks.extend(sub_chunks)
                    chunk_index += len(sub_chunks)
                elif isinstance(item, list):
                    # åµŒå¥—æ•°ç»„ï¼šé€’å½’å¤„ç†
                    sub_chunks = self._chunk_array(item, doc_metadata)
                    for sub_chunk in sub_chunks:
                        sub_chunk.metadata['parent_array_index'] = array_index
                    chunks.extend(sub_chunks)
                    chunk_index += len(sub_chunks)
                elif isinstance(item, str):
                    # å­—ç¬¦ä¸²ç±»å‹ï¼šæŒ‰å¥å­è¾¹ç•Œåˆ†ç‰‡
                    sub_chunks = self._chunk_large_string(item, doc_metadata, chunk_index, array_index)
                    chunks.extend(sub_chunks)
                    chunk_index += len(sub_chunks)
                else:
                    # å…¶ä»–åŸºæœ¬ç±»å‹ï¼ˆæ•°å­—ã€å¸ƒå°”ç­‰ï¼‰ï¼šç›´æ¥ä¿å­˜ï¼ˆé€šå¸¸ä¸ä¼šå¾ˆå¤§ï¼‰
                    chunk = ChunkResult(
                        content=item_str,
                        metadata={
                            'chunker': 'json',
                            'json_type': 'array_item',
                            'array_index': array_index,
                            'oversized': True,
                            **(doc_metadata or {})
                        },
                        chunk_index=chunk_index,
                        completeness=True
                    )
                    chunks.append(self.enrich_metadata(chunk, doc_metadata))
                    chunk_index += 1
        
        logger.info(f"âœ… æ•°ç»„åˆ†ç‰‡å®Œæˆ: {len(array)} ä¸ªå…ƒç´  â†’ {len(chunks)} ä¸ªåˆ†ç‰‡")
        return chunks
    
    def _chunk_object(
        self,
        obj: dict,
        doc_metadata: Optional[Dict[str, Any]] = None,
        start_index: int = 0
    ) -> List[ChunkResult]:
        """
        åˆ†ç‰‡JSONå¯¹è±¡ï¼Œä¿æŒé”®å€¼å¯¹å®Œæ•´æ€§
        
        Args:
            obj: JSONå¯¹è±¡
            doc_metadata: æ–‡æ¡£å…ƒæ•°æ®
            start_index: èµ·å§‹ç´¢å¼•
            
        Returns:
            åˆ†ç‰‡ç»“æœåˆ—è¡¨
        """
        chunks = []
        current_obj = {}
        current_size = 0
        chunk_index = start_index
        
        for key, value in obj.items():
            kv_str = json.dumps({key: value}, ensure_ascii=False, indent=2)
            kv_size = len(kv_str)
            
            # å•ä¸ªé”®å€¼å¯¹å°±è¶…è¿‡chunk_size
            if kv_size > self.config.chunk_size:
                # å…ˆä¿å­˜å½“å‰å¯¹è±¡
                if current_obj:
                    chunk = self._create_object_chunk(current_obj, chunk_index, doc_metadata)
                    chunks.append(chunk)
                    chunk_index += 1
                    current_obj = {}
                    current_size = 0
                
                # å¤§é”®å€¼å¯¹ç‰¹æ®Šå¤„ç†
                if isinstance(value, dict):
                    # åµŒå¥—å¯¹è±¡ï¼Œé€’å½’å¤„ç†
                    sub_chunks = self._chunk_object(value, doc_metadata, chunk_index)
                    # ä¸ºå­åˆ†ç‰‡æ·»åŠ çˆ¶é”®ä¿¡æ¯
                    for sub_chunk in sub_chunks:
                        sub_chunk.metadata['parent_key'] = key
                    chunks.extend(sub_chunks)
                    chunk_index += len(sub_chunks)
                elif isinstance(value, list):
                    # åµŒå¥—æ•°ç»„ï¼Œé€’å½’å¤„ç†
                    sub_chunks = self._chunk_array(value, doc_metadata)
                    for sub_chunk in sub_chunks:
                        sub_chunk.metadata['parent_key'] = key
                    chunks.extend(sub_chunks)
                    chunk_index += len(sub_chunks)
                else:
                    # åŸºæœ¬ç±»å‹ä½†å¾ˆå¤§ï¼Œå•ç‹¬ä¿å­˜
                    chunk = ChunkResult(
                        content=kv_str,
                        metadata={
                            'chunker': 'json',
                            'json_type': 'object_field',
                            'field_key': key,
                            'oversized': True,
                            **(doc_metadata or {})
                        },
                        chunk_index=chunk_index,
                        completeness=False
                    )
                    chunks.append(self.enrich_metadata(chunk, doc_metadata))
                    chunk_index += 1
                
                continue
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡chunk_size
            if current_size + kv_size > self.config.chunk_size and current_obj:
                # ä¿å­˜å½“å‰å¯¹è±¡
                chunk = self._create_object_chunk(current_obj, chunk_index, doc_metadata)
                chunks.append(chunk)
                chunk_index += 1
                
                current_obj = {key: value}
                current_size = kv_size
            else:
                current_obj[key] = value
                current_size += kv_size
        
        # å¤„ç†æœ€åä¸€ä¸ªå¯¹è±¡
        if current_obj:
            chunk = self._create_object_chunk(current_obj, chunk_index, doc_metadata)
            chunks.append(chunk)
        
        return chunks
    
    def _create_array_chunk(
        self,
        items: list,
        chunk_index: int,
        doc_metadata: Optional[Dict[str, Any]] = None
    ) -> ChunkResult:
        """åˆ›å»ºæ•°ç»„åˆ†ç‰‡"""
        content = json.dumps(items, ensure_ascii=False, indent=2)
        
        chunk = ChunkResult(
            content=content,
            metadata={
                'chunker': 'json',
                'json_type': 'array',
                'item_count': len(items),
                **(doc_metadata or {})
            },
            chunk_index=chunk_index,
            completeness=True
        )
        
        return self.enrich_metadata(chunk, doc_metadata)
    
    def _create_object_chunk(
        self,
        obj: dict,
        chunk_index: int,
        doc_metadata: Optional[Dict[str, Any]] = None
    ) -> ChunkResult:
        """åˆ›å»ºå¯¹è±¡åˆ†ç‰‡"""
        content = json.dumps(obj, ensure_ascii=False, indent=2)
        
        chunk = ChunkResult(
            content=content,
            metadata={
                'chunker': 'json',
                'json_type': 'object',
                'keys': list(obj.keys()),
                'key_count': len(obj),
                **(doc_metadata or {})
            },
            chunk_index=chunk_index,
            completeness=True
        )
        
        return self.enrich_metadata(chunk, doc_metadata)
    
    def _chunk_large_object(
        self,
        obj: dict,
        doc_metadata: Optional[Dict[str, Any]] = None,
        start_index: int = 0,
        array_index: Optional[int] = None
    ) -> List[ChunkResult]:
        """
        åˆ†ç‰‡è¶…å¤§JSONå¯¹è±¡ï¼Œåœ¨å­—æ®µè¾¹ç•Œæˆªæ–­
        
        ç­–ç•¥ï¼š
        1. æŒ‰å­—æ®µé€ä¸ªæ·»åŠ åˆ°å½“å‰åˆ†ç‰‡
        2. å½“ç´¯ç§¯å¤§å°æ¥è¿‘ chunk_size æ—¶ï¼Œåˆ›å»ºæ–°åˆ†ç‰‡
        3. ä¿æŒå­—æ®µå®Œæ•´æ€§ï¼ˆä¸æ‹†åˆ†å•ä¸ªå­—æ®µï¼‰
        
        Args:
            obj: è¶…å¤§JSONå¯¹è±¡
            doc_metadata: æ–‡æ¡£å…ƒæ•°æ®
            start_index: èµ·å§‹åˆ†ç‰‡ç´¢å¼•
            array_index: å¦‚æœæ˜¯æ•°ç»„å…ƒç´ ï¼Œè®°å½•å…¶åœ¨æ•°ç»„ä¸­çš„ç´¢å¼•
            
        Returns:
            åˆ†ç‰‡ç»“æœåˆ—è¡¨
        """
        chunks = []
        current_obj = {}
        current_size = 2  # èµ·å§‹å¤§å°ï¼š"{}"
        chunk_index = start_index
        total_fields = len(obj)
        
        logger.info(f"  ğŸ”ª å¼€å§‹åˆ†ç‰‡è¶…å¤§å¯¹è±¡: {total_fields} ä¸ªå­—æ®µ")
        
        for field_index, (key, value) in enumerate(obj.items()):
            # è®¡ç®—å•ä¸ªå­—æ®µçš„å¤§å°
            field_str = json.dumps({key: value}, ensure_ascii=False, indent=2)
            field_size = len(field_str)
            
            # æ£€æŸ¥å•ä¸ªå­—æ®µæ˜¯å¦è¶…è¿‡ chunk_size
            if field_size > self.config.chunk_size:
                # å…ˆä¿å­˜å½“å‰ç´¯ç§¯çš„å¯¹è±¡
                if current_obj:
                    chunk = ChunkResult(
                        content=json.dumps(current_obj, ensure_ascii=False, indent=2),
                        metadata={
                            'chunker': 'json',
                            'json_type': 'object_partial',
                            'keys': list(current_obj.keys()),
                            'array_index': array_index,
                            'part_of_large_object': True,
                            'total_fields': total_fields,
                            **(doc_metadata or {})
                        },
                        chunk_index=chunk_index,
                        completeness=False
                    )
                    chunks.append(self.enrich_metadata(chunk, doc_metadata))
                    chunk_index += 1
                    current_obj = {}
                    current_size = 2
                
                # å¤„ç†è¶…å¤§å­—æ®µ
                logger.info(f"    âš ï¸ å­—æ®µ '{key}' è¶…è¿‡ chunk_size ({field_size} > {self.config.chunk_size})")
                
                if isinstance(value, dict):
                    # åµŒå¥—å¯¹è±¡ï¼šé€’å½’åˆ†ç‰‡
                    sub_chunks = self._chunk_large_object(value, doc_metadata, chunk_index, array_index)
                    for sub_chunk in sub_chunks:
                        sub_chunk.metadata['parent_field'] = key
                    chunks.extend(sub_chunks)
                    chunk_index += len(sub_chunks)
                elif isinstance(value, list):
                    # åµŒå¥—æ•°ç»„ï¼šé€’å½’åˆ†ç‰‡
                    sub_chunks = self._chunk_array(value, doc_metadata)
                    for sub_chunk in sub_chunks:
                        sub_chunk.metadata['parent_field'] = key
                        sub_chunk.metadata['array_index'] = array_index
                    chunks.extend(sub_chunks)
                    chunk_index += len(sub_chunks)
                elif isinstance(value, str):
                    # è¶…å¤§å­—ç¬¦ä¸²ï¼šæŒ‰å¥å­è¾¹ç•Œåˆ†ç‰‡
                    sub_chunks = self._chunk_large_string(value, doc_metadata, chunk_index, array_index, key)
                    chunks.extend(sub_chunks)
                    chunk_index += len(sub_chunks)
                else:
                    # å…¶ä»–ç±»å‹ï¼ˆé€šå¸¸ä¸ä¼šå¾ˆå¤§ï¼‰ï¼šå•ç‹¬ä¿å­˜
                    chunk = ChunkResult(
                        content=field_str,
                        metadata={
                            'chunker': 'json',
                            'json_type': 'large_field',
                            'field_key': key,
                            'array_index': array_index,
                            **(doc_metadata or {})
                        },
                        chunk_index=chunk_index,
                        completeness=False
                    )
                    chunks.append(self.enrich_metadata(chunk, doc_metadata))
                    chunk_index += 1
                
                continue
            
            # æ£€æŸ¥æ·»åŠ å½“å‰å­—æ®µåæ˜¯å¦ä¼šè¶…è¿‡ chunk_size
            estimated_size = current_size + field_size + 2  # +2 for comma and newline
            
            if estimated_size > self.config.chunk_size and current_obj:
                # ä¿å­˜å½“å‰å¯¹è±¡ï¼Œå¼€å§‹æ–°åˆ†ç‰‡
                chunk = ChunkResult(
                    content=json.dumps(current_obj, ensure_ascii=False, indent=2),
                    metadata={
                        'chunker': 'json',
                        'json_type': 'object_partial',
                        'keys': list(current_obj.keys()),
                        'array_index': array_index,
                        'part_of_large_object': True,
                        'total_fields': total_fields,
                        **(doc_metadata or {})
                    },
                    chunk_index=chunk_index,
                    completeness=False
                )
                chunks.append(self.enrich_metadata(chunk, doc_metadata))
                chunk_index += 1
                
                # å¼€å§‹æ–°åˆ†ç‰‡
                current_obj = {key: value}
                current_size = field_size + 2
            else:
                # æ·»åŠ åˆ°å½“å‰å¯¹è±¡
                current_obj[key] = value
                current_size = estimated_size
        
        # ä¿å­˜æœ€åä¸€ä¸ªåˆ†ç‰‡
        if current_obj:
            chunk = ChunkResult(
                content=json.dumps(current_obj, ensure_ascii=False, indent=2),
                metadata={
                    'chunker': 'json',
                    'json_type': 'object_partial',
                    'keys': list(current_obj.keys()),
                    'array_index': array_index,
                    'part_of_large_object': True,
                    'total_fields': total_fields,
                    **(doc_metadata or {})
                },
                chunk_index=chunk_index,
                completeness=False
            )
            chunks.append(self.enrich_metadata(chunk, doc_metadata))
        
        logger.info(f"  âœ… è¶…å¤§å¯¹è±¡åˆ†ç‰‡å®Œæˆ: {total_fields} ä¸ªå­—æ®µ â†’ {len(chunks)} ä¸ªåˆ†ç‰‡")
        return chunks
    
    def _chunk_large_string(
        self,
        text: str,
        doc_metadata: Optional[Dict[str, Any]] = None,
        start_index: int = 0,
        array_index: Optional[int] = None,
        field_key: Optional[str] = None
    ) -> List[ChunkResult]:
        """
        åˆ†ç‰‡è¶…å¤§å­—ç¬¦ä¸²ï¼Œåœ¨å¥å­è¾¹ç•Œæˆªæ–­
        
        Args:
            text: è¶…å¤§å­—ç¬¦ä¸²
            doc_metadata: æ–‡æ¡£å…ƒæ•°æ®
            start_index: èµ·å§‹åˆ†ç‰‡ç´¢å¼•
            array_index: æ•°ç»„ç´¢å¼•ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
            field_key: å­—æ®µåï¼ˆå¦‚æœé€‚ç”¨ï¼‰
            
        Returns:
            åˆ†ç‰‡ç»“æœåˆ—è¡¨
        """
        chunks = []
        chunk_index = start_index
        
        # ä½¿ç”¨é…ç½®çš„åˆ†éš”ç¬¦è¿›è¡Œåˆ†ç‰‡
        separators = self.config.separators
        
        # ç®€å•å®ç°ï¼šæŒ‰ chunk_size åˆ‡åˆ†ï¼Œå°½é‡åœ¨å¥å­è¾¹ç•Œ
        current_pos = 0
        text_len = len(text)
        
        logger.info(f"  ğŸ”ª å¼€å§‹åˆ†ç‰‡è¶…å¤§å­—ç¬¦ä¸²: é•¿åº¦={text_len}")
        
        while current_pos < text_len:
            # è®¡ç®—æœ¬æ¬¡åˆ†ç‰‡çš„ç»“æŸä½ç½®
            end_pos = min(current_pos + self.config.chunk_size, text_len)
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€æ®µï¼Œå°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­
            if end_pos < text_len:
                # åœ¨ chunk_size èŒƒå›´å†…æŸ¥æ‰¾æœ€åä¸€ä¸ªå¥å­åˆ†éš”ç¬¦
                best_split = end_pos
                for sep in separators:
                    # åœ¨å½“å‰ä½ç½®å¾€å‰æŸ¥æ‰¾åˆ†éš”ç¬¦
                    last_sep = text.rfind(sep, current_pos, end_pos)
                    if last_sep > current_pos:
                        best_split = last_sep + len(sep)
                        break
                end_pos = best_split
            
            # æå–åˆ†ç‰‡å†…å®¹
            chunk_text = text[current_pos:end_pos]
            
            # åˆ›å»ºåˆ†ç‰‡
            chunk = ChunkResult(
                content=json.dumps(chunk_text, ensure_ascii=False),
                metadata={
                    'chunker': 'json',
                    'json_type': 'string_partial',
                    'field_key': field_key,
                    'array_index': array_index,
                    'string_part': f"{chunk_index - start_index + 1}",
                    'char_range': f"{current_pos}-{end_pos}",
                    **(doc_metadata or {})
                },
                chunk_index=chunk_index,
                completeness=False
            )
            chunks.append(self.enrich_metadata(chunk, doc_metadata))
            
            current_pos = end_pos
            chunk_index += 1
        
        logger.info(f"  âœ… è¶…å¤§å­—ç¬¦ä¸²åˆ†ç‰‡å®Œæˆ: {text_len} å­—ç¬¦ â†’ {len(chunks)} ä¸ªåˆ†ç‰‡")
        return chunks

