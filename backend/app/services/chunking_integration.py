"""
æ™ºèƒ½åˆ†ç‰‡ç³»ç»Ÿé›†æˆæ¨¡å—
å°†æ–°çš„æ™ºèƒ½åˆ†ç‰‡ç³»ç»Ÿé›†æˆåˆ°ç°æœ‰çš„æ–‡æ¡£å¤„ç†æµç¨‹ä¸­
"""

import logging
from typing import List, Dict, Any, Optional
import asyncio

from .chunking import (
    ChunkingConfig,
    ChunkingStrategy,
    ChunkerFactory
)
from .chunking.async_processor import (
    AsyncChunkingProcessor,
    ChunkingTask,
    ExecutorType
)

logger = logging.getLogger(__name__)


class ChunkingIntegration:
    """æ™ºèƒ½åˆ†ç‰‡é›†æˆæœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–é›†æˆæœåŠ¡"""
        self.processor = None
    
    def _get_processor(self) -> AsyncChunkingProcessor:
        """è·å–æˆ–åˆ›å»ºå¼‚æ­¥å¤„ç†å™¨"""
        if self.processor is None:
            self.processor = AsyncChunkingProcessor(
                max_workers=4,
                executor_type=ExecutorType.THREAD
            )
        return self.processor
    
    async def chunk_text_smart(
        self,
        text: str,
        filename: str,
        kb_settings: Dict[str, Any]
    ) -> List[str]:
        """
        æ™ºèƒ½åˆ†ç‰‡æ–‡æœ¬ï¼ˆå…¼å®¹ç°æœ‰æ¥å£ï¼‰
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            filename: æ–‡ä»¶åï¼ˆç”¨äºæ£€æµ‹æ–‡ä»¶ç±»å‹ï¼‰
            kb_settings: çŸ¥è¯†åº“é…ç½®
            
        Returns:
            åˆ†ç‰‡æ–‡æœ¬åˆ—è¡¨
        """
        try:
            # ä»kb_settingsä¸­æå–åˆ†ç‰‡é…ç½®
            config = self._build_config_from_kb_settings(kb_settings)
            
            # æ£€æµ‹æ–‡ä»¶ç±»å‹
            file_type = self._detect_file_type(filename)
            
            # ä½¿ç”¨å¼‚æ­¥å¤„ç†å™¨è¿›è¡Œåˆ†ç‰‡
            loop = asyncio.get_event_loop()
            
            def _chunk():
                # åˆ›å»ºåˆ†ç‰‡å™¨
                chunker = ChunkerFactory.create_chunker(
                    file_type=file_type,
                    content=text,
                    config=config
                )
                
                # æ‰§è¡Œåˆ†ç‰‡
                chunks = chunker.chunk(text, metadata={'filename': filename})
                
                # è¿”å›åˆ†ç‰‡æ–‡æœ¬åˆ—è¡¨
                return [chunk.content for chunk in chunks]
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼ˆé¿å…é˜»å¡ï¼‰
            chunks = await loop.run_in_executor(None, _chunk)
            
            logger.info(f"âœ… æ™ºèƒ½åˆ†ç‰‡å®Œæˆ: {filename}, ç”Ÿæˆ {len(chunks)} ä¸ªåˆ†ç‰‡")
            
            return chunks
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½åˆ†ç‰‡å¤±è´¥: {e}, é™çº§åˆ°ä¼ ç»Ÿåˆ†ç‰‡", exc_info=True)
            # é™çº§åˆ°ä¼ ç»Ÿåˆ†ç‰‡æ–¹æ³•
            return await self._fallback_chunk(text, kb_settings)
    
    def _build_config_from_kb_settings(self, kb_settings: Dict[str, Any]) -> ChunkingConfig:
        """
        ä»çŸ¥è¯†åº“é…ç½®æ„å»ºåˆ†ç‰‡é…ç½®
        
        Args:
            kb_settings: çŸ¥è¯†åº“é…ç½®
            
        Returns:
            åˆ†ç‰‡é…ç½®å¯¹è±¡
        """
        sp = kb_settings.get("split_params", {})
        
        # æå–åˆ†ç‰‡ç­–ç•¥ï¼ˆå¦‚æœæœ‰ï¼‰
        strategy_str = sp.get("chunking_strategy", sp.get("strategy", "document_aware"))
        try:
            strategy = ChunkingStrategy(strategy_str)
        except ValueError:
            logger.warning(f"Unknown chunking strategy: {strategy_str}, using document_aware")
            strategy = ChunkingStrategy.DOCUMENT_AWARE
        
        # æ„å»ºé…ç½®
        chunk_size = int(sp.get("chunk_size", 1024))
        chunk_overlap = int(sp.get("chunk_overlap", 100))
        
        config = ChunkingConfig(
            strategy=strategy,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=sp.get("separators", ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]),
            use_sentence_boundary=sp.get("use_sentence_boundary", True),
            semantic_threshold=float(sp.get("semantic_threshold", 0.5)),
            preserve_structure=sp.get("preserve_structure", True),
            ast_parsing=sp.get("ast_parsing", True),
            enable_hierarchy=sp.get("enable_hierarchy", False),
            parent_chunk_size=int(sp.get("parent_chunk_size", 4096)),
            max_workers=int(sp.get("max_workers", 4)),
            batch_size=int(sp.get("batch_size", 100))
        )
        
        logger.info(f"ğŸ“‹ åˆ†ç‰‡é…ç½®: strategy={strategy}, chunk_size={chunk_size}, chunk_overlap={chunk_overlap}")
        
        return config
    
    def _detect_file_type(self, filename: str) -> str:
        """
        ä»æ–‡ä»¶åæ£€æµ‹æ–‡ä»¶ç±»å‹
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            æ–‡ä»¶ç±»å‹ï¼ˆæ‰©å±•åï¼‰
        """
        if '.' not in filename:
            return 'txt'
        
        return filename.rsplit('.', 1)[-1].lower()
    
    async def _fallback_chunk(self, text: str, kb_settings: Dict[str, Any]) -> List[str]:
        """
        é™çº§åˆ†ç‰‡æ–¹æ³•ï¼ˆä½¿ç”¨ä¼ ç»Ÿçš„RecursiveCharacterTextSplitterï¼‰
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            kb_settings: çŸ¥è¯†åº“é…ç½®
            
        Returns:
            åˆ†ç‰‡æ–‡æœ¬åˆ—è¡¨
        """
        loop = asyncio.get_event_loop()
        
        def _chunk():
            from langchain.text_splitter import RecursiveCharacterTextSplitter
            
            sp = kb_settings.get("split_params", {})
            chunk_size = int(sp.get("chunk_size", 1024))
            chunk_overlap = int(sp.get("chunk_overlap", 100))
            separators = sp.get("separators", ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""])
            
            # ç¡®ä¿åˆ†éš”ç¬¦åˆ—è¡¨æœ«å°¾æœ‰ç©ºå­—ç¬¦ä¸²
            if isinstance(separators, list):
                separators = list(separators)
                if "" not in separators:
                    separators.append("")
            else:
                separators = ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
            
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=separators,
                length_function=len
            )
            
            return splitter.split_text(text)
        
        return await loop.run_in_executor(None, _chunk)
    
    async def batch_chunk_documents(
        self,
        documents: List[Dict[str, Any]],
        kb_settings: Dict[str, Any],
        progress_callback: Optional[callable] = None
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡åˆ†ç‰‡å¤šä¸ªæ–‡æ¡£ï¼ˆå¹¶å‘å¤„ç†ï¼‰
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ [{"content": str, "filename": str, "doc_id": str}, ...]
            kb_settings: çŸ¥è¯†åº“é…ç½®
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (completed, total)
            
        Returns:
            åˆ†ç‰‡ç»“æœåˆ—è¡¨ [{"doc_id": str, "chunks": List[str], "success": bool}, ...]
        """
        if not documents:
            return []
        
        # æ„å»ºé…ç½®
        config = self._build_config_from_kb_settings(kb_settings)
        
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = []
        for doc in documents:
            file_type = self._detect_file_type(doc['filename'])
            task = ChunkingTask(
                task_id=doc['doc_id'],
                content=doc['content'],
                file_type=file_type,
                config=config,
                metadata={'filename': doc['filename'], 'doc_id': doc['doc_id']}
            )
            tasks.append(task)
        
        # ä½¿ç”¨å¼‚æ­¥å¤„ç†å™¨æ‰¹é‡å¤„ç†
        processor = self._get_processor()
        
        def _progress_callback(completed, total):
            if progress_callback:
                progress_callback(completed, total)
        
        results = processor.process_batch(tasks, _progress_callback)
        
        # è½¬æ¢ç»“æœæ ¼å¼
        output = []
        for result in results:
            output.append({
                'doc_id': result.task_id,
                'chunks': [chunk.content for chunk in result.chunks] if result.success else [],
                'success': result.success,
                'error': result.error,
                'chunk_count': len(result.chunks) if result.success else 0,
                'duration': result.duration
            })
        
        return output
    
    def shutdown(self):
        """å…³é—­å¤„ç†å™¨"""
        if self.processor:
            self.processor.shutdown()
            self.processor = None


# å…¨å±€å•ä¾‹
_integration_instance: Optional[ChunkingIntegration] = None


def get_chunking_integration() -> ChunkingIntegration:
    """
    è·å–å…¨å±€æ™ºèƒ½åˆ†ç‰‡é›†æˆæœåŠ¡å®ä¾‹
    
    Returns:
        ChunkingIntegrationå®ä¾‹
    """
    global _integration_instance
    
    if _integration_instance is None:
        _integration_instance = ChunkingIntegration()
    
    return _integration_instance

