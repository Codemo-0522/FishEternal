"""
æ–‡æ¡£å¤„ç†æœåŠ¡ - å¼‚æ­¥éé˜»å¡ç‰ˆæœ¬

ç‰¹æ€§ï¼š
1. å®Œå…¨å¼‚æ­¥å¤„ç†ï¼Œä¸é˜»å¡ä¸»æœåŠ¡
2. ä½¿ç”¨åå°ä»»åŠ¡é˜Ÿåˆ—
3. ç”¨æˆ·æ“ä½œå®Œå…¨éš”ç¦»
4. æ”¯æŒå¹¶å‘æ–‡æ¡£å¤„ç†
5. è‡ªåŠ¨é”™è¯¯æ¢å¤
"""
import logging
import asyncio
from typing import Optional, Dict, Any
from pathlib import Path
from datetime import datetime
from bson import ObjectId

from motor.motor_asyncio import AsyncIOMotorDatabase

from ..config import settings
from .async_task_processor import get_task_processor, TaskPriority

logger = logging.getLogger(__name__)


class DocumentProcessor:
    """
    æ–‡æ¡£å¤„ç†æœåŠ¡
    
    è´Ÿè´£æ–‡æ¡£çš„è§£æã€åˆ†å—ã€å‘é‡åŒ–ï¼Œå…¨ç¨‹å¼‚æ­¥éé˜»å¡
    """
    
    def __init__(self, db: AsyncIOMotorDatabase):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†æœåŠ¡
        
        Args:
            db: MongoDB æ•°æ®åº“å®ä¾‹
        """
        self.db = db
        self.task_processor = get_task_processor()
        
        # é™æµå™¨ï¼ˆé˜²æ­¢å•ä¸ªç”¨æˆ·æäº¤è¿‡å¤šä»»åŠ¡ï¼‰
        self.user_rate_limits: Dict[str, asyncio.Semaphore] = {}
        self.max_user_concurrent_tasks = 5  # æ¯ä¸ªç”¨æˆ·æœ€å¤šåŒæ—¶å¤„ç†5ä¸ªæ–‡æ¡£
    
    async def submit_document_processing(
        self,
        kb_id: str,
        doc_id: str,
        user_id: str,
        file_path: str,
        filename: str,
        kb_settings: Dict[str, Any],
        priority: TaskPriority = TaskPriority.NORMAL
    ) -> str:
        """
        æäº¤æ–‡æ¡£å¤„ç†ä»»åŠ¡ï¼ˆå¼‚æ­¥ã€éé˜»å¡ï¼‰
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            doc_id: æ–‡æ¡£ID
            user_id: ç”¨æˆ·ID
            file_path: æ–‡ä»¶è·¯å¾„
            filename: æ–‡ä»¶å
            kb_settings: çŸ¥è¯†åº“é…ç½®
            priority: ä»»åŠ¡ä¼˜å…ˆçº§
            
        Returns:
            ä»»åŠ¡ID
            
        Raises:
            RuntimeError: æäº¤å¤±è´¥
        """
        try:
            # ç”¨æˆ·çº§åˆ«çš„é™æµï¼ˆä¸åœ¨è¿™é‡Œåšé™åˆ¶æ£€æŸ¥ï¼Œè€Œæ˜¯åœ¨å®é™…å¤„ç†æ—¶é€šè¿‡ semaphore æ’é˜Ÿï¼‰
            if user_id not in self.user_rate_limits:
                self.user_rate_limits[user_id] = asyncio.Semaphore(
                    self.max_user_concurrent_tasks
                )
            
            # æäº¤ä»»åŠ¡åˆ°å¼‚æ­¥é˜Ÿåˆ—
            # æ³¨æ„ï¼šä½¿ç”¨åŒ…è£…å‡½æ•°é¿å… user_id å‚æ•°åå†²çª
            async def _handler_wrapper(**kwargs):
                return await self._process_document_async(
                    kb_id=kwargs['kb_id'],
                    doc_id=kwargs['doc_id'],
                    user_id=kwargs['task_user_id'],  # ä½¿ç”¨é‡å‘½ååçš„å‚æ•°
                    file_path=kwargs['file_path'],
                    filename=kwargs['filename'],
                    kb_settings=kwargs['kb_settings']
                )
            
            task_id = await self.task_processor.submit_task(
                task_type="document_processing",
                user_id=user_id,
                handler=_handler_wrapper,
                kb_id=kb_id,
                doc_id=doc_id,
                task_user_id=user_id,  # é‡å‘½åä»¥é¿å…ä¸ submit_task çš„ user_id å†²çª
                file_path=file_path,
                filename=filename,
                kb_settings=kb_settings,
                priority=priority
            )
            
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå¤„ç†ä¸­
            await self.db.kb_documents.update_one(
                {"_id": ObjectId(doc_id)},
                {
                    "$set": {
                        "status": "processing",
                        "task_id": task_id,
                        "updated_at": datetime.utcnow().isoformat()
                    }
                }
            )
            
            logger.info(
                f"æ–‡æ¡£å¤„ç†ä»»åŠ¡å·²æäº¤: doc_id={doc_id}, task_id={task_id}, "
                f"user_id={user_id}"
            )
            
            return task_id
            
        except Exception as e:
            logger.error(f"æäº¤æ–‡æ¡£å¤„ç†ä»»åŠ¡å¤±è´¥: {str(e)}")
            raise
    
    async def _process_document_async(
        self,
        kb_id: str,
        doc_id: str,
        user_id: str,
        file_path: str,
        filename: str,
        kb_settings: Dict[str, Any]
    ):
        """
        å¼‚æ­¥å¤„ç†æ–‡æ¡£ï¼ˆåœ¨åå°ä»»åŠ¡ä¸­æ‰§è¡Œï¼‰
        
        è¿™ä¸ªæ–¹æ³•ä¼šåœ¨ç‹¬ç«‹çš„åç¨‹ä¸­æ‰§è¡Œï¼Œä¸ä¼šé˜»å¡ä¸»æœåŠ¡
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            doc_id: æ–‡æ¡£ID
            user_id: ç”¨æˆ·ID
            file_path: æ–‡ä»¶è·¯å¾„
            filename: æ–‡ä»¶å
            kb_settings: çŸ¥è¯†åº“é…ç½®
        """
        # è·å–ç”¨æˆ·çš„é™æµå™¨
        semaphore = self.user_rate_limits.get(user_id)
        
        # ğŸ¯ è·å–å½“å‰ä»»åŠ¡çš„ task_idï¼ˆä»æ•°æ®åº“è¯»å–ï¼‰
        task_id = None
        try:
            doc_record = await self.db.kb_documents.find_one({"_id": ObjectId(doc_id)})
            if doc_record:
                task_id = doc_record.get("task_id")
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•è·å– task_id: {e}")
        
        async def update_progress(progress: float, message: str = ""):
            """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
            if task_id:
                try:
                    await self.task_processor.update_task_progress(task_id, progress, message)
                except Exception as e:
                    logger.warning(f"âš ï¸ æ›´æ–°è¿›åº¦å¤±è´¥: {e}")
        
        try:
            # åº”ç”¨ç”¨æˆ·çº§é™æµ
            async with semaphore if semaphore else asyncio.Semaphore(1):
                logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {filename} (doc_id: {doc_id})")
                await update_progress(0.1, "å¼€å§‹å¤„ç†æ–‡æ¡£...")
                
                # æ­¥éª¤1: è¯»å–æ–‡ä»¶
                file_content = await self._read_file_async(file_path)
                await update_progress(0.2, "è¯»å–æ–‡ä»¶å®Œæˆ")
                
                # æ­¥éª¤2: è§£ææ–‡æ¡£
                text_content = await self._parse_document_async(
                    file_content, filename
                )
                await update_progress(0.4, "æ–‡æ¡£è§£æå®Œæˆ")
                
                # æ­¥éª¤3: æ–‡æœ¬åˆ†å—ï¼ˆä½¿ç”¨æ™ºèƒ½åˆ†ç‰‡ç³»ç»Ÿï¼‰
                chunks = await self._chunk_text_async(
                    text_content, kb_settings, filename
                )
                await update_progress(0.5, f"æ–‡æœ¬åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªåˆ†å—")
                logger.info(f"æ–‡æ¡£åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªåˆ†å—")
                
                # æ­¥éª¤4: å‘é‡åŒ–å¹¶å­˜å‚¨ï¼ˆå¼‚æ­¥æ‰¹é‡å¤„ç†ï¼‰
                await self._embed_and_store_async(
                    kb_id, doc_id, chunks, kb_settings, filename, update_progress
                )
                await update_progress(0.9, "å‘é‡åŒ–å­˜å‚¨å®Œæˆ")
                
                # æ­¥éª¤4.5: ğŸ”¥ ã€å·²åºŸå¼ƒã€‘å•æ–‡æ¡£æŒä¹…åŒ–æ£€æŸ¥
                # æ”¹ä¸ºï¼šä»…åœ¨æœ€ç»ˆå®Œæˆæ—¶å…¨å±€æ£€æŸ¥ï¼ˆé¿å…é‡å¤ç­‰å¾…ï¼‰
                # await self._final_persistence_check(kb_id, kb_settings)
                
                # æ­¥éª¤5: æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå®Œæˆ
                await self._update_document_completed(
                    doc_id, len(chunks)
                )
                await update_progress(1.0, "æ–‡æ¡£å¤„ç†å®Œæˆ")
                
                logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {filename} (doc_id: {doc_id})")
                
        except asyncio.CancelledError:
            logger.info(f"æ–‡æ¡£å¤„ç†è¢«å–æ¶ˆ: {doc_id}")
            await update_progress(0.0, "ä»»åŠ¡è¢«å–æ¶ˆ")
            await self._update_document_failed(
                doc_id, "ä»»åŠ¡è¢«å–æ¶ˆ"
            )
            raise
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {doc_id}, é”™è¯¯: {str(e)}", exc_info=True)
            await update_progress(0.0, f"å¤„ç†å¤±è´¥: {str(e)}")
            await self._update_document_failed(
                doc_id, str(e)
            )
            raise
    
    async def _read_file_async(self, file_path: str) -> bytes:
        """
        å¼‚æ­¥è¯»å–æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶å†…å®¹ï¼ˆå­—èŠ‚ï¼‰
        """
        # ä½¿ç”¨ asyncio çš„å¼‚æ­¥æ–‡ä»¶æ“ä½œ
        loop = asyncio.get_event_loop()
        
        def _read():
            with open(file_path, 'rb') as f:
                return f.read()
        
        return await loop.run_in_executor(None, _read)
    
    async def _parse_document_async(
        self, content: bytes, filename: str
    ) -> str:
        """
        å¼‚æ­¥è§£ææ–‡æ¡£ï¼ˆæå–æ–‡æœ¬ï¼‰
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            filename: æ–‡ä»¶å
            
        Returns:
            æå–çš„æ–‡æœ¬
        """
        # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¯åŠ¨æ—¶åŠ è½½
        loop = asyncio.get_event_loop()
        
        def _parse():
            from ..services.document_upload_service import DocumentUploadService
            service = DocumentUploadService()
            
            # ä¸´æ—¶å†™å…¥æ–‡ä»¶ä¾›è§£æå™¨ä½¿ç”¨
            import tempfile
            with tempfile.NamedTemporaryFile(
                delete=False, 
                suffix=Path(filename).suffix
            ) as tmp:
                tmp.write(content)
                tmp_path = tmp.name
            
            try:
                # åŒæ­¥è§£æ
                success, text, error = asyncio.run(
                    service.parse_document(content, filename)
                )
                if not success:
                    raise RuntimeError(error or "æ–‡æ¡£è§£æå¤±è´¥")
                return text
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                import os
                try:
                    os.unlink(tmp_path)
                except:
                    pass
        
        return await loop.run_in_executor(None, _parse)
    
    async def _chunk_text_async(
        self, text: str, kb_settings: Dict[str, Any], filename: str = "unknown"
    ) -> list:
        """
        å¼‚æ­¥æ–‡æœ¬åˆ†å—ï¼ˆä½¿ç”¨æ™ºèƒ½åˆ†ç‰‡ç³»ç»Ÿï¼‰
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            kb_settings: çŸ¥è¯†åº“é…ç½®
            filename: æ–‡ä»¶åï¼ˆç”¨äºæ£€æµ‹æ–‡ä»¶ç±»å‹ï¼‰
            
        Returns:
            åˆ†å—åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨æ–°çš„æ™ºèƒ½åˆ†ç‰‡ç³»ç»Ÿ
            from .chunking_integration import ChunkingIntegration
            
            chunking_service = ChunkingIntegration()
            chunks = await chunking_service.chunk_text_smart(
                text=text,
                filename=filename,
                kb_settings=kb_settings
            )
            
            logger.info(f"âœ… ä½¿ç”¨æ™ºèƒ½åˆ†ç‰‡ç³»ç»Ÿå®Œæˆåˆ†å—: {len(chunks)} ä¸ªåˆ†ç‰‡")
            return chunks
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½åˆ†ç‰‡å¤±è´¥ï¼Œé™çº§åˆ°ä¼ ç»Ÿåˆ†ç‰‡: {e}", exc_info=True)
            
            # é™çº§ï¼šä½¿ç”¨ä¼ ç»Ÿåˆ†ç‰‡æ–¹æ³•
            loop = asyncio.get_event_loop()
            
            def _chunk():
                from langchain.text_splitter import RecursiveCharacterTextSplitter
                
                sp = kb_settings.get("split_params", {})
                chunk_size = int(sp.get("chunk_size", 500))
                chunk_overlap = int(sp.get("chunk_overlap", 50))
                separators = sp.get("separators", ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""])
                
                if isinstance(separators, list):
                    separators = list(separators)
                    if "" not in separators:
                        separators.append("")
                else:
                    separators = ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
                
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap,
                    separators=separators,
                    length_function=len
                )
                
                return splitter.split_text(text)
            
            return await loop.run_in_executor(None, _chunk)
    
    async def _embed_and_store_async(
        self,
        kb_id: str,
        doc_id: str,
        chunks: list,
        kb_settings: Dict[str, Any],
        filename: str = None,
        progress_callback = None
    ):
        """
        å¼‚æ­¥å‘é‡åŒ–å¹¶å­˜å‚¨
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            doc_id: æ–‡æ¡£ID
            chunks: åˆ†å—åˆ—è¡¨
            kb_settings: çŸ¥è¯†åº“é…ç½®
            filename: æ–‡æ¡£æ–‡ä»¶åï¼ˆç”¨äºåœ¨metadataä¸­å­˜å‚¨ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        """
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å‘é‡å­˜å‚¨æœåŠ¡
        # ä¸ºäº†é¿å…é˜»å¡ï¼Œä½¿ç”¨æ‰¹å¤„ç†
        
        # ğŸ”¥ æ‰¹å¤„ç†ç­–ç•¥ï¼šæ¯æ‰¹100ä¸ªåˆ†å—
        # ChromaDB çš„ HNSW ç´¢å¼•æ„å»ºå’ŒæŒä¹…åŒ–åœ¨ VectorStoreWithLock ä¸­å·²å¤„ç†
        # æ–‡ä»¶é”å†…éƒ¨ä¼šæ ¹æ®æ‰¹æ¬¡å¤§å°è‡ªåŠ¨ç­‰å¾…ï¼ˆ3-5ç§’ï¼‰ï¼Œç¡®ä¿ç´¢å¼•å®Œæ•´å†™å…¥ç£ç›˜
        batch_size = 100  # æ¯æ‰¹å¤„ç†100ä¸ªåˆ†å—
        total_batches = (len(chunks) + batch_size - 1) // batch_size
        
        for batch_idx, i in enumerate(range(0, len(chunks), batch_size)):
            batch = chunks[i:i + batch_size]
            
            # å¼‚æ­¥å¤„ç†ä¸€æ‰¹ï¼ˆå†…éƒ¨ä½¿ç”¨æ–‡ä»¶é”ä¿æŠ¤ï¼Œè‡ªåŠ¨ç­‰å¾…ç´¢å¼•æŒä¹…åŒ–ï¼‰
            await self._process_chunk_batch(
                kb_id, doc_id, batch, i, kb_settings, filename
            )
            
            # æ›´æ–°è¿›åº¦ï¼ˆ0.5-0.9ä¹‹é—´ï¼‰
            if progress_callback:
                batch_progress = 0.5 + (batch_idx + 1) / total_batches * 0.4
                await progress_callback(batch_progress, f"å‘é‡åŒ–è¿›åº¦: {batch_idx + 1}/{total_batches} æ‰¹æ¬¡")
    
    async def _process_chunk_batch(
        self,
        kb_id: str,
        doc_id: str,
        chunks: list,
        start_idx: int,
        kb_settings: Dict[str, Any],
        filename: str = None
    ):
        """
        å¤„ç†ä¸€æ‰¹åˆ†å—
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            doc_id: æ–‡æ¡£ID
            chunks: åˆ†å—åˆ—è¡¨
            start_idx: èµ·å§‹ç´¢å¼•
            kb_settings: çŸ¥è¯†åº“é…ç½®
        """
        from langchain_core.documents import Document
        from ..services.embedding_manager import get_embedding_manager
        from ..services.vectorstore_manager import get_vectorstore_manager
        from ..config import settings
        from ..utils.embedding.path_utils import (
            build_chroma_persist_dir, get_chroma_collection_name,
            build_faiss_persist_dir, get_faiss_collection_name
        )
        
        # è·å–å‘é‡æ•°æ®åº“ç±»å‹
        vector_db_type = kb_settings.get("vector_db", "chroma")
        
        # è·å–çŸ¥è¯†åº“é…ç½®ï¼ˆä½¿ç”¨å·¥å…·å‡½æ•°å¤„ç†collectionåç§°å’ŒæŒä¹…åŒ–ç›®å½•ï¼‰
        collection_name_raw = kb_settings.get('collection_name', 'default')
        
        if vector_db_type == "chroma":
            collection_name = get_chroma_collection_name(collection_name_raw)
            persist_dir = build_chroma_persist_dir(collection_name_raw)
        elif vector_db_type == "faiss":
            collection_name = get_faiss_collection_name(collection_name_raw)
            persist_dir = build_faiss_persist_dir(collection_name_raw)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å‘é‡æ•°æ®åº“ç±»å‹: {vector_db_type}")
        
        # è·å– Embedding ç®¡ç†å™¨å’Œ VectorStore ç®¡ç†å™¨
        embedding_mgr = get_embedding_manager()
        vectorstore_mgr = get_vectorstore_manager()
        
        # è·å– embedding é…ç½®ï¼ˆä»çŸ¥è¯†åº“åˆ›å»ºæ—¶ä¿å­˜çš„é…ç½®ä¸­è¯»å–ï¼‰
        embeddings_config = kb_settings.get("embeddings") or {}
        provider = embeddings_config.get("provider", "ollama")
        model = embeddings_config.get("model")
        base_url = embeddings_config.get("base_url")
        api_key = embeddings_config.get("api_key")
        local_model_path = embeddings_config.get("local_model_path")
        
        # ä½¿ç”¨ EmbeddingManager çš„ get_or_create æ–¹æ³•ï¼ˆä¸ kb.py çš„ _get_kb_components å®Œå…¨ä¸€è‡´ï¼‰
        embedding_func = embedding_mgr.get_or_create(
            provider=provider,
            model=model,
            base_url=base_url,
            api_key=api_key,
            local_model_path=local_model_path,
            max_length=512,
            batch_size=8,
            normalize=True
        )
        
        # è·å–æœç´¢å‚æ•°ï¼ˆåŒ…å«è·ç¦»åº¦é‡ï¼‰
        search_params = kb_settings.get("search_params") or {}
        distance_metric = search_params.get("distance_metric", "cosine")
        
        # è·å– VectorStore å®ä¾‹
        vectorstore = vectorstore_mgr.get_or_create(
            collection_name=collection_name,
            persist_dir=persist_dir,
            embedding_function=embedding_func,
            vector_db_type=vector_db_type,
            distance_metric=distance_metric  # ğŸ¯ ä¼ é€’è·ç¦»åº¦é‡å‚æ•°
        )
        
        # å‡†å¤‡æ–‡æ¡£ï¼ˆä½¿ç”¨ langchain Document å¯¹è±¡ï¼‰
        docs = []
        chunk_ids = []
        
        for idx, chunk_text in enumerate(chunks):
            # ç”Ÿæˆç¨³å®šçš„ chunk_idï¼ˆä½¿ç”¨ UUID è€Œä¸æ˜¯ MD5ï¼‰
            import uuid
            chunk_id = str(uuid.uuid4())
            chunk_ids.append(chunk_id)
            
            # åˆ›å»º Document å¯¹è±¡
            doc = Document(
                page_content=chunk_text,
                metadata={
                    "kb_id": kb_id,
                    "doc_id": doc_id,
                    "chunk_id": chunk_id,
                    "chunk_index": start_idx + idx,
                    "source": f"{doc_id}_{start_idx + idx}",
                    "filename": filename  # æ·»åŠ æ–‡ä»¶ååˆ°å…ƒæ•°æ®
                }
            )
            docs.append(doc)
        
        # ğŸ”’ ä½¿ç”¨å¸¦é”çš„å¼‚æ­¥æ–¹æ³•ï¼Œé˜²æ­¢å¹¶å‘å†™å…¥å¯¼è‡´ç´¢å¼•æŸå
        await vectorstore.add_documents_async(docs, ids=chunk_ids)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ¯ä¸ªæ–‡æ¡£å†™å…¥åï¼Œè§¦å‘å…¨å±€æŒä¹…åŒ–æ£€æŸ¥
        # ä½¿ç”¨æ™ºèƒ½å»é‡ï¼Œé¿å…çŸ­æ—¶é—´å†…é‡å¤ç­‰å¾…
        collection_name = kb_settings.get("collection_name")
        if collection_name:
            try:
                from .vectorstore_manager import get_vectorstore_manager
                vectorstore_mgr = get_vectorstore_manager()
                
                # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(
                    None,
                    self._trigger_global_persistence_if_needed,
                    vectorstore_mgr,
                    collection_name,
                    kb_id
                )
            except Exception as e:
                logger.warning(f"âš ï¸ å…¨å±€æŒä¹…åŒ–æ£€æŸ¥å¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")
    
    def _trigger_global_persistence_if_needed(self, vectorstore_mgr, collection_name: str, kb_id: str):
        """
        åŒæ­¥æ–¹æ³•ï¼šæ™ºèƒ½è§¦å‘å…¨å±€æŒä¹…åŒ–ï¼ˆé¿å…é‡å¤ï¼‰
        
        åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œä¸é˜»å¡äº‹ä»¶å¾ªç¯
        """
        import time
        
        # ä½¿ç”¨ç±»çº§åˆ«çš„å­—å…¸è®°å½•æœ€åæŒä¹…åŒ–æ—¶é—´
        if not hasattr(self.__class__, '_last_global_persistence'):
            self.__class__._last_global_persistence = {}
        
        current_time = time.time()
        last_time = self.__class__._last_global_persistence.get(kb_id, 0)
        
        # å¦‚æœè·ç¦»ä¸Šæ¬¡æŒä¹…åŒ–ä¸è¶³60ç§’ï¼Œè·³è¿‡
        if current_time - last_time < 60:
            logger.debug(f"â­ï¸ è·ç¦»ä¸Šæ¬¡å…¨å±€æŒä¹…åŒ–ä¸è¶³60ç§’ï¼Œè·³è¿‡ (kb_id: {kb_id})")
            return
        
        # æ›´æ–°æ—¶é—´æˆ³
        self.__class__._last_global_persistence[kb_id] = current_time
        
        # æ‰§è¡Œå…¨å±€æŒä¹…åŒ–
        vectorstore_mgr.force_global_compaction_wait(collection_name)
    
    async def _global_persistence_check_if_needed(self, kb_id: str, kb_settings: Dict[str, Any]):
        """
        ğŸ”¥ å…¨å±€æŒä¹…åŒ–æ£€æŸ¥ï¼ˆæ™ºèƒ½ç‰ˆï¼‰
        
        **é—®é¢˜åˆ†æ**ï¼š
        æ‰¹é‡å¤„ç†100ä¸ªæ–‡æ¡£æ—¶ï¼Œå¦‚æœæ¯ä¸ªæ–‡æ¡£å®Œæˆåéƒ½ç­‰å¾…æŒä¹…åŒ–ï¼Œä¼šæµªè´¹å¤§é‡æ—¶é—´ã€‚
        
        **ä¼˜åŒ–ç­–ç•¥**ï¼š
        - ä½¿ç”¨è¿›ç¨‹çº§æ ‡è®°ï¼Œç¡®ä¿åŒä¸€çŸ¥è¯†åº“åœ¨çŸ­æ—¶é—´å†…åªæ‰§è¡Œä¸€æ¬¡å…¨å±€æŒä¹…åŒ–
        - é¿å…100ä¸ªæ–‡æ¡£éƒ½é‡å¤ç­‰å¾…30ç§’
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            kb_settings: çŸ¥è¯†åº“é…ç½®
        """
        import time
        import asyncio
        
        # ä½¿ç”¨ç±»çº§åˆ«çš„å­—å…¸è®°å½•æœ€åæŒä¹…åŒ–æ—¶é—´
        if not hasattr(self.__class__, '_last_persistence_time'):
            self.__class__._last_persistence_time = {}
        
        current_time = time.time()
        last_time = self.__class__._last_persistence_time.get(kb_id, 0)
        
        # å¦‚æœè·ç¦»ä¸Šæ¬¡æŒä¹…åŒ–ä¸è¶³60ç§’ï¼Œè·³è¿‡
        if current_time - last_time < 60:
            logger.debug(f"â­ï¸ è·ç¦»ä¸Šæ¬¡å…¨å±€æŒä¹…åŒ–ä¸è¶³60ç§’ï¼Œè·³è¿‡ (kb_id: {kb_id})")
            return
        
        # æ›´æ–°æ—¶é—´æˆ³
        self.__class__._last_persistence_time[kb_id] = current_time
        
        # æ‰§è¡Œå…¨å±€æŒä¹…åŒ–
        collection_name = kb_settings.get("collection_name")
        if not collection_name:
            logger.warning("âš ï¸ ç¼ºå°‘ collection_nameï¼Œè·³è¿‡å…¨å±€æŒä¹…åŒ–æ£€æŸ¥")
            return
        
        try:
            from .vectorstore_manager import get_vectorstore_manager
            
            vectorstore_mgr = get_vectorstore_manager()
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,
                vectorstore_mgr.force_global_compaction_wait,
                collection_name
            )
            
        except Exception as e:
            logger.error(f"âŒ å…¨å±€æŒä¹…åŒ–æ£€æŸ¥å¤±è´¥: {e}", exc_info=True)
    
    async def _final_persistence_check(self, kb_id: str, kb_settings: Dict[str, Any]):
        """
        ğŸ”¥ æœ€ç»ˆæŒä¹…åŒ–ç¡®è®¤ï¼šç¡®ä¿ChromaDBåå°compactionå®Œå…¨å®Œæˆ
        
        é—®é¢˜åœºæ™¯ï¼š
        å½“å¤šä¸ªæ–‡æ¡£å¹¶å‘å¤„ç†æ—¶ï¼ˆ3ä¸ªworkeråŒæ—¶å†™å…¥ï¼‰ï¼š
        - worker-1 å®Œæˆå†™å…¥ â†’ ç­‰å¾…10ç§’ â†’ é‡Šæ”¾é”
        - worker-2 å®Œæˆå†™å…¥ â†’ ç­‰å¾…10ç§’ â†’ é‡Šæ”¾é”
        - worker-3 å®Œæˆå†™å…¥ â†’ ç­‰å¾…10ç§’ â†’ é‡Šæ”¾é”
        
        ä½†æ˜¯ï¼ChromaDBçš„åå°compactionæ˜¯å…¨å±€çš„ï¼Œå¯èƒ½è¿˜åœ¨åˆå¹¶å‰é¢workerçš„ç´¢å¼•æ®µã€‚
        æ­¤æ—¶å¦‚æœé‡å¯æœåŠ¡å™¨ï¼Œç´¢å¼•å°±ä¼šæŸåã€‚
        
        è§£å†³æ–¹æ¡ˆï¼š
        åœ¨æ–‡æ¡£å¤„ç†çš„æœ€åï¼Œå†æ¬¡å¼ºåˆ¶è§¦å‘compactionå¹¶ç­‰å¾…ï¼Œç¡®ä¿ï¼š
        1. æ‰€æœ‰æ‰¹æ¬¡çš„ç´¢å¼•æ®µéƒ½å·²åˆå¹¶
        2. SQLite WALå·²ç»checkpoint
        3. HNSWç´¢å¼•å®Œå…¨æŒä¹…åŒ–åˆ°ç£ç›˜
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            kb_settings: çŸ¥è¯†åº“é…ç½®
        """
        import asyncio
        
        try:
            # è·å–vectorstoreå®ä¾‹ï¼ˆä¸éœ€è¦embeddingï¼Œåªç”¨æ¥è®¿é—®collectionï¼‰
            from .embedding_manager import get_embedding_manager
            from .vectorstore_manager import get_vectorstore_manager
            
            collection_name = kb_settings.get("collection_name")
            persist_dir = kb_settings.get("persist_dir")
            
            if not collection_name or not persist_dir:
                logger.warning("âš ï¸ ç¼ºå°‘ collection_name æˆ– persist_dirï¼Œè·³è¿‡æœ€ç»ˆæŒä¹…åŒ–æ£€æŸ¥")
                return
            
            # è·å–embeddingé…ç½®ï¼ˆä»…ç”¨äºè·å–vectorstoreå®ä¾‹ï¼‰
            embeddings_config = kb_settings.get("embeddings") or {}
            provider = embeddings_config.get("provider", "local")
            model = embeddings_config.get("model", "checkpoints/embeddings/all-MiniLM-L6-v2")
            base_url = embeddings_config.get("base_url")
            api_key = embeddings_config.get("api_key")
            local_model_path = embeddings_config.get("local_model_path")
            
            embedding_mgr = get_embedding_manager()
            vectorstore_mgr = get_vectorstore_manager()
            
            embedding_func = embedding_mgr.get_or_create(
                provider=provider,
                model=model,
                base_url=base_url,
                api_key=api_key,
                local_model_path=local_model_path,
                max_length=512,
                batch_size=8,
                normalize=True
            )
            
            search_params = kb_settings.get("search_params") or {}
            distance_metric = search_params.get("distance_metric")
            
            vectorstore = vectorstore_mgr.get_or_create(
                collection_name=collection_name,
                persist_dir=persist_dir,
                embedding_function=embedding_func,
                distance_metric=distance_metric
            )
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæœ€ç»ˆæŒä¹…åŒ–æ£€æŸ¥ï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,
                self._force_final_compaction,
                vectorstore
            )
            
        except Exception as e:
            logger.warning(f"âš ï¸ æœ€ç»ˆæŒä¹…åŒ–æ£€æŸ¥å¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")
    
    def _force_final_compaction(self, vectorstore):
        """
        åŒæ­¥æ–¹æ³•ï¼šå¼ºåˆ¶è§¦å‘æœ€ç»ˆçš„compaction
        
        åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œä¸é˜»å¡äº‹ä»¶å¾ªç¯
        """
        import time
        
        try:
            if hasattr(vectorstore, '_vectorstore'):
                # å¦‚æœæ˜¯VectorStoreWithLockåŒ…è£…ç±»ï¼Œè·å–å†…éƒ¨å®ä¾‹
                vectorstore = vectorstore._vectorstore
            
            if hasattr(vectorstore, '_store') and hasattr(vectorstore._store, '_collection'):
                collection = vectorstore._store._collection
                
                logger.info(f"ğŸ’¾ [æœ€ç»ˆæŒä¹…åŒ–] å¼€å§‹å¼ºåˆ¶è§¦å‘æœ€ç»ˆcompaction...")
                
                # è§¦å‘compaction
                doc_count = collection.count()
                logger.info(f"ğŸ’¾ [æœ€ç»ˆæŒä¹…åŒ–] å·²è§¦å‘compactionï¼Œå½“å‰æ–‡æ¡£æ•°: {doc_count}")
                
                # ğŸ”¥ å…³é”®ï¼šç­‰å¾…è¶³å¤Ÿé•¿çš„æ—¶é—´è®©æ‰€æœ‰ç§¯å‹çš„compactionä»»åŠ¡å®Œæˆ
                # è¿™æ˜¯åœ¨æ–‡æ¡£å¤„ç†çš„æœ€åé˜¶æ®µï¼Œä¸å†æœ‰æ–°çš„å†™å…¥ï¼Œå¯ä»¥æ”¾å¿ƒç­‰å¾…
                wait_time = 10.0
                time.sleep(wait_time)
                logger.warning(f"ğŸ’¾ [æœ€ç»ˆæŒä¹…åŒ–] å·²ç­‰å¾… {wait_time}ç§’ ç¡®ä¿åå°compactionå®Œæˆ")
                
                # å†æ¬¡ç¡®è®¤
                final_count = collection.count()
                logger.info(f"âœ… [æœ€ç»ˆæŒä¹…åŒ–] æœ€ç»ˆç¡®è®¤å®Œæˆï¼Œæ–‡æ¡£æ•°: {final_count}")
                
        except Exception as e:
            logger.warning(f"âš ï¸ æœ€ç»ˆcompactionè§¦å‘å¤±è´¥: {e}")
    
    async def _update_document_completed(self, doc_id: str, chunk_count: int):
        """
        æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå®Œæˆï¼ŒåŒæ—¶æ›´æ–°çŸ¥è¯†åº“çš„åˆ†ç‰‡è®¡æ•°
        
        Args:
            doc_id: æ–‡æ¡£ID
            chunk_count: åˆ†å—æ•°é‡
        """
        # 1. æ›´æ–°æ–‡æ¡£çŠ¶æ€
        result = await self.db.kb_documents.update_one(
            {"_id": ObjectId(doc_id)},
            {
                "$set": {
                    "status": "completed",
                    "chunk_count": chunk_count,
                    "error_message": None,
                    "updated_at": datetime.utcnow().isoformat()
                }
            }
        )
        
        # 2. æ›´æ–°çŸ¥è¯†åº“çš„åˆ†ç‰‡è®¡æ•°ï¼ˆåŸå­æ“ä½œï¼‰
        if result.modified_count > 0:
            doc = await self.db.kb_documents.find_one({"_id": ObjectId(doc_id)})
            if doc and doc.get("kb_id"):
                await self.db.knowledge_bases.update_one(
                    {"_id": ObjectId(doc["kb_id"])},
                    {
                        "$inc": {"chunk_count": chunk_count},
                        "$set": {"updated_at": datetime.utcnow().isoformat()}
                    }
                )
                logger.info(f"å·²æ›´æ–°çŸ¥è¯†åº“ {doc['kb_id']} çš„ chunk_countï¼Œå¢åŠ  {chunk_count}")
    
    async def _update_document_failed(self, doc_id: str, error: str):
        """
        æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå¤±è´¥
        
        Args:
            doc_id: æ–‡æ¡£ID
            error: é”™è¯¯ä¿¡æ¯
        """
        await self.db.kb_documents.update_one(
            {"_id": ObjectId(doc_id)},
            {
                "$set": {
                    "status": "failed",
                    "error_message": error,
                    "updated_at": datetime.utcnow().isoformat()
                }
            }
        )


# ä¾èµ–æ³¨å…¥å‡½æ•°
async def get_document_processor(
    db: AsyncIOMotorDatabase = None
) -> DocumentProcessor:
    """
    è·å–æ–‡æ¡£å¤„ç†æœåŠ¡å®ä¾‹
    
    Args:
        db: æ•°æ®åº“è¿æ¥
        
    Returns:
        æ–‡æ¡£å¤„ç†æœåŠ¡å®ä¾‹
    """
    if db is None:
        from ..database import get_database
        db_client = await anext(get_database())
        db = db_client[settings.mongodb_db_name]
    
    return DocumentProcessor(db)

