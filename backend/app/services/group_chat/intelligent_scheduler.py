"""
AIæ™ºèƒ½è°ƒåº¦ä¼˜åŒ–å™¨ v2.0

æ ¸å¿ƒç›®æ ‡ï¼š
1. å¤šç»´åº¦é˜ˆå€¼æ§åˆ¶å¹¶å‘æ•°é‡
2. æ™ºèƒ½å»¶è¿Ÿåˆ†çº§ï¼ˆè®©åå‘AIèƒ½çœ‹åˆ°å…ˆå‘AIï¼‰
3. å†…å®¹å»é‡æ£€æµ‹ï¼ˆé¿å…é›·åŒå›å¤ï¼‰
4. è¡Œä¸ºçœŸå®æ„Ÿå¢å¼ºï¼ˆè®©AIæ— æ³•åŒºåˆ†å½¼æ­¤ï¼‰
5. ä¿ƒè¿›AI-to-AIæ·±åº¦äº’åŠ¨

è®¾è®¡ç†å¿µï¼š
- åƒå¯¼æ¼”æŒ‡æŒ¥ç¾¤æˆï¼Œè€Œä¸æ˜¯æœºæ¢°è°ƒåº¦
- è®©å¯¹è¯"æ´»"èµ·æ¥ï¼Œæœ‰èµ·ä¼ã€æœ‰èŠ‚å¥
- è®©çœŸäººå’ŒAIéƒ½è§‰å¾—è¿™æ˜¯çœŸå®ç¾¤èŠ
"""
import asyncio
import random
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict, deque
from ...models.group_chat import (
    GroupMessage, GroupMember, AIReplyDecision,
    GroupChatContext, MemberType
)

logger = logging.getLogger(__name__)


class ConcurrencyStrategy:
    """å¹¶å‘æ§åˆ¶ç­–ç•¥"""
    
    # é»˜è®¤å¤šç»´åº¦é˜ˆå€¼é…ç½®
    DEFAULT_THRESHOLDS = {
        # ç»´åº¦1ï¼šæ ¹æ®ç¾¤ç»„æ´»è·ƒåº¦
        "activity": {
            "cold": {  # å†·æ¸…ç¾¤ï¼ˆæœ€è¿‘5åˆ†é’Ÿ < 3æ¡æ¶ˆæ¯ï¼‰
                "max_concurrent": 1,
                "min_delay_gap": 5.0,  # æœ€å°å»¶è¿Ÿé—´éš”
                "description": "å†·æ¸…ç¾¤ï¼Œ1ä¸ªAIæ…¢æ…¢å›å¤"
            },
            "warm": {  # æ¸©å’Œç¾¤ï¼ˆ3-10æ¡æ¶ˆæ¯ï¼‰
                "max_concurrent": 2,
                "min_delay_gap": 3.0,
                "description": "æ¸©å’Œç¾¤ï¼Œæœ€å¤š2ä¸ªAIï¼Œé—´éš”3ç§’"
            },
            "hot": {  # çƒ­é—¹ç¾¤ï¼ˆ> 10æ¡æ¶ˆæ¯ï¼‰
                "max_concurrent": 3,
                "min_delay_gap": 2.0,
                "description": "çƒ­é—¹ç¾¤ï¼Œæœ€å¤š3ä¸ªAIï¼Œé—´éš”2ç§’"
            }
        },
        
        # ç»´åº¦2ï¼šæ ¹æ®è§¦å‘æ¶ˆæ¯ç±»å‹
        "trigger_type": {
            "human_message": {
                "max_concurrent": 3,
                "prefer_multiple": True,  # äººç±»æ¶ˆæ¯é¼“åŠ±å¤šAIå›å¤
                "description": "äººç±»æ¶ˆæ¯ï¼Œå¯ä»¥å¤šä¸ªAIå›å¤"
            },
            "ai_message": {
                "max_concurrent": 2,
                "prefer_multiple": False,  # AIæ¶ˆæ¯æ§åˆ¶å›å¤æ•°
                "description": "AIæ¶ˆæ¯ï¼Œæœ€å¤š2ä¸ªAIå›å¤"
            },
            "at_mention": {
                "max_concurrent": 1,
                "prefer_multiple": False,  # @æ¶ˆæ¯é€šå¸¸åªéœ€è¦è¢«@çš„AIå›å¤
                "description": "@æ¶ˆæ¯ï¼Œä¼˜å…ˆè¢«@çš„AI"
            }
        },
        
        # ç»´åº¦3ï¼šæ ¹æ®AIè¿ç»­å›å¤æƒ…å†µ
        "ai_consecutive": {
            0: {"multiplier": 1.0, "description": "æ— AIè¿ç»­ï¼Œæ­£å¸¸"},
            1: {"multiplier": 0.8, "description": "1æ¬¡AIè¿ç»­ï¼Œæ¦‚ç‡-20%"},
            2: {"multiplier": 0.5, "description": "2æ¬¡AIè¿ç»­ï¼Œæ¦‚ç‡-50%"},
            3: {"multiplier": 0.2, "description": "3æ¬¡AIè¿ç»­ï¼Œæ¦‚ç‡-80%"}
        },
        
        # ç»´åº¦4ï¼šæ ¹æ®æœ€è¿‘å›å¤çš„AIæ•°é‡
        "recent_ai_density": {
            "sparse": {  # æœ€è¿‘5æ¡æ¶ˆæ¯ä¸­ < 2æ¡AI
                "encourage": True,
                "description": "AIå›å¤ç¨€ç–ï¼Œé¼“åŠ±å‚ä¸"
            },
            "balanced": {  # 2-3æ¡AI
                "encourage": False,
                "description": "AIå›å¤é€‚ä¸­ï¼Œæ­£å¸¸"
            },
            "dense": {  # > 3æ¡AI
                "encourage": False,
                "multiplier": 0.5,
                "description": "AIå›å¤è¿‡å¯†ï¼Œé™ä½æ¦‚ç‡50%"
            }
        }
    }
    
    def __init__(self, custom_thresholds: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–ç­–ç•¥
        
        Args:
            custom_thresholds: è‡ªå®šä¹‰é˜ˆå€¼é…ç½®ï¼ˆç”¨äºæ— é™åˆ¶æ¨¡å¼ç­‰ç‰¹æ®Šåœºæ™¯ï¼‰
        """
        self.thresholds = custom_thresholds if custom_thresholds else self.DEFAULT_THRESHOLDS
    
    def analyze_situation(
        self,
        message: GroupMessage,
        context: GroupChatContext,
        ai_consecutive_count: int
    ) -> Dict[str, Any]:
        """
        åˆ†æå½“å‰ç¾¤èŠæƒ…å†µï¼Œè¿”å›ç»¼åˆç­–ç•¥
        
        Returns:
            {
                "max_concurrent": int,
                "min_delay_gap": float,
                "probability_multiplier": float,
                "reasoning": str
            }
        """
        recent_messages = context.recent_messages[-10:]  # æœ€è¿‘10æ¡
        
        # 1. æ´»è·ƒåº¦åˆ†æ
        recent_5min_count = len([
            m for m in recent_messages
            if (datetime.now() - m.timestamp).total_seconds() < 300
        ])
        
        if recent_5min_count < 3:
            activity_level = "cold"
        elif recent_5min_count < 10:
            activity_level = "warm"
        else:
            activity_level = "hot"
        
        activity_config = self.thresholds["activity"][activity_level]
        
        # 2. è§¦å‘ç±»å‹åˆ†æ
        if message.sender_type != MemberType.AI:
            trigger_type = "human_message"
        elif "@" in message.content:
            trigger_type = "at_mention"
        else:
            trigger_type = "ai_message"
        
        trigger_config = self.thresholds["trigger_type"][trigger_type]
        
        # 3. AIè¿ç»­å›å¤åˆ†æ
        consecutive_config = self.thresholds["ai_consecutive"].get(
            ai_consecutive_count,
            self.thresholds["ai_consecutive"][3]  # è¶…è¿‡3æ¬¡ï¼ŒæŒ‰3æ¬¡å¤„ç†
        )
        
        # 4. AIå¯†åº¦åˆ†æ
        recent_ai_count = len([m for m in recent_messages[-5:] if m.sender_type == MemberType.AI])
        if recent_ai_count < 2:
            density_level = "sparse"
        elif recent_ai_count <= 3:
            density_level = "balanced"
        else:
            density_level = "dense"
        
        density_config = self.thresholds["recent_ai_density"][density_level]
        
        # 5. ç»¼åˆå†³ç­–
        max_concurrent = min(
            activity_config["max_concurrent"],
            trigger_config["max_concurrent"]
        )
        
        min_delay_gap = activity_config["min_delay_gap"]
        
        probability_multiplier = consecutive_config["multiplier"]
        if "multiplier" in density_config:
            probability_multiplier *= density_config["multiplier"]
        
        reasoning = (
            f"æ´»è·ƒåº¦={activity_level}({activity_config['description']}) | "
            f"è§¦å‘ç±»å‹={trigger_type}({trigger_config['description']}) | "
            f"AIè¿ç»­={ai_consecutive_count}æ¬¡({consecutive_config['description']}) | "
            f"AIå¯†åº¦={density_level}({density_config['description']})"
        )
        
        return {
            "max_concurrent": max_concurrent,
            "min_delay_gap": min_delay_gap,
            "probability_multiplier": probability_multiplier,
            "reasoning": reasoning,
            "activity_level": activity_level,
            "trigger_type": trigger_type
        }


class DelayTierCalculator:
    """å»¶è¿Ÿåˆ†çº§è®¡ç®—å™¨ï¼ˆè®©åå‘AIèƒ½çœ‹åˆ°å…ˆå‘AIï¼‰"""
    
    @staticmethod
    def calculate_tiered_delays(
        decisions: List[AIReplyDecision],
        min_gap: float = 3.0,
        delay_config: Optional[Dict[str, float]] = None
    ) -> List[AIReplyDecision]:
        """
        è®¡ç®—åˆ†çº§å»¶è¿Ÿ
        
        ç­–ç•¥ï¼š
        1. ç¬¬ä¸€ä¸ªAIï¼šçŸ­å»¶è¿Ÿï¼ˆè®©TAå…ˆå›å¤ï¼‰
        2. ç¬¬äºŒä¸ªAIï¼šç¬¬ä¸€ä¸ª + min_gapï¼ˆç¡®ä¿èƒ½çœ‹åˆ°ç¬¬ä¸€ä¸ªçš„å›å¤ï¼‰
        3. ç¬¬ä¸‰ä¸ªAIï¼šç¬¬äºŒä¸ª + min_gap
        
        Args:
            decisions: AIå†³ç­–åˆ—è¡¨ï¼ˆå·²æŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
            min_gap: æœ€å°å»¶è¿Ÿé—´éš”ï¼ˆç§’ï¼‰
            delay_config: å»¶è¿Ÿé…ç½®å­—å…¸ï¼ŒåŒ…å«å„ç§å»¶è¿ŸèŒƒå›´
        
        Returns:
            å¸¦æœ‰åˆ†çº§å»¶è¿Ÿçš„å†³ç­–åˆ—è¡¨
        """
        if not decisions:
            return []
        
        logger.info(
            f"\n{'='*60}\n"
            f"ğŸ“Š å»¶è¿Ÿåˆ†çº§è®¡ç®— | AIæ•°é‡={len(decisions)} | æœ€å°é—´éš”={min_gap}s\n"
            f"{'='*60}"
        )
        
        for i, decision in enumerate(decisions):
            if i == 0:
                # ç¬¬ä¸€ä¸ªAIï¼šæ ¹æ®åŸå§‹è§„åˆ™è®¡ç®—åŸºç¡€å»¶è¿Ÿ
                base_delay = DelayTierCalculator._calculate_base_delay(decision, delay_config)
                decision.delay_seconds = base_delay
                decision.tier = 1
                
                logger.info(
                    f"ğŸ¥‡ ç¬¬{i+1}æ¢¯é˜Ÿ: {decision.ai_member_id} | "
                    f"å»¶è¿Ÿ={base_delay:.2f}sï¼ˆåŸºç¡€å»¶è¿Ÿï¼‰"
                )
            else:
                # åç»­AIï¼šåœ¨å‰ä¸€ä¸ªAIçš„åŸºç¡€ä¸Šå¢åŠ min_gap
                prev_delay = decisions[i-1].delay_seconds
                decision.delay_seconds = prev_delay + min_gap
                decision.tier = i + 1
                
                logger.info(
                    f"ğŸ¥ˆ ç¬¬{i+1}æ¢¯é˜Ÿ: {decision.ai_member_id} | "
                    f"å»¶è¿Ÿ={decision.delay_seconds:.2f}s "
                    f"ï¼ˆå‰ä¸€ä¸ª{prev_delay:.2f}s + é—´éš”{min_gap}sï¼‰"
                )
            
            decision.scheduled_time = datetime.now() + timedelta(seconds=decision.delay_seconds)
        
        return decisions
    
    @staticmethod
    def _calculate_base_delay(decision: AIReplyDecision, delay_config: Optional[Dict[str, float]] = None) -> float:
        """
        è®¡ç®—åŸºç¡€å»¶è¿Ÿï¼ˆç¬¬ä¸€ä¸ªAIï¼‰
        
        Args:
            decision: AIå†³ç­–
            delay_config: å»¶è¿Ÿé…ç½®å­—å…¸
        
        Returns:
            å»¶è¿Ÿç§’æ•°
        """
        # é»˜è®¤å»¶è¿Ÿé…ç½®ï¼ˆä¿æŒåŸæœ‰çš„é»˜è®¤å€¼ï¼‰
        default_config = {
            "mention_delay_min": 0.5,
            "mention_delay_max": 1.5,
            "high_interest_delay_min": 1.0,
            "high_interest_delay_max": 2.0,
            "normal_delay_min": 1.5,
            "normal_delay_max": 3.0,
        }
        
        # ğŸ”¥ ä½¿ç”¨ä¼ å…¥çš„é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        if delay_config:
            config = delay_config
            logger.info(f"ğŸ“Š ä½¿ç”¨ç”¨æˆ·é…ç½®çš„å»¶è¿Ÿå‚æ•°: {config}")
        else:
            config = default_config
            logger.info(f"ğŸ“Š ä½¿ç”¨é»˜è®¤å»¶è¿Ÿå‚æ•°: {config}")
        
        # è¢«@ï¼šå¿«é€Ÿå“åº”ï¼ˆä½¿ç”¨mention_delayé…ç½®ï¼‰
        if "è¢«@æåŠ" in decision.decision_reason:
            delay = random.uniform(
                config.get("mention_delay_min", 0.5),
                config.get("mention_delay_max", 1.5)
            )
            logger.info(f"âš¡ è¢«@æ¶ˆæ¯å»¶è¿Ÿ: {delay:.2f}s (èŒƒå›´: {config.get('mention_delay_min')}-{config.get('mention_delay_max')}s)")
            return delay
        
        # é«˜æ¦‚ç‡ï¼šä¸­ç­‰å»¶è¿Ÿï¼ˆä½¿ç”¨high_interest_delayé…ç½®ï¼‰
        if decision.probability_score >= 0.7:
            delay = random.uniform(
                config.get("high_interest_delay_min", 1.0),
                config.get("high_interest_delay_max", 2.0)
            )
            logger.info(f"ğŸ”¥ é«˜å…´è¶£æ¶ˆæ¯å»¶è¿Ÿ: {delay:.2f}s (èŒƒå›´: {config.get('high_interest_delay_min')}-{config.get('high_interest_delay_max')}s)")
            return delay
        
        # æ™®é€šï¼šç¨é•¿å»¶è¿Ÿï¼ˆä½¿ç”¨normal_delayé…ç½®ï¼‰
        delay = random.uniform(
            config.get("normal_delay_min", 1.5),
            config.get("normal_delay_max", 3.0)
        )
        logger.info(f"ğŸ’¬ æ™®é€šæ¶ˆæ¯å»¶è¿Ÿ: {delay:.2f}s (èŒƒå›´: {config.get('normal_delay_min')}-{config.get('normal_delay_max')}s)")
        return delay


class ContentSimilarityDetector:
    """å†…å®¹ç›¸ä¼¼åº¦æ£€æµ‹å™¨ï¼ˆé¿å…é›·åŒå›å¤ï¼‰"""
    
    @staticmethod
    def is_similar_response(
        response1: str,
        response2: str,
        threshold: float = 0.6
    ) -> bool:
        """
        æ£€æµ‹ä¸¤ä¸ªå›å¤æ˜¯å¦è¿‡äºç›¸ä¼¼
        
        Args:
            response1: å›å¤1
            response2: å›å¤2
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
        
        Returns:
            True: ç›¸ä¼¼åº¦è¿‡é«˜
            False: ç›¸ä¼¼åº¦å¯æ¥å—
        """
        # ç®€å•å®ç°ï¼šåŸºäºå…³é”®è¯é‡å åº¦
        # ç”Ÿäº§ç¯å¢ƒå¯ç”¨æ›´å¤æ‚çš„ç®—æ³•ï¼ˆå¦‚TF-IDFã€BERTç›¸ä¼¼åº¦ï¼‰
        
        # æå–å…³é”®è¯ï¼ˆå»é™¤æ ‡ç‚¹å’Œå¸¸è§è¯ï¼‰
        stopwords = {"æˆ‘", "ä½ ", "çš„", "äº†", "æ˜¯", "åœ¨", "ä¹Ÿ", "éƒ½", "å’Œ", "å“ˆå“ˆ", "å•Š", "å‘¢", "å—"}
        
        def extract_keywords(text: str) -> set:
            import re
            # ç§»é™¤@æåŠ
            text = re.sub(r'@\S+', '', text)
            # åˆ†è¯ï¼ˆç®€å•æŒ‰å­—ç¬¦ï¼‰
            words = [w for w in text if w.strip() and w not in stopwords and not re.match(r'[^\w\s]', w)]
            return set(words)
        
        keywords1 = extract_keywords(response1)
        keywords2 = extract_keywords(response2)
        
        if not keywords1 or not keywords2:
            return False
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        intersection = len(keywords1 & keywords2)
        union = len(keywords1 | keywords2)
        
        similarity = intersection / union if union > 0 else 0
        
        logger.debug(
            f"ğŸ“Š ç›¸ä¼¼åº¦æ£€æµ‹: {similarity:.2%} | "
            f"å…³é”®è¯1={keywords1} | å…³é”®è¯2={keywords2}"
        )
        
        return similarity >= threshold


class BehaviorRealism:
    """è¡Œä¸ºçœŸå®æ„Ÿå¢å¼º"""
    
    # AIè¡Œä¸ºæ¨¡å¼é…ç½®
    BEHAVIOR_PATTERNS = {
        "active": {  # æ´»è·ƒå‹AI
            "reply_boost": 1.2,
            "min_interval": 1.0,
            "description": "æ€§æ ¼æ´»è·ƒï¼Œå›å¤ç§¯æ"
        },
        "cautious": {  # è°¨æ…å‹AI
            "reply_boost": 0.8,
            "min_interval": 3.0,
            "description": "æ€§æ ¼è°¨æ…ï¼Œå›å¤è¾ƒæ…¢"
        },
        "balanced": {  # å¹³è¡¡å‹AI
            "reply_boost": 1.0,
            "min_interval": 2.0,
            "description": "æ€§æ ¼å¹³è¡¡ï¼Œå›å¤é€‚ä¸­"
        }
    }
    
    @staticmethod
    def adjust_for_realism(
        decision: AIReplyDecision,
        ai_member: GroupMember,
        recent_ai_replies: List[Dict]
    ) -> AIReplyDecision:
        """
        æ ¹æ®AIæ€§æ ¼å’Œå†å²è¡Œä¸ºè°ƒæ•´å†³ç­–
        
        Args:
            decision: åŸå§‹å†³ç­–
            ai_member: AIæˆå‘˜ä¿¡æ¯
            recent_ai_replies: è¯¥AIæœ€è¿‘çš„å›å¤è®°å½•
        
        Returns:
            è°ƒæ•´åçš„å†³ç­–
        """
        # 1. æ ¹æ®AIæ€§æ ¼è°ƒæ•´ï¼ˆä»metadataæˆ–è§’è‰²è®¾å®šè·å–ï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šæ ¹æ®AI ID hashå€¼åˆ†é…æ€§æ ¼
        personality = BehaviorRealism._get_personality(ai_member.member_id)
        pattern = BehaviorRealism.BEHAVIOR_PATTERNS[personality]
        
        decision.probability_score *= pattern["reply_boost"]
        
        # 2. é¿å…AIå›å¤è¿‡äºé¢‘ç¹ï¼ˆæ¨¡æ‹Ÿäººç±»éœ€è¦æ—¶é—´æ€è€ƒï¼‰
        if recent_ai_replies:
            last_reply_time = recent_ai_replies[-1].get("timestamp")
            if last_reply_time:
                time_since_last = (datetime.now() - last_reply_time).total_seconds()
                
                # å¦‚æœè·ç¦»ä¸Šæ¬¡å›å¤å¤ªè¿‘ï¼Œé™ä½æ¦‚ç‡
                if time_since_last < pattern["min_interval"]:
                    cooldown_penalty = 0.5
                    decision.probability_score *= cooldown_penalty
                    logger.debug(
                        f"â³ {ai_member.display_name or ai_member.member_id} å›å¤è¿‡äºé¢‘ç¹ï¼Œ"
                        f"é™ä½æ¦‚ç‡ï¼ˆ{cooldown_penalty:.0%}ï¼‰"
                    )
        
        return decision
    
    @staticmethod
    def _get_personality(ai_id: str) -> str:
        """æ ¹æ®AI IDåˆ†é…æ€§æ ¼ï¼ˆä¼ªéšæœºï¼Œä¿æŒä¸€è‡´æ€§ï¼‰"""
        hash_val = hash(ai_id) % 100
        if hash_val < 30:
            return "active"
        elif hash_val < 60:
            return "balanced"
        else:
            return "cautious"


class IntelligentScheduler:
    """æ™ºèƒ½è°ƒåº¦å™¨ v2.0"""
    
    def __init__(self, custom_thresholds: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–æ™ºèƒ½è°ƒåº¦å™¨
        
        Args:
            custom_thresholds: è‡ªå®šä¹‰é˜ˆå€¼é…ç½®ï¼ˆç”¨äºæ— é™åˆ¶æ¨¡å¼ç­‰ç‰¹æ®Šåœºæ™¯ï¼‰
        """
        self.concurrency_strategy = ConcurrencyStrategy(custom_thresholds)
        self.delay_calculator = DelayTierCalculator()
        self.similarity_detector = ContentSimilarityDetector()
        self.realism_enhancer = BehaviorRealism()
        
        # AIå›å¤å†å²ï¼ˆç”¨äºç›¸ä¼¼åº¦æ£€æµ‹å’Œè¡Œä¸ºåˆ†æï¼‰
        # group_id -> List[{ai_id, content, timestamp}]
        self.reply_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=20))
        
        # AIä¸ªäººå›å¤å†å²ï¼ˆç”¨äºé¢‘ç‡æ§åˆ¶ï¼‰
        # ai_member_id -> List[{group_id, timestamp}]
        self.ai_reply_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=10))
    
    def optimize_decisions(
        self,
        decisions: List[AIReplyDecision],
        message: GroupMessage,
        context: GroupChatContext,
        ai_consecutive_count: int,
        ai_members: List[GroupMember],
        delay_config: Optional[Dict[str, float]] = None
    ) -> List[AIReplyDecision]:
        """
        æ™ºèƒ½ä¼˜åŒ–AIå†³ç­–åˆ—è¡¨
        
        æµç¨‹ï¼š
        1. åˆ†æå½“å‰æƒ…å†µï¼ˆå¤šç»´åº¦é˜ˆå€¼ï¼‰
        2. é™åˆ¶å¹¶å‘æ•°é‡
        3. è°ƒæ•´æ¦‚ç‡ï¼ˆæ ¹æ®æƒ…å†µï¼‰
        4. è®¡ç®—åˆ†çº§å»¶è¿Ÿ
        5. å¢å¼ºè¡Œä¸ºçœŸå®æ„Ÿ
        
        Args:
            decisions: åŸå§‹å†³ç­–åˆ—è¡¨
            message: è§¦å‘æ¶ˆæ¯
            context: ç¾¤èŠä¸Šä¸‹æ–‡
            ai_consecutive_count: AIè¿ç»­å›å¤æ¬¡æ•°
            ai_members: æ‰€æœ‰AIæˆå‘˜
            delay_config: å»¶è¿Ÿé…ç½®å­—å…¸ï¼ˆåŒ…å«å„ç§å»¶è¿ŸèŒƒå›´ï¼‰
        
        Returns:
            ä¼˜åŒ–åçš„å†³ç­–åˆ—è¡¨
        """
        if not decisions:
            return []
        
        logger.info(
            f"\n{'='*80}\n"
            f"ğŸ§  æ™ºèƒ½è°ƒåº¦ä¼˜åŒ–å¼€å§‹ | åŸå§‹å€™é€‰æ•°={len(decisions)}\n"
            f"{'='*80}"
        )
        
        # 1. åˆ†æå½“å‰æƒ…å†µ
        situation = self.concurrency_strategy.analyze_situation(
            message, context, ai_consecutive_count
        )
        
        logger.info(
            f"ğŸ“Š æƒ…å†µåˆ†æ:\n"
            f"  - æœ€å¤§å¹¶å‘æ•°: {situation['max_concurrent']}\n"
            f"  - æœ€å°å»¶è¿Ÿé—´éš”: {situation['min_delay_gap']}s\n"
            f"  - æ¦‚ç‡å€æ•°: {situation['probability_multiplier']:.2%}\n"
            f"  - å†³ç­–ç†ç”±: {situation['reasoning']}"
        )
        
        # 2. åº”ç”¨æ¦‚ç‡è°ƒæ•´
        for decision in decisions:
            original_prob = decision.probability_score
            decision.probability_score *= situation['probability_multiplier']
            
            if original_prob != decision.probability_score:
                logger.debug(
                    f"  ğŸ“‰ {decision.ai_member_id}: "
                    f"{original_prob:.2%} -> {decision.probability_score:.2%}"
                )
        
        # 3. å¢å¼ºè¡Œä¸ºçœŸå®æ„Ÿ
        ai_member_map = {ai.member_id: ai for ai in ai_members}
        for decision in decisions:
            ai_member = ai_member_map.get(decision.ai_member_id)
            if ai_member:
                recent_replies = list(self.ai_reply_history[decision.ai_member_id])
                decision = self.realism_enhancer.adjust_for_realism(
                    decision, ai_member, recent_replies
                )
        
        # 4. åˆ†ç¦»è¢«@çš„AIï¼ˆä¼˜å…ˆä¿ç•™ï¼‰
        mentioned_decisions = []
        normal_decisions = []
        
        for decision in decisions:
            if "è¢«@" in decision.decision_reason or "è¿‘æœŸè¢«@" in decision.decision_reason:
                mentioned_decisions.append(decision)
            else:
                normal_decisions.append(decision)
        
        # 5. æŒ‰æ¦‚ç‡æ’åº
        mentioned_decisions.sort(key=lambda d: d.probability_score, reverse=True)
        normal_decisions.sort(key=lambda d: d.probability_score, reverse=True)
        
        # 6. é™åˆ¶å¹¶å‘æ•°é‡ï¼ˆè¢«@çš„AIä¼˜å…ˆä¿ç•™ï¼Œå‰©ä½™åé¢ç»™æ™®é€šAIï¼‰
        max_concurrent = situation['max_concurrent']
        
        # ğŸ”¥ è¢«@çš„AIå…¨éƒ¨ä¿ç•™ï¼ˆä¸å—å¹¶å‘é™åˆ¶ï¼‰
        selected_decisions = mentioned_decisions.copy()
        
        # å‰©ä½™åé¢åˆ†é…ç»™æ™®é€šAI
        remaining_slots = max(0, max_concurrent - len(mentioned_decisions))
        selected_decisions.extend(normal_decisions[:remaining_slots])
        
        if mentioned_decisions:
            logger.info(
                f"ğŸ¯ è¢«@çš„AIä¼˜å…ˆä¿ç•™: {len(mentioned_decisions)}ä¸ªï¼ˆä¸å—å¹¶å‘é™åˆ¶ï¼‰"
            )
        
        if len(decisions) > len(selected_decisions):
            logger.info(
                f"âœ‚ï¸ å¹¶å‘é™åˆ¶: {len(decisions)} -> {len(selected_decisions)} "
                f"(è¢«@AI: {len(mentioned_decisions)}, æ™®é€šAI: {len(selected_decisions) - len(mentioned_decisions)}, "
                f"ä¸¢å¼ƒ: {len(decisions) - len(selected_decisions)}ä¸ª)"
            )
        
        # 6. è®¡ç®—åˆ†çº§å»¶è¿Ÿ
        selected_decisions = self.delay_calculator.calculate_tiered_delays(
            selected_decisions,
            min_gap=situation['min_delay_gap'],
            delay_config=delay_config
        )
        
        logger.info(
            f"\n{'='*80}\n"
            f"âœ… æ™ºèƒ½è°ƒåº¦ä¼˜åŒ–å®Œæˆ | æœ€ç»ˆé€‰æ‹©={len(selected_decisions)}ä¸ªAI\n"
            f"{'='*80}"
        )
        
        return selected_decisions
    
    def record_reply(self, group_id: str, ai_member_id: str, content: str):
        """è®°å½•AIå›å¤ï¼ˆç”¨äºç›¸ä¼¼åº¦æ£€æµ‹å’Œè¡Œä¸ºåˆ†æï¼‰"""
        timestamp = datetime.now()
        
        # è®°å½•åˆ°ç¾¤ç»„å†å²
        self.reply_history[group_id].append({
            "ai_id": ai_member_id,
            "content": content,
            "timestamp": timestamp
        })
        
        # è®°å½•åˆ°AIä¸ªäººå†å²
        self.ai_reply_history[ai_member_id].append({
            "group_id": group_id,
            "timestamp": timestamp
        })
    
    def check_similarity_with_recent(
        self,
        group_id: str,
        content: str,
        lookback: int = 3,
        threshold: float = 0.6
    ) -> Tuple[bool, Optional[str]]:
        """
        æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸æœ€è¿‘çš„å›å¤ç›¸ä¼¼
        
        Args:
            group_id: ç¾¤ç»„ID
            content: å¾…æ£€æŸ¥å†…å®¹
            lookback: å›æº¯æ¡æ•°
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
        
        Returns:
            (æ˜¯å¦ç›¸ä¼¼, ç›¸ä¼¼çš„å›å¤å†…å®¹)
        """
        recent_replies = list(self.reply_history[group_id])[-lookback:]
        
        for reply in recent_replies:
            if self.similarity_detector.is_similar_response(content, reply["content"], threshold):
                logger.warning(
                    f"âš ï¸ å†…å®¹ç›¸ä¼¼åº¦è¿‡é«˜ï¼\n"
                    f"  æ–°å›å¤: {content[:50]}...\n"
                    f"  ç›¸ä¼¼å›å¤: {reply['content'][:50]}... (æ¥è‡ª {reply['ai_id']})\n"
                    f"  é˜ˆå€¼: {threshold}"
                )
                return True, reply["content"]
        
        return False, None


# å…¨å±€å•ä¾‹ï¼ˆä¿ç•™é»˜è®¤å®ä¾‹ç”¨äºå‘åå…¼å®¹ï¼‰
_intelligent_scheduler = None


def get_intelligent_scheduler(custom_thresholds: Optional[Dict[str, Any]] = None) -> IntelligentScheduler:
    """
    è·å–æ™ºèƒ½è°ƒåº¦å™¨å®ä¾‹
    
    Args:
        custom_thresholds: è‡ªå®šä¹‰é˜ˆå€¼é…ç½®ï¼ˆç”¨äºæ— é™åˆ¶æ¨¡å¼ç­‰ç‰¹æ®Šåœºæ™¯ï¼‰
                         å¦‚æœæä¾›ï¼Œåˆ™åˆ›å»ºæ–°å®ä¾‹ï¼›å¦åˆ™è¿”å›å…¨å±€å•ä¾‹
    
    Returns:
        IntelligentSchedulerå®ä¾‹
    """
    # å¦‚æœæä¾›è‡ªå®šä¹‰é…ç½®ï¼Œåˆ›å»ºæ–°å®ä¾‹
    if custom_thresholds is not None:
        return IntelligentScheduler(custom_thresholds)
    
    # å¦åˆ™è¿”å›å…¨å±€å•ä¾‹
    global _intelligent_scheduler
    if _intelligent_scheduler is None:
        _intelligent_scheduler = IntelligentScheduler()
    return _intelligent_scheduler

