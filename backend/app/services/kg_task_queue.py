"""
çŸ¥è¯†å›¾è°±ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†å™¨

åŸºäºRediså®ç°çš„æŒä¹…åŒ–ä»»åŠ¡é˜Ÿåˆ—ï¼Œæ”¯æŒï¼š
- ä»»åŠ¡æäº¤å’ŒæŒä¹…åŒ–
- ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª
- æ–­ç‚¹ç»­ä¼ ï¼ˆé‡å¯åæ¢å¤ï¼‰
- å¹¶å‘æ§åˆ¶
"""

import json
import asyncio
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
from redis import asyncio as aioredis

logger = logging.getLogger(__name__)


class KGTaskQueue:
    """çŸ¥è¯†å›¾è°±ä»»åŠ¡é˜Ÿåˆ—"""
    
    # Redisé”®å‰ç¼€
    QUEUE_KEY = "kg:task:queue"  # ä»»åŠ¡é˜Ÿåˆ—ï¼ˆListï¼‰
    TASK_STATUS_PREFIX = "kg:task:status:"  # ä»»åŠ¡çŠ¶æ€ï¼ˆHashï¼‰
    BATCH_STATUS_PREFIX = "kg:batch:status:"  # æ‰¹æ¬¡çŠ¶æ€ï¼ˆHashï¼‰
    PROCESSING_SET = "kg:task:processing"  # æ­£åœ¨å¤„ç†çš„ä»»åŠ¡é›†åˆï¼ˆSetï¼‰
    
    def __init__(self, redis_url: str = "redis://localhost:6379/1"):
        """
        åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—
        
        Args:
            redis_url: Redisè¿æ¥URL
        """
        self.redis_url = redis_url
        self.redis: Optional[aioredis.Redis] = None
        
    async def connect(self):
        """è¿æ¥Redis"""
        if self.redis is None:
            self.redis = await aioredis.from_url(
                self.redis_url,
                encoding="utf-8",
                decode_responses=True
            )
            logger.info(f"âœ… ä»»åŠ¡é˜Ÿåˆ—å·²è¿æ¥Redis: {self.redis_url}")
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.redis:
            await self.redis.close()
            self.redis = None
    
    async def submit_batch(
        self,
        batch_id: str,
        tasks: List[Dict[str, Any]],
        user_id: str,
        kb_id: str
    ) -> Dict[str, Any]:
        """
        æäº¤æ‰¹é‡ä»»åŠ¡
        
        Args:
            batch_id: æ‰¹æ¬¡ID
            tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å« {doc_id, filename}
            user_id: ç”¨æˆ·ID
            kb_id: çŸ¥è¯†åº“ID
            
        Returns:
            {
                "success": bool,
                "batch_id": str,
                "total_tasks": int,
                "message": str
            }
        """
        await self.connect()
        
        try:
            # ä¿å­˜æ‰¹æ¬¡ä¿¡æ¯ï¼ˆRedis hsetéœ€è¦æ‰€æœ‰å€¼éƒ½æ˜¯å­—ç¬¦ä¸²ï¼‰
            batch_info = {
                "batch_id": batch_id,
                "user_id": user_id,
                "kb_id": kb_id,
                "total_tasks": str(len(tasks)),
                "completed": str(0),
                "failed": str(0),
                "status": "pending",  # pending, processing, completed, failed
                "created_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat()
            }
            
            batch_key = f"{self.BATCH_STATUS_PREFIX}{batch_id}"
            # ä½¿ç”¨hmsetå…¼å®¹Redis 3.xï¼ˆhsetçš„mappingå‚æ•°éœ€è¦Redis 4.0+ï¼‰
            await self.redis.hmset(batch_key, batch_info)
            
            # å°†ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—
            task_count = 0
            for task in tasks:
                task_data = {
                    "batch_id": batch_id,
                    "task_id": f"{batch_id}:{task['doc_id']}",
                    "doc_id": task["doc_id"],
                    "kb_id": kb_id,
                    "user_id": user_id,
                    "filename": task.get("filename", ""),
                    "status": "pending",
                    "created_at": datetime.utcnow().isoformat(),
                    "retries": "0",
                    "max_retries": "3"
                }
                
                # ä¿å­˜ä»»åŠ¡çŠ¶æ€
                task_key = f"{self.TASK_STATUS_PREFIX}{task_data['task_id']}"
                await self.redis.hmset(task_key, task_data)
                
                # æ·»åŠ åˆ°é˜Ÿåˆ—
                await self.redis.rpush(self.QUEUE_KEY, task_data["task_id"])
                task_count += 1
            
            logger.info(f"âœ… æ‰¹æ¬¡ {batch_id} å·²æäº¤ï¼š{task_count} ä¸ªä»»åŠ¡")
            
            return {
                "success": True,
                "batch_id": batch_id,
                "total_tasks": task_count,
                "message": f"å·²æäº¤ {task_count} ä¸ªä»»åŠ¡åˆ°é˜Ÿåˆ—"
            }
            
        except Exception as e:
            logger.error(f"âŒ æäº¤æ‰¹æ¬¡å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "batch_id": batch_id,
                "total_tasks": 0,
                "message": f"æäº¤å¤±è´¥: {str(e)}"
            }
    
    async def get_next_task(self, timeout: int = 5) -> Optional[Dict[str, Any]]:
        """
        è·å–ä¸‹ä¸€ä¸ªå¾…å¤„ç†ä»»åŠ¡ï¼ˆé˜»å¡å¼ï¼‰
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            ä»»åŠ¡ä¿¡æ¯ï¼Œå¦‚æœé˜Ÿåˆ—ä¸ºç©ºåˆ™è¿”å›None
        """
        await self.connect()
        
        try:
            # ä»é˜Ÿåˆ—å¤´éƒ¨å–å‡ºä»»åŠ¡ï¼ˆé˜»å¡ï¼‰
            result = await self.redis.blpop(self.QUEUE_KEY, timeout=timeout)
            
            if not result:
                return None
            
            _, task_id = result
            
            # è·å–ä»»åŠ¡è¯¦æƒ…
            task_key = f"{self.TASK_STATUS_PREFIX}{task_id}"
            task_data = await self.redis.hgetall(task_key)
            
            if not task_data:
                logger.warning(f"âš ï¸ ä»»åŠ¡ {task_id} è¯¦æƒ…ä¸å­˜åœ¨")
                return None
            
            # æ ‡è®°ä¸ºå¤„ç†ä¸­
            await self.redis.sadd(self.PROCESSING_SET, task_id)
            await self.redis.hset(task_key, "status", "processing")
            await self.redis.hset(task_key, "started_at", datetime.utcnow().isoformat())
            
            # è½¬æ¢ä¸ºå­—å…¸
            task = dict(task_data)
            task["retries"] = int(task.get("retries", 0))
            task["max_retries"] = int(task.get("max_retries", 3))
            
            return task
            
        except Exception as e:
            logger.error(f"âŒ è·å–ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
            return None
    
    async def mark_task_completed(self, task_id: str):
        """
        æ ‡è®°ä»»åŠ¡å®Œæˆ
        
        Args:
            task_id: ä»»åŠ¡ID
        """
        await self.connect()
        
        try:
            task_key = f"{self.TASK_STATUS_PREFIX}{task_id}"
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            await self.redis.hset(task_key, "status", "completed")
            await self.redis.hset(task_key, "completed_at", datetime.utcnow().isoformat())
            
            # ä»å¤„ç†ä¸­é›†åˆç§»é™¤
            await self.redis.srem(self.PROCESSING_SET, task_id)
            
            # æ›´æ–°æ‰¹æ¬¡è¿›åº¦
            task_data = await self.redis.hgetall(task_key)
            batch_id = task_data.get("batch_id")
            
            if batch_id:
                batch_key = f"{self.BATCH_STATUS_PREFIX}{batch_id}"
                await self.redis.hincrby(batch_key, "completed", 1)
                await self.redis.hset(batch_key, "updated_at", datetime.utcnow().isoformat())
                
                # æ£€æŸ¥æ‰¹æ¬¡æ˜¯å¦å…¨éƒ¨å®Œæˆ
                await self._check_batch_completion(batch_id)
            
            logger.info(f"âœ… ä»»åŠ¡å®Œæˆ: {task_id}")
            
        except Exception as e:
            logger.error(f"âŒ æ ‡è®°ä»»åŠ¡å®Œæˆå¤±è´¥: {e}", exc_info=True)
    
    async def mark_task_failed(self, task_id: str, error: str, retry: bool = True):
        """
        æ ‡è®°ä»»åŠ¡å¤±è´¥
        
        Args:
            task_id: ä»»åŠ¡ID
            error: é”™è¯¯ä¿¡æ¯
            retry: æ˜¯å¦é‡è¯•
        """
        await self.connect()
        
        try:
            task_key = f"{self.TASK_STATUS_PREFIX}{task_id}"
            task_data = await self.redis.hgetall(task_key)
            
            if not task_data:
                logger.warning(f"âš ï¸ ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
                return
            
            retries = int(task_data.get("retries", 0))
            max_retries = int(task_data.get("max_retries", 3))
            
            # ä»å¤„ç†ä¸­é›†åˆç§»é™¤
            await self.redis.srem(self.PROCESSING_SET, task_id)
            
            # åˆ¤æ–­æ˜¯å¦é‡è¯•
            if retry and retries < max_retries:
                # é‡è¯•ï¼šé‡æ–°åŠ å…¥é˜Ÿåˆ—
                retries += 1
                await self.redis.hset(task_key, "retries", str(retries))
                await self.redis.hset(task_key, "status", "pending")
                await self.redis.hset(task_key, "last_error", error)
                await self.redis.rpush(self.QUEUE_KEY, task_id)
                
                logger.warning(f"âš ï¸ ä»»åŠ¡å¤±è´¥ï¼Œé‡è¯• {retries}/{max_retries}: {task_id}")
                
            else:
                # æœ€ç»ˆå¤±è´¥
                await self.redis.hset(task_key, "status", "failed")
                await self.redis.hset(task_key, "error", error)
                await self.redis.hset(task_key, "failed_at", datetime.utcnow().isoformat())
                
                # æ›´æ–°æ‰¹æ¬¡å¤±è´¥è®¡æ•°
                batch_id = task_data.get("batch_id")
                if batch_id:
                    batch_key = f"{self.BATCH_STATUS_PREFIX}{batch_id}"
                    await self.redis.hincrby(batch_key, "failed", 1)
                    await self.redis.hset(batch_key, "updated_at", datetime.utcnow().isoformat())
                    
                    # æ£€æŸ¥æ‰¹æ¬¡æ˜¯å¦å®Œæˆ
                    await self._check_batch_completion(batch_id)
                
                logger.error(f"âŒ ä»»åŠ¡æœ€ç»ˆå¤±è´¥: {task_id}, é”™è¯¯: {error}")
            
        except Exception as e:
            logger.error(f"âŒ æ ‡è®°ä»»åŠ¡å¤±è´¥æ—¶å‡ºé”™: {e}", exc_info=True)
    
    async def get_batch_status(self, batch_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æ‰¹æ¬¡çŠ¶æ€
        
        Args:
            batch_id: æ‰¹æ¬¡ID
            
        Returns:
            æ‰¹æ¬¡çŠ¶æ€ä¿¡æ¯
        """
        await self.connect()
        
        try:
            batch_key = f"{self.BATCH_STATUS_PREFIX}{batch_id}"
            batch_data = await self.redis.hgetall(batch_key)
            
            if not batch_data:
                return None
            
            # è½¬æ¢æ•°å­—å­—æ®µ
            batch_info = dict(batch_data)
            batch_info["total_tasks"] = int(batch_info.get("total_tasks", 0))
            batch_info["completed"] = int(batch_info.get("completed", 0))
            batch_info["failed"] = int(batch_info.get("failed", 0))
            batch_info["progress"] = (
                batch_info["completed"] / batch_info["total_tasks"] * 100
                if batch_info["total_tasks"] > 0 else 0
            )
            
            return batch_info
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ‰¹æ¬¡çŠ¶æ€å¤±è´¥: {e}", exc_info=True)
            return None
    
    async def get_queue_stats(self) -> Dict[str, Any]:
        """
        è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            {
                "queue_length": int,  # é˜Ÿåˆ—é•¿åº¦
                "processing_count": int,  # æ­£åœ¨å¤„ç†çš„ä»»åŠ¡æ•°
                "total_batches": int  # æ‰¹æ¬¡æ€»æ•°
            }
        """
        await self.connect()
        
        try:
            queue_length = await self.redis.llen(self.QUEUE_KEY)
            processing_count = await self.redis.scard(self.PROCESSING_SET)
            
            # ç»Ÿè®¡æ‰¹æ¬¡æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
            total_batches = 0  # éœ€è¦æ‰«ææ‰€æœ‰æ‰¹æ¬¡é”®ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
            
            return {
                "queue_length": queue_length,
                "processing_count": processing_count,
                "total_batches": total_batches
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–é˜Ÿåˆ—ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {
                "queue_length": 0,
                "processing_count": 0,
                "total_batches": 0
            }
    
    async def _check_batch_completion(self, batch_id: str):
        """
        æ£€æŸ¥æ‰¹æ¬¡æ˜¯å¦å®Œæˆ
        
        Args:
            batch_id: æ‰¹æ¬¡ID
        """
        try:
            batch_key = f"{self.BATCH_STATUS_PREFIX}{batch_id}"
            batch_data = await self.redis.hgetall(batch_key)
            
            total_tasks = int(batch_data.get("total_tasks", 0))
            completed = int(batch_data.get("completed", 0))
            failed = int(batch_data.get("failed", 0))
            
            if completed + failed >= total_tasks:
                # æ‰¹æ¬¡å®Œæˆ
                final_status = "completed" if failed == 0 else "partial_failed"
                await self.redis.hset(batch_key, "status", final_status)
                await self.redis.hset(batch_key, "finished_at", datetime.utcnow().isoformat())
                
                logger.info(
                    f"ğŸ‰ æ‰¹æ¬¡ {batch_id} å®Œæˆ: "
                    f"æ€»è®¡={total_tasks}, æˆåŠŸ={completed}, å¤±è´¥={failed}"
                )
            
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥æ‰¹æ¬¡å®ŒæˆçŠ¶æ€å¤±è´¥: {e}", exc_info=True)


# å…¨å±€ä»»åŠ¡é˜Ÿåˆ—å®ä¾‹
_task_queue: Optional[KGTaskQueue] = None


def get_task_queue() -> KGTaskQueue:
    """è·å–ä»»åŠ¡é˜Ÿåˆ—å•ä¾‹"""
    global _task_queue
    
    if _task_queue is None:
        from app.config import settings
        # ä½¿ç”¨ç‹¬ç«‹çš„Redis database (1) é¿å…ä¸ä¸»åº”ç”¨å†²çª
        if settings.redis_password:
            redis_url = f"redis://:{settings.redis_password}@{settings.redis_host}:{settings.redis_port}/1"
        else:
            redis_url = f"redis://{settings.redis_host}:{settings.redis_port}/1"
        _task_queue = KGTaskQueue(redis_url)
    
    return _task_queue

