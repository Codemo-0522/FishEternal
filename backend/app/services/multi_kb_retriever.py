"""
å¤šçŸ¥è¯†åº“å¹¶è¡Œæ£€ç´¢æœåŠ¡

åŠŸèƒ½ç‰¹æ€§:
1. å®Œå…¨å¼‚æ­¥å¹¶è¡Œæ£€ç´¢,ä¸é˜»å¡ä¸»çº¿ç¨‹
2. ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°,é¿å…èµ„æºè€—å°½
3. æ™ºèƒ½ç»“æœåˆå¹¶å’Œå»é‡
4. æ”¯æŒå¤šç§åˆå¹¶ç­–ç•¥
5. ç”¨æˆ·çº§åˆ«éš”ç¦»,äº’ä¸å½±å“
6. å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—

ä½œè€…: FishChat Team
åˆ›å»ºæ—¶é—´: 2025-01-29
"""

import asyncio
import logging
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict
import hashlib
from ..utils.distance_utils import calculate_score_from_distance

logger = logging.getLogger(__name__)


@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç±»"""
    content: str
    score: float
    distance: float
    metadata: Dict[str, Any]
    kb_id: str  # æ¥æºçŸ¥è¯†åº“ID
    kb_name: str  # æ¥æºçŸ¥è¯†åº“åç§°
    chunk_id: Optional[str] = None
    doc_id: Optional[str] = None
    document_name: Optional[str] = None  # æ–‡æ¡£åç§°ï¼ˆå‰ç«¯æ˜¾ç¤ºç”¨ï¼‰


class MultiKBRetriever:
    """
    å¤šçŸ¥è¯†åº“å¹¶è¡Œæ£€ç´¢å™¨
    
    è®¾è®¡åŸåˆ™:
    - å¼‚æ­¥éé˜»å¡: æ‰€æœ‰IOæ“ä½œä½¿ç”¨async/await
    - å¹¶å‘æ§åˆ¶: ä½¿ç”¨ä¿¡å·é‡é™åˆ¶åŒæ—¶æ£€ç´¢çš„çŸ¥è¯†åº“æ•°é‡
    - èµ„æºéš”ç¦»: æ¯ä¸ªçŸ¥è¯†åº“ä½¿ç”¨ç‹¬ç«‹çš„vectorstoreå®ä¾‹
    - å®¹é”™è®¾è®¡: å•ä¸ªçŸ¥è¯†åº“å¤±è´¥ä¸å½±å“å…¶ä»–çŸ¥è¯†åº“
    """
    
    # å¹¶å‘æ§åˆ¶: åŒæ—¶æ£€ç´¢çš„æœ€å¤§çŸ¥è¯†åº“æ•°é‡ (å¯æ ¹æ®æœåŠ¡å™¨æ€§èƒ½è°ƒæ•´)
    MAX_CONCURRENT_KB = 5
    
    def __init__(self):
        """åˆå§‹åŒ–å¤šçŸ¥è¯†åº“æ£€ç´¢å™¨"""
        self._semaphore = asyncio.Semaphore(self.MAX_CONCURRENT_KB)
        logger.info(f"ğŸ”§ å¤šçŸ¥è¯†åº“æ£€ç´¢å™¨å·²åˆå§‹åŒ– (æœ€å¤§å¹¶å‘: {self.MAX_CONCURRENT_KB})")
    
    async def retrieve_from_multiple_kbs(
        self,
        query: str,
        kb_configs: List[Dict[str, Any]],
        top_k_per_kb: int = 3,
        similarity_threshold: Optional[float] = None,
        merge_strategy: str = "weighted_score",
        final_top_k: int = 10
    ) -> List[RetrievalResult]:
        """
        ä»å¤šä¸ªçŸ¥è¯†åº“å¹¶è¡Œæ£€ç´¢å¹¶åˆå¹¶ç»“æœ
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            kb_configs: çŸ¥è¯†åº“é…ç½®åˆ—è¡¨,æ¯é¡¹åŒ…å« kb_id, kb_name, kb_settings
            top_k_per_kb: æ¯ä¸ªçŸ¥è¯†åº“è¿”å›çš„æœ€å¤§ç»“æœæ•°
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (L2è·ç¦»)
            merge_strategy: åˆå¹¶ç­–ç•¥ (weighted_score/simple_concat/interleave)
            final_top_k: æœ€ç»ˆè¿”å›çš„ç»“æœæ•°é‡
            
        Returns:
            åˆå¹¶åçš„æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        if not kb_configs:
            logger.warning("âš ï¸ çŸ¥è¯†åº“é…ç½®åˆ—è¡¨ä¸ºç©º")
            return []
        
        logger.info(f"ğŸ” å¼€å§‹å¤šçŸ¥è¯†åº“æ£€ç´¢: query='{query[:50]}...', kb_count={len(kb_configs)}, "
                   f"top_k_per_kb={top_k_per_kb}, merge_strategy={merge_strategy}")
        
        # å¹¶è¡Œæ£€ç´¢æ‰€æœ‰çŸ¥è¯†åº“
        tasks = []
        for kb_config in kb_configs:
            task = self._retrieve_single_kb_with_semaphore(
                query=query,
                kb_config=kb_config,
                top_k=top_k_per_kb,
                similarity_threshold=similarity_threshold
            )
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰æ£€ç´¢ä»»åŠ¡å®Œæˆ (å¹¶è¡Œæ‰§è¡Œ)
        results_list = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤å¼‚å¸¸ç»“æœ
        valid_results = []
        for i, result in enumerate(results_list):
            if isinstance(result, Exception):
                kb_id = kb_configs[i].get('kb_id', 'unknown')
                logger.error(f"âŒ çŸ¥è¯†åº“ {kb_id} æ£€ç´¢å¤±è´¥: {result}")
            else:
                valid_results.append(result)
        
        if not valid_results:
            logger.warning("âš ï¸ æ‰€æœ‰çŸ¥è¯†åº“æ£€ç´¢éƒ½å¤±è´¥äº†")
            return []
        
        # åˆå¹¶ç»“æœ
        merged_results = self._merge_results(
            results_list=valid_results,
            merge_strategy=merge_strategy,
            final_top_k=final_top_k
        )
        
        logger.info(f"âœ… å¤šçŸ¥è¯†åº“æ£€ç´¢å®Œæˆ: æ€»ç»“æœæ•°={len(merged_results)}")
        return merged_results
    
    async def _retrieve_single_kb_with_semaphore(
        self,
        query: str,
        kb_config: Dict[str, Any],
        top_k: int,
        similarity_threshold: Optional[float]
    ) -> List[RetrievalResult]:
        """
        ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶çš„å•çŸ¥è¯†åº“æ£€ç´¢ (é˜²æ­¢å¹¶å‘è¿‡é«˜)
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            kb_config: çŸ¥è¯†åº“é…ç½® {kb_id, kb_name, kb_settings}
            top_k: è¿”å›ç»“æœæ•°
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        async with self._semaphore:  # ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            return await self._retrieve_single_kb(
                query=query,
                kb_config=kb_config,
                top_k=top_k,
                similarity_threshold=similarity_threshold
            )
    
    async def _retrieve_single_kb(
        self,
        query: str,
        kb_config: Dict[str, Any],
        top_k: int,
        similarity_threshold: Optional[float]
    ) -> List[RetrievalResult]:
        """
        å•ä¸ªçŸ¥è¯†åº“çš„å¼‚æ­¥æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            kb_config: çŸ¥è¯†åº“é…ç½®
            top_k: è¿”å›ç»“æœæ•°
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        kb_id = kb_config.get('kb_id', 'unknown')
        kb_name = kb_config.get('kb_name', kb_id)
        kb_settings = kb_config.get('kb_settings', {})
        
        try:
            logger.debug(f"ğŸ“š å¼€å§‹æ£€ç´¢çŸ¥è¯†åº“: {kb_name} (ID: {kb_id})")
            
            # æ„å»ºå‘é‡å­˜å‚¨å’Œæ£€ç´¢å™¨
            from ..routers.kb import _get_kb_components
            from ..utils.embedding.pipeline import Retriever
            
            _, vectorstore, _ = _get_kb_components(kb_settings)
            
            # âœ… ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“è‡ªå·±çš„ç›¸ä¼¼åº¦é˜ˆå€¼é…ç½®
            # å¦‚æœä¼šè¯çº§åˆ«ä¼ å…¥äº†é˜ˆå€¼ï¼ˆsimilarity_thresholdå‚æ•°ï¼‰ï¼Œåˆ™ä½œä¸ºå…œåº•é»˜è®¤å€¼
            kb_threshold = kb_settings.get("similarity_threshold")
            if kb_threshold is not None:
                # çŸ¥è¯†åº“æœ‰è‡ªå·±çš„é˜ˆå€¼é…ç½®ï¼Œä½¿ç”¨å®ƒ
                final_threshold = kb_threshold
                logger.info(f"ğŸ“Š ä½¿ç”¨çŸ¥è¯†åº“ {kb_name} è‡ªå·±çš„ç›¸ä¼¼åº¦é˜ˆå€¼: {final_threshold}")
            elif similarity_threshold is not None:
                # çŸ¥è¯†åº“æ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨ä¼šè¯çº§åˆ«çš„é˜ˆå€¼
                final_threshold = similarity_threshold
                logger.info(f"ğŸ“Š çŸ¥è¯†åº“ {kb_name} æœªé…ç½®é˜ˆå€¼ï¼Œä½¿ç”¨ä¼šè¯é»˜è®¤å€¼: {final_threshold}")
            else:
                # éƒ½æ²¡æœ‰ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤å€¼ 0.5ï¼ˆç›¸ä¼¼åº¦åˆ†æ•°ï¼‰
                final_threshold = 0.5
                logger.info(f"ğŸ“Š çŸ¥è¯†åº“ {kb_name} ä½¿ç”¨ç³»ç»Ÿé»˜è®¤é˜ˆå€¼: {final_threshold}")
            
            # è·å–è·ç¦»åº¦é‡ç±»å‹
            search_params = kb_settings.get("search_params", {})
            distance_metric = search_params.get("distance_metric", "cosine")
            
            retriever = Retriever(
                vector_store=vectorstore,
                top_k=top_k,
                similarity_threshold=final_threshold,
                distance_metric=distance_metric
            )
            
            # å¼‚æ­¥æ£€ç´¢
            search_results = await retriever.search(query, top_k=top_k)
            
            # ğŸ”§ æ‰¹é‡æŸ¥è¯¢æ–‡æ¡£åç§°
            from motor.motor_asyncio import AsyncIOMotorClient
            from ..config import settings
            from ..database import get_database
            
            doc_ids = []
            for doc, _ in search_results:
                doc_id = doc.metadata.get("doc_id")
                if doc_id:
                    doc_ids.append(doc_id)
            
            # æ‰¹é‡æŸ¥è¯¢æ–‡æ¡£åç§°
            filename_map = {}
            if doc_ids:
                try:
                    from bson import ObjectId
                    db = await anext(get_database())  # è·å–æ•°æ®åº“è¿æ¥
                    docs_cursor = db[settings.mongodb_db_name].documents.find(
                        {"_id": {"$in": [ObjectId(did) for did in doc_ids if ObjectId.is_valid(did)]}},
                        {"_id": 1, "filename": 1}
                    )
                    async for doc_record in docs_cursor:
                        filename_map[str(doc_record["_id"])] = doc_record.get("filename", "")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ‰¹é‡æŸ¥è¯¢filenameå¤±è´¥: {e}")
            
            # æ ¼å¼åŒ–ç»“æœ
            results = []
            for doc, distance in search_results:
                # æ ¹æ®è·ç¦»åº¦é‡ç±»å‹è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
                score = calculate_score_from_distance(distance, distance_metric)
                
                # è·å–æ–‡æ¡£åç§°
                doc_id = doc.metadata.get("doc_id")
                filename = doc.metadata.get("filename") or filename_map.get(doc_id, "")
                
                result = RetrievalResult(
                    content=doc.page_content,
                    score=score,
                    distance=float(distance),
                    metadata=doc.metadata,
                    kb_id=kb_id,
                    kb_name=kb_name,
                    chunk_id=doc.metadata.get("chunk_id"),
                    doc_id=doc_id,
                    document_name=filename or doc.metadata.get("source", "æœªçŸ¥æ–‡æ¡£")  # ğŸ†• æ·»åŠ æ–‡æ¡£åç§°
                )
                results.append(result)
            
            logger.debug(f"âœ… çŸ¥è¯†åº“ {kb_name} æ£€ç´¢å®Œæˆ: {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†åº“ {kb_name} æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
            raise  # æŠ›å‡ºå¼‚å¸¸ä¾›ä¸Šå±‚å¤„ç†
    
    def _merge_results(
        self,
        results_list: List[List[RetrievalResult]],
        merge_strategy: str,
        final_top_k: int
    ) -> List[RetrievalResult]:
        """
        åˆå¹¶å¤šä¸ªçŸ¥è¯†åº“çš„æ£€ç´¢ç»“æœ
        
        Args:
            results_list: å¤šä¸ªçŸ¥è¯†åº“çš„æ£€ç´¢ç»“æœåˆ—è¡¨
            merge_strategy: åˆå¹¶ç­–ç•¥
            final_top_k: æœ€ç»ˆè¿”å›çš„ç»“æœæ•°é‡
            
        Returns:
            åˆå¹¶åçš„ç»“æœåˆ—è¡¨
        """
        if not results_list:
            return []
        
        if merge_strategy == "weighted_score":
            return self._merge_by_weighted_score(results_list, final_top_k)
        elif merge_strategy == "simple_concat":
            return self._merge_by_simple_concat(results_list, final_top_k)
        elif merge_strategy == "interleave":
            return self._merge_by_interleave(results_list, final_top_k)
        else:
            logger.warning(f"âš ï¸ æœªçŸ¥çš„åˆå¹¶ç­–ç•¥ '{merge_strategy}', ä½¿ç”¨é»˜è®¤ç­–ç•¥ 'weighted_score'")
            return self._merge_by_weighted_score(results_list, final_top_k)
    
    def _merge_by_weighted_score(
        self,
        results_list: List[List[RetrievalResult]],
        final_top_k: int
    ) -> List[RetrievalResult]:
        """
        åŠ æƒåˆ†æ•°åˆå¹¶ç­–ç•¥
        
        ç®—æ³•:
        1. å¯¹æ¯ä¸ªçŸ¥è¯†åº“çš„ç»“æœè¿›è¡Œå½’ä¸€åŒ– (MinMaxå½’ä¸€åŒ–)
        2. å»é‡ (ç›¸åŒå†…å®¹çš„ä¿ç•™æœ€é«˜åˆ†)
        3. æŒ‰åˆ†æ•°é™åºæ’åº
        4. è¿”å› top_k
        """
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_results = []
        for results in results_list:
            all_results.extend(results)
        
        if not all_results:
            return []
        
        # å»é‡: ä½¿ç”¨å†…å®¹å“ˆå¸Œå»é‡,ä¿ç•™åˆ†æ•°æœ€é«˜çš„
        deduplicated = self._deduplicate_by_content(all_results)
        
        # æŒ‰åˆ†æ•°é™åºæ’åº
        sorted_results = sorted(deduplicated, key=lambda x: x.score, reverse=True)
        
        return sorted_results[:final_top_k]
    
    def _merge_by_simple_concat(
        self,
        results_list: List[List[RetrievalResult]],
        final_top_k: int
    ) -> List[RetrievalResult]:
        """
        ç®€å•æ‹¼æ¥ç­–ç•¥
        
        ç®—æ³•:
        1. ä¾æ¬¡æ‹¼æ¥æ‰€æœ‰çŸ¥è¯†åº“çš„ç»“æœ
        2. å»é‡
        3. è¿”å› top_k
        """
        all_results = []
        for results in results_list:
            all_results.extend(results)
        
        # å»é‡
        deduplicated = self._deduplicate_by_content(all_results)
        
        return deduplicated[:final_top_k]
    
    def _merge_by_interleave(
        self,
        results_list: List[List[RetrievalResult]],
        final_top_k: int
    ) -> List[RetrievalResult]:
        """
        äº¤é”™åˆå¹¶ç­–ç•¥ (è½®æµå–æ¯ä¸ªçŸ¥è¯†åº“çš„ç»“æœ)
        
        ç®—æ³•:
        1. è½®æµä»æ¯ä¸ªçŸ¥è¯†åº“å–ä¸€ä¸ªç»“æœ
        2. å»é‡
        3. è¿”å› top_k
        """
        merged = []
        max_len = max(len(results) for results in results_list) if results_list else 0
        
        for i in range(max_len):
            for results in results_list:
                if i < len(results):
                    merged.append(results[i])
        
        # å»é‡
        deduplicated = self._deduplicate_by_content(merged)
        
        return deduplicated[:final_top_k]
    
    def _deduplicate_by_content(
        self,
        results: List[RetrievalResult]
    ) -> List[RetrievalResult]:
        """
        æ ¹æ®å†…å®¹å»é‡,ä¿ç•™åˆ†æ•°æœ€é«˜çš„
        
        Args:
            results: ç»“æœåˆ—è¡¨
            
        Returns:
            å»é‡åçš„ç»“æœåˆ—è¡¨
        """
        content_hash_map: Dict[str, RetrievalResult] = {}
        
        for result in results:
            # è®¡ç®—å†…å®¹å“ˆå¸Œ
            content_hash = hashlib.md5(result.content.encode('utf-8')).hexdigest()
            
            # å¦‚æœå·²å­˜åœ¨,æ¯”è¾ƒåˆ†æ•°,ä¿ç•™æ›´é«˜çš„
            if content_hash in content_hash_map:
                if result.score > content_hash_map[content_hash].score:
                    content_hash_map[content_hash] = result
            else:
                content_hash_map[content_hash] = result
        
        return list(content_hash_map.values())
    
    def format_results_for_api(
        self,
        results: List[RetrievalResult]
    ) -> List[Dict[str, Any]]:
        """
        æ ¼å¼åŒ–ç»“æœä¸ºAPIå“åº”æ ¼å¼
        
        Args:
            results: æ£€ç´¢ç»“æœåˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„å­—å…¸åˆ—è¡¨
        """
        return [
            {
                "content": r.content,
                "score": r.score,
                "distance": r.distance,
                "metadata": r.metadata,
                "kb_id": r.kb_id,
                "kb_name": r.kb_name,
                "chunk_id": r.chunk_id,
                "doc_id": r.doc_id,
                "document_name": r.document_name  # ğŸ†• æ·»åŠ æ–‡æ¡£åç§°
            }
            for r in results
        ]


# å…¨å±€å•ä¾‹ (çº¿ç¨‹å®‰å…¨)
_multi_kb_retriever: Optional[MultiKBRetriever] = None
_retriever_lock = asyncio.Lock()


async def get_multi_kb_retriever() -> MultiKBRetriever:
    """
    è·å–å¤šçŸ¥è¯†åº“æ£€ç´¢å™¨å•ä¾‹
    
    Returns:
        MultiKBRetrieverå®ä¾‹
    """
    global _multi_kb_retriever
    
    if _multi_kb_retriever is None:
        async with _retriever_lock:
            if _multi_kb_retriever is None:  # åŒé‡æ£€æŸ¥
                _multi_kb_retriever = MultiKBRetriever()
    
    return _multi_kb_retriever

