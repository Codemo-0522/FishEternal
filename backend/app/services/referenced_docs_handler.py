"""
å¼•ç”¨æ–‡æ¡£æ™ºèƒ½å¤„ç†å™¨

æ ¹æ®æ–‡æ¡£å¤§å°è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç­–ç•¥ï¼š
1. å°æ–‡æ¡£ (< 8000 tokens): ç›´æ¥å…¨æ–‡æ³¨å…¥åˆ°ç”¨æˆ·æ¶ˆæ¯
2. å¤§æ–‡æ¡£ (>= 8000 tokens): ä½¿ç”¨ RAG æ£€ç´¢ç›¸å…³ç‰‡æ®µ
"""
import logging
import re
from typing import List, Dict, Any, Optional, Tuple
from motor.motor_asyncio import AsyncIOMotorClient
from bson import ObjectId
import tiktoken

logger = logging.getLogger(__name__)

# Token é˜ˆå€¼é…ç½®
SMALL_DOC_TOKEN_THRESHOLD = 8000  # å°äºæ­¤å€¼ç›´æ¥å…¨æ–‡æ³¨å…¥
MAX_TOTAL_TOKENS = 32000  # æ‰€æœ‰å¼•ç”¨æ–‡æ¡£æ€» token ä¸Šé™


def clean_referenced_content(text: str) -> str:
    """
    æ¸…æ´—æ–‡æœ¬ä¸­çš„å¼•ç”¨æ–‡æ¡£æ³¨å…¥å†…å®¹ï¼ˆç§»é™¤ <referenced_documents>...</referenced_documents> æ ‡ç­¾åŠå…¶å†…å®¹ï¼‰
    
    Args:
        text: åŒ…å«å¼•ç”¨æ–‡æ¡£æ³¨å…¥çš„æ–‡æœ¬
    
    Returns:
        æ¸…æ´—åçš„æ–‡æœ¬ï¼ˆç§»é™¤äº†æ‰€æœ‰å¼•ç”¨æ–‡æ¡£æ³¨å…¥å†…å®¹ï¼‰
    """
    # ç§»é™¤ <referenced_documents>...</referenced_documents> åŠå…¶å†…éƒ¨æ‰€æœ‰å†…å®¹
    # ä½¿ç”¨ DOTALL æ ‡å¿—è®© . åŒ¹é…æ¢è¡Œç¬¦
    cleaned = re.sub(
        r'<referenced_documents>.*?</referenced_documents>\s*',
        '',
        text,
        flags=re.DOTALL
    )
    return cleaned.strip()


class ReferencedDocsHandler:
    """å¼•ç”¨æ–‡æ¡£æ™ºèƒ½å¤„ç†å™¨"""
    
    def __init__(self, db: AsyncIOMotorClient, db_name: str):
        self.db = db
        self.db_name = db_name
        try:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")  # GPT-4 ç¼–ç 
        except Exception as e:
            logger.warning(f"tiktokenåˆå§‹åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•ä¼°ç®—")
            self.tokenizer = None
    
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
        if self.tokenizer:
            try:
                return len(self.tokenizer.encode(text))
            except:
                pass
        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡çº¦ 1.5 å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦ 4 å­—ç¬¦/token
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        other_chars = len(text) - chinese_chars
        return int(chinese_chars / 1.5 + other_chars / 4)
    
    async def get_document_content(self, doc_id: str, user_id: str) -> Optional[str]:
        """
        è·å–æ–‡æ¡£çš„å®Œæ•´æ–‡æœ¬å†…å®¹
        
        æµç¨‹ï¼š
        1. ä» MongoDB æŸ¥è¯¢æ–‡æ¡£è®°å½•ï¼Œè·å– file_url (MinIOè·¯å¾„)
        2. ä» MinIO ä¸‹è½½åŸå§‹æ–‡ä»¶
        3. è§£ææ–‡ä»¶æå–æ–‡æœ¬å†…å®¹
        """
        try:
            from ..utils.minio_client import minio_client
            from app.utils.document_parsers import DocumentParserFactory
            
            # å°†å­—ç¬¦ä¸² doc_id è½¬æ¢ä¸º ObjectId
            try:
                doc_object_id = ObjectId(doc_id)
            except Exception as e:
                logger.error(f"æ— æ•ˆçš„ doc_id æ ¼å¼: {doc_id}, é”™è¯¯: {e}")
                return None
            
            # ä» knowledge_base_documents é›†åˆä¸­è·å–æ–‡æ¡£è®°å½•
            doc = await self.db[self.db_name].kb_documents.find_one({
                "_id": doc_object_id
            })
            
            if not doc:
                logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨: doc_id={doc_id}")
                return None
            
            # è·å– MinIO æ–‡ä»¶è·¯å¾„
            file_url = doc.get("file_url")
            if not file_url:
                logger.warning(f"æ–‡æ¡£æ²¡æœ‰å…³è”çš„æ–‡ä»¶: doc_id={doc_id}")
                return None
            
            # ä» MinIO ä¸‹è½½æ–‡æ¡£
            logger.info(f"ä» MinIO ä¸‹è½½æ–‡æ¡£: {file_url}")
            file_content = minio_client.download_kb_document(file_url)
            
            # è§£ææ–‡æ¡£å†…å®¹ï¼ˆæå–æ–‡æœ¬ï¼‰
            if not hasattr(DocumentParserFactory, '_initialized'):
                DocumentParserFactory.initialize_default_parsers()
                DocumentParserFactory._initialized = True
            
            filename = doc.get("filename", "unknown.txt")
            parse_result = await DocumentParserFactory.parse_document(
                file_content,
                filename
            )
            
            if not parse_result.success:
                logger.error(f"æ–‡æ¡£è§£æå¤±è´¥: {parse_result.error_message}")
                return None
            
            logger.info(f"âœ… æˆåŠŸè·å–æ–‡æ¡£å†…å®¹: {filename}, é•¿åº¦: {len(parse_result.text)} å­—ç¬¦")
            return parse_result.text
        
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£å†…å®¹å¤±è´¥: {e}", exc_info=True)
            return None
    
    async def process_referenced_docs(
        self,
        referenced_docs: List[Dict[str, Any]],
        user_id: str,
        query: Optional[str] = None
    ) -> tuple[Optional[str], Optional[str]]:
        """
        æ™ºèƒ½å¤„ç†å¼•ç”¨æ–‡æ¡£
        
        ç­–ç•¥ï¼š
        1. å°æ–‡æ¡£ (< 8000 tokens): ç›´æ¥å…¨æ–‡æ³¨å…¥åˆ°ç”¨æˆ·æ¶ˆæ¯
        2. å¤§æ–‡æ¡£ (>= 8000 tokens): æç¤ºç”¨æˆ·æ–‡æ¡£å¤ªå¤§ï¼Œå»ºè®®ä½¿ç”¨çŸ¥è¯†åº“é—®ç­”
        
        Args:
            referenced_docs: å¼•ç”¨æ–‡æ¡£åˆ—è¡¨ [{"doc_id": "xxx", "filename": "xxx"}]
            user_id: ç”¨æˆ· ID
            query: ç”¨æˆ·é—®é¢˜ï¼ˆç”¨äº RAG æ£€ç´¢ï¼‰
        
        Returns:
            (user_message_content, system_prompt_addition):
                - user_message_content: æ³¨å…¥åˆ°ç”¨æˆ·æ¶ˆæ¯çš„å†…å®¹ï¼ˆ@æ–‡æ¡£ï¼‰
                - system_prompt_addition: æ³¨å…¥åˆ°ç³»ç»Ÿæç¤ºè¯çš„å†…å®¹ï¼ˆ@çŸ¥è¯†åº“ï¼‰
        """
        if not referenced_docs:
            return None, None
        
        logger.info(f"ğŸ“„ å¼€å§‹å¤„ç† {len(referenced_docs)} ä¸ªå¼•ç”¨æ–‡æ¡£")
        
        small_docs = []  # å°æ–‡æ¡£ï¼šç›´æ¥æ³¨å…¥
        large_docs = []  # å¤§æ–‡æ¡£ï¼šæç¤ºä¿¡æ¯
        total_tokens = 0
        kb_mentioned = False  # æ ‡è®°æ˜¯å¦@äº†çŸ¥è¯†åº“
        
        for doc_info in referenced_docs:
            doc_id = doc_info.get("doc_id")
            filename = doc_info.get("filename", "æœªçŸ¥æ–‡æ¡£")
            
            # ğŸ†• ç‰¹æ®Šå¤„ç†ï¼š@çŸ¥è¯†åº“æ ‡è®°
            if filename == "çŸ¥è¯†åº“" and doc_id == "knowledge-base":
                kb_mentioned = True
                logger.info(f"ğŸ“š æ£€æµ‹åˆ° @çŸ¥è¯†åº“ æ ‡è®°ï¼Œå°†æ³¨å…¥çŸ¥è¯†åº“æç¤ºè¯")
                continue
            
            # è·å–æ–‡æ¡£å†…å®¹
            content = await self.get_document_content(doc_id, user_id)
            if not content:
                logger.warning(f"è·³è¿‡æ— å†…å®¹çš„æ–‡æ¡£: {filename}")
                continue
            
            # è®¡ç®— token æ•°
            token_count = self.count_tokens(content)
            logger.info(f"ğŸ“Š æ–‡æ¡£ '{filename}' çš„ token æ•°: {token_count}")
            
            # åˆ¤æ–­ç­–ç•¥
            if token_count < SMALL_DOC_TOKEN_THRESHOLD and (total_tokens + token_count) < MAX_TOTAL_TOKENS:
                # å°æ–‡æ¡£ï¼šç›´æ¥æ³¨å…¥
                small_docs.append({
                    "filename": filename,
                    "content": content,
                    "tokens": token_count
                })
                total_tokens += token_count
                logger.info(f"âœ… '{filename}' å½’ç±»ä¸ºå°æ–‡æ¡£ï¼Œå°†ç›´æ¥æ³¨å…¥ (tokens: {token_count})")
            else:
                # å¤§æ–‡æ¡£ï¼šè®°å½•ä¿¡æ¯ï¼Œç¨åæç¤º
                large_docs.append({
                    "filename": filename,
                    "tokens": token_count
                })
                logger.info(f"âš ï¸ '{filename}' å½’ç±»ä¸ºå¤§æ–‡æ¡£ï¼Œæ— æ³•ç›´æ¥æ³¨å…¥ (tokens: {token_count})")
        
        # ğŸ†• åˆ†ç¦»ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æ–‡æ¡£å†…å®¹
        system_prompt_addition = None
        user_message_parts = []
        
        # ğŸ†• å¦‚æœç”¨æˆ·@äº†çŸ¥è¯†åº“ï¼Œç”Ÿæˆç³»ç»Ÿæç¤ºè¯ï¼ˆæ³¨å…¥åˆ° system promptï¼‰
        if kb_mentioned:
            system_prompt_addition = (
                "\n\nã€çŸ¥è¯†åº“æ¨¡å¼ã€‘\n"
                "ç”¨æˆ·å¸Œæœ›ä½¿ç”¨çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·ä¼˜å…ˆåŸºäºçŸ¥è¯†åº“æ£€ç´¢åˆ°çš„å†…å®¹è¿›è¡Œå›ç­”ã€‚"
            )
            logger.info(f"ğŸ“š ç”ŸæˆçŸ¥è¯†åº“ç³»ç»Ÿæç¤ºè¯ï¼Œé•¿åº¦: {len(system_prompt_addition)}")
        
        # æ·»åŠ å°æ–‡æ¡£çš„å®Œæ•´å†…å®¹ï¼ˆæ¯ä¸ªæ–‡æ¡£ç”¨å•ç‹¬çš„ XML æ ‡ç­¾ï¼‰
        if small_docs:
            for doc in small_docs:
                user_message_parts.append(
                    f"<document filename=\"{doc['filename']}\" tokens=\"{doc['tokens']}\">\n"
                    f"{doc['content']}\n"
                    f"</document>\n\n"
                )
        
        # æ·»åŠ å¤§æ–‡æ¡£çš„æç¤º
        if large_docs:
            user_message_parts.append("<large_documents>\n")
            for doc in large_docs:
                user_message_parts.append(
                    f"<large_doc filename=\"{doc['filename']}\" tokens=\"{doc['tokens']}\">\n"
                    f"æ­¤æ–‡æ¡£å› å†…å®¹è¿‡å¤šæ— æ³•å®Œæ•´åŠ è½½ï¼ˆçº¦ {doc['tokens']} tokensï¼Œè¶…è¿‡é˜ˆå€¼ï¼‰ã€‚\n"
                    f"å»ºè®®ï¼šè¯·å»ºè®®ç”¨æˆ·åœ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢æ­¤æ–‡æ¡£çš„ç›¸å…³å†…å®¹ï¼Œæˆ–ç¼©å°é—®é¢˜èŒƒå›´ã€‚\n"
                    f"</large_doc>\n"
                )
            user_message_parts.append("</large_documents>\n\n")
        
        # æ„å»ºç”¨æˆ·æ¶ˆæ¯æ³¨å…¥å†…å®¹
        user_message_content = None
        if user_message_parts:
            # ç”¨æœ€å¤–å±‚çš„ XML æ ‡ç­¾åŒ…è£¹æ‰€æœ‰å¼•ç”¨æ–‡æ¡£å†…å®¹
            user_message_content = (
                "<referenced_documents>\n"
                + ''.join(user_message_parts) +
                "</referenced_documents>"
            )
        
        logger.info(f"âœ… å¤„ç†å®Œæˆ - å°æ–‡æ¡£: {len(small_docs)} (æ€» {total_tokens} tokens), å¤§æ–‡æ¡£: {len(large_docs)}")
        if system_prompt_addition:
            logger.info(f"âœ… ç”Ÿæˆç³»ç»Ÿæç¤ºè¯æ³¨å…¥å†…å®¹")
        if user_message_content:
            logger.info(f"âœ… ç”Ÿæˆç”¨æˆ·æ¶ˆæ¯æ³¨å…¥å†…å®¹")
        
        return user_message_content, system_prompt_addition

