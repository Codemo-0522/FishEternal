from typing import List, Optional, Tuple
import threading
import asyncio
import logging
from concurrent.futures import ThreadPoolExecutor

from langchain_core.embeddings import Embeddings
from langchain_core.documents import Document

from .interfaces import VectorStoreLike

logger = logging.getLogger(__name__)

# âš¡ åå°çº¿ç¨‹é¢„åŠ è½½ Chromaï¼Œé¿å…å¯åŠ¨æ—¶é˜»å¡ä¸»çº¿ç¨‹
_CHROMA_AVAILABLE = None
_Chroma = None
_chroma_loading = False
_chroma_lock = threading.Lock()
_chroma_loaded_event = threading.Event()

def _preload_chroma_in_background():
	"""åœ¨åå°çº¿ç¨‹é¢„åŠ è½½ Chromaï¼Œä¸é˜»å¡ä¸»å¯åŠ¨æµç¨‹"""
	global _CHROMA_AVAILABLE, _Chroma, _chroma_loading
	
	with _chroma_lock:
		if _chroma_loading or _CHROMA_AVAILABLE is not None:
			return  # å·²ç»åœ¨åŠ è½½æˆ–å·²åŠ è½½å®Œæˆ
		_chroma_loading = True
	
	def _load():
		global _CHROMA_AVAILABLE, _Chroma
		try:
			logger.info("åå°é¢„åŠ è½½ ChromaDB å¼€å§‹...")
			# ä¼˜å…ˆä½¿ç”¨å®˜æ–¹æ‹†åˆ†åŒ… langchain-chromaï¼Œå…¶æ¬¡å›é€€åˆ° langchain_community
			try:
				from langchain_chroma import Chroma  # æ–°ç‰ˆå®˜æ–¹é›†æˆåŒ…
				_CHROMA_AVAILABLE = True
				_Chroma = Chroma
				logger.info("âœ“ ChromaDB é¢„åŠ è½½æˆåŠŸ (langchain-chroma)")
			except Exception:
				try:
					from langchain_community.vectorstores import Chroma  # æ—§ç‰ˆ/ç¤¾åŒºåŒ…
					_CHROMA_AVAILABLE = True
					_Chroma = Chroma
					logger.info("âœ“ ChromaDB é¢„åŠ è½½æˆåŠŸ (langchain_community)")
				except Exception as e:
					_CHROMA_AVAILABLE = False
					_Chroma = None
					logger.warning(f"ChromaDB é¢„åŠ è½½å¤±è´¥: {e}")
		finally:
			_chroma_loaded_event.set()  # æ ‡è®°åŠ è½½å®Œæˆ
	
	thread = threading.Thread(target=_load, daemon=True, name="ChromaPreloader")
	thread.start()

def _get_chroma(timeout: float = 30.0):
	"""
	è·å– Chroma ç±»ï¼Œå¦‚æœæ­£åœ¨åå°åŠ è½½åˆ™ç­‰å¾…åŠ è½½å®Œæˆ
	
	Args:
		timeout: ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
	
	Returns:
		Chroma ç±»æˆ– None
	"""
	global _CHROMA_AVAILABLE, _Chroma
	
	# å¦‚æœå·²ç»åŠ è½½å®Œæˆï¼Œç›´æ¥è¿”å›
	if _CHROMA_AVAILABLE is not None:
		return _Chroma
	
	# ç­‰å¾…åå°åŠ è½½å®Œæˆ
	if _chroma_loading:
		logger.info(f"ç­‰å¾… ChromaDB åå°åŠ è½½å®Œæˆï¼ˆæœ€å¤šç­‰å¾… {timeout}ç§’ï¼‰...")
		if _chroma_loaded_event.wait(timeout=timeout):
			logger.info("ChromaDB åŠ è½½å®Œæˆ")
		else:
			logger.warning(f"ç­‰å¾… ChromaDB åŠ è½½è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰")
	
	return _Chroma


class ChromaVectorStore(VectorStoreLike):
	"""
	Chroma çš„è½»é‡å°è£…ã€‚é€šè¿‡ä¼ å…¥ Embeddings ä¸æŒä¹…åŒ–å‚æ•°è¿›è¡Œæ„é€ ï¼Œé¿å…åœ¨
	é¡¹ç›®å…¶ä»–æ¨¡å—ä¸­å‡ºç°å¯¹ Chroma çš„ç›´æ¥ä¾èµ–ã€‚
	
	æ‰€æœ‰æ£€ç´¢æ–¹æ³•å‡ä¸ºå¼‚æ­¥å®ç°ï¼Œä½¿ç”¨çº¿ç¨‹æ± åŒ…è£… ChromaDB çš„åŒæ­¥è°ƒç”¨ã€‚
	"""

	# å…±äº«çº¿ç¨‹æ± ï¼Œç”¨äºå¼‚æ­¥è°ƒç”¨ï¼ˆé¿å…åˆ›å»ºè¿‡å¤šçº¿ç¨‹ï¼‰
	_executor: Optional[ThreadPoolExecutor] = None
	_executor_lock = threading.Lock()

	@classmethod
	def _get_executor(cls) -> ThreadPoolExecutor:
		"""è·å–å…±äº«çš„çº¿ç¨‹æ± ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
		if cls._executor is None:
			with cls._executor_lock:
				if cls._executor is None:
					# åˆ›å»ºå›ºå®šå¤§å°çš„çº¿ç¨‹æ± ï¼Œé¿å…æ— é™åˆ¶åˆ›å»ºçº¿ç¨‹
					cls._executor = ThreadPoolExecutor(
						max_workers=4,  # æœ€å¤š4ä¸ªå¹¶å‘æ£€ç´¢
						thread_name_prefix="VectorStore"
					)
		return cls._executor

	def __init__(
		self,
		embedding_function: Embeddings,
		persist_directory: Optional[str] = None,
		collection_name: Optional[str] = None,
		client_settings: Optional[dict] = None,
		distance_metric: str = "cosine",  # æ–°å¢ï¼šè·ç¦»åº¦é‡å‚æ•°
	):
		"""
		åˆå§‹åŒ– ChromaVectorStore
		
		Args:
			embedding_function: Embedding å‡½æ•°
			persist_directory: æŒä¹…åŒ–ç›®å½•
			collection_name: é›†åˆåç§°
			client_settings: å®¢æˆ·ç«¯è®¾ç½®
			distance_metric: è·ç¦»åº¦é‡æ–¹å¼ ("cosine", "l2", "ip")
		"""
		# âš¡ è·å– Chromaï¼ˆå¦‚æœæ­£åœ¨åå°åŠ è½½åˆ™ç­‰å¾…ï¼‰
		Chroma = _get_chroma(timeout=30.0)
		if Chroma is None:
			raise RuntimeError(
				"æœªæ£€æµ‹åˆ° Chroma é›†æˆï¼Œè¯·å®‰è£…: pip install -U langchain-chroma æˆ– pip install -U langchain-community"
			)

		# éªŒè¯è·ç¦»åº¦é‡å‚æ•°
		valid_metrics = ["cosine", "l2", "ip"]
		if distance_metric not in valid_metrics:
			logger.warning(f"æ— æ•ˆçš„è·ç¦»åº¦é‡ '{distance_metric}'ï¼Œä½¿ç”¨é»˜è®¤å€¼ 'cosine'")
			distance_metric = "cosine"
		
		self._distance_metric = distance_metric
		self._persist_directory = persist_directory  # ä¿å­˜ä»¥ä¾›WAL checkpointä½¿ç”¨
		self._collection_name = collection_name  # ä¿å­˜collectionåç§°
		logger.info(f"ğŸ¯ ChromaVectorStore ä½¿ç”¨è·ç¦»åº¦é‡: {distance_metric}")

		kwargs = {
			"embedding_function": embedding_function,
			"persist_directory": persist_directory,
			"collection_name": collection_name,
			# Chroma é€šè¿‡ collection_metadata è®¾ç½®è·ç¦»åº¦é‡
			"collection_metadata": {"hnsw:space": distance_metric}
		}
		if client_settings is not None:
			kwargs["client_settings"] = client_settings

		self._store = Chroma(**kwargs)
		
		# ğŸ”¥ å…³é”®ä¿®å¤ï¼šåˆ›å»ºåç«‹å³éªŒè¯UUIDä¸€è‡´æ€§
		if persist_directory and collection_name:
			self._verify_and_fix_uuid_consistency(persist_directory, collection_name)
	
	def _verify_and_fix_uuid_consistency(self, persist_directory: str, collection_name: str):
		"""
		éªŒè¯å¹¶ä¿®å¤ChromaDBçš„UUIDä¸€è‡´æ€§é—®é¢˜
		
		é—®é¢˜èƒŒæ™¯ï¼š
		åœ¨å¹¶å‘ç¯å¢ƒä¸‹ï¼ŒChromaDBçš„get_or_create_collection()å¯èƒ½å¯¼è‡´ï¼š
		- SQLiteä¸­è®°å½•çš„UUIDä¸æ–‡ä»¶ç³»ç»Ÿä¸­çš„UUIDç›®å½•ä¸åŒ¹é…
		- å¯¼è‡´åç»­è¯»å–æ—¶æŠ¥é”™ï¼šError loading hnsw index
		
		è§£å†³æ–¹æ¡ˆï¼š
		1. è¯»å–SQLiteä¸­çš„expected_uuid
		2. æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿä¸­çš„å®é™…UUIDç›®å½•
		3. å¦‚æœä¸åŒ¹é…ï¼Œé‡å‘½åç›®å½•ä»¥ä¿æŒä¸€è‡´æ€§
		"""
		import sqlite3
		from pathlib import Path
		import shutil
		
		try:
			# 1. ä»SQLiteè¯»å–expected UUID
			db_path = Path(persist_directory) / "chroma.sqlite3"
			if not db_path.exists():
				logger.warning(f"âš ï¸ SQLiteæ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
				return
			
			conn = sqlite3.connect(str(db_path))
			cursor = conn.cursor()
			cursor.execute(
				"SELECT id FROM collections WHERE name = ?",
				(collection_name,)
			)
			row = cursor.fetchone()
			conn.close()
			
			if not row:
				logger.warning(f"âš ï¸ æœªæ‰¾åˆ°collection: {collection_name}")
				return
			
			expected_uuid = row[0]
			expected_dir = Path(persist_directory) / expected_uuid
			
			# 2. æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿä¸­çš„UUIDç›®å½•
			uuid_dirs = [
				d for d in Path(persist_directory).iterdir()
				if d.is_dir() and d.name not in ["chroma.sqlite3"]
			]
			
			# è¿‡æ»¤å‡ºçœ‹èµ·æ¥åƒUUIDçš„ç›®å½•
			import re
			uuid_pattern = re.compile(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', re.I)
			uuid_dirs = [d for d in uuid_dirs if uuid_pattern.match(d.name)]
			
			# 3. æ£€æŸ¥æ˜¯å¦åŒ¹é…
			if expected_dir.exists():
				# åŒ¹é…ï¼æ¸…ç†é¢å¤–çš„UUIDç›®å½•
				extra_dirs = [d for d in uuid_dirs if d != expected_dir]
				if extra_dirs:
					logger.warning(f"ğŸ—‘ï¸ å‘ç°{len(extra_dirs)}ä¸ªé¢å¤–çš„UUIDç›®å½•ï¼Œæ¸…ç†ä¸­...")
					for extra_dir in extra_dirs:
						try:
							shutil.rmtree(extra_dir)
							logger.info(f"  âœ… å·²åˆ é™¤: {extra_dir.name}")
						except Exception as e:
							logger.error(f"  âŒ åˆ é™¤å¤±è´¥ {extra_dir.name}: {e}")
			else:
				# ä¸åŒ¹é…ï¼éœ€è¦ä¿®å¤
				if len(uuid_dirs) == 1:
					actual_dir = uuid_dirs[0]
					logger.warning(
						f"âš ï¸ UUIDä¸åŒ¹é…! "
						f"SQLite={expected_uuid}, æ–‡ä»¶ç³»ç»Ÿ={actual_dir.name}"
					)
					logger.info(f"ğŸ”§ è‡ªåŠ¨ä¿®å¤ï¼šé‡å‘½å {actual_dir.name} â†’ {expected_uuid}")
					try:
						actual_dir.rename(expected_dir)
						logger.info("âœ… UUIDå·²ä¿®å¤")
					except Exception as e:
						logger.error(f"âŒ UUIDä¿®å¤å¤±è´¥: {e}")
				elif len(uuid_dirs) > 1:
					logger.error(
						f"âŒ å‘ç°å¤šä¸ªUUIDç›®å½•: {[d.name for d in uuid_dirs]}, "
						f"expected: {expected_uuid}"
					)
				else:
					logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½•UUIDç›®å½•ï¼Œexpected: {expected_uuid}")
					
		except Exception as e:
			logger.error(f"âŒ UUIDä¸€è‡´æ€§éªŒè¯å¤±è´¥: {e}", exc_info=True)

	def add_documents(self, documents: List[Document], ids: Optional[List[str]] = None) -> None:
		"""
		ğŸ”’ å†…éƒ¨æ–¹æ³•ï¼šæ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
		
		âš ï¸ è­¦å‘Šï¼šæ­¤æ–¹æ³•ä»…ä¾› VectorStoreWithLock å†…éƒ¨ä½¿ç”¨ï¼ˆåœ¨æ–‡ä»¶é”ä¿æŠ¤ä¸‹è°ƒç”¨ï¼‰
		
		å¤–éƒ¨ä»£ç åº”ä½¿ç”¨:
		  vectorstore_mgr = get_vectorstore_manager()
		  vectorstore = vectorstore_mgr.get_or_create(...)
		  await vectorstore.add_documents_async(documents, ids)
		
		ç›´æ¥è°ƒç”¨æ­¤æ–¹æ³•ä¼šå¯¼è‡´å¤šè¿›ç¨‹å¹¶å‘å†™å…¥ï¼Œé€ æˆç´¢å¼•æŸåï¼
		"""
		# æ·»åŠ ç±»å‹æ£€æŸ¥å’Œè°ƒè¯•æ—¥å¿—
		if not documents:
			logger.warning("add_documents æ”¶åˆ°ç©ºæ–‡æ¡£åˆ—è¡¨")
			return
			
		# æ£€æŸ¥ documents æ˜¯å¦éƒ½æ˜¯ Document å¯¹è±¡
		for idx, doc in enumerate(documents):
			if not isinstance(doc, Document):
				logger.error(f"æ–‡æ¡£ {idx} ä¸æ˜¯ Document å¯¹è±¡ï¼Œç±»å‹: {type(doc)}, å€¼: {doc!r}")
				raise TypeError(f"æœŸæœ› Document å¯¹è±¡ï¼Œä½†æ”¶åˆ° {type(doc).__name__}")
		
		# ç›´æ¥ä¼ é€’ç»™åº•å±‚ Chroma
		self._store.add_documents(documents, ids=ids)

	async def similarity_search_with_score(self, query: str, k: int = 4) -> List[Tuple[Document, float]]:
		"""å¼‚æ­¥ç›¸ä¼¼åº¦æ£€ç´¢ï¼ˆä½¿ç”¨çº¿ç¨‹æ± åŒ…è£…ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
		
		å°†åŒæ­¥çš„ embedding + å‘é‡æ£€ç´¢æ“ä½œæ”¾åˆ°çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œ
		é¿å…é˜»å¡ asyncio äº‹ä»¶å¾ªç¯ã€‚
		"""
		loop = asyncio.get_event_loop()
		executor = self._get_executor()
		
		# åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥æ“ä½œ
		result = await loop.run_in_executor(
			executor,
			self._store.similarity_search_with_score,
			query,
			k
		)
		return result

	def _get_by_ids_sync(self, ids: List[str]) -> List[Document]:
		"""å†…éƒ¨åŒæ­¥æ–¹æ³•ï¼šç”¨äºçº¿ç¨‹æ± æ‰§è¡Œ"""
		if not ids:
			return []
		
		# è°ƒè¯•ä¿¡æ¯
		logger.info(f"ğŸ” ChromaVectorStore.get_by_ids: æŸ¥è¯¢ {len(ids)} ä¸ª chunk_id")
		logger.info(f"ğŸ” ChromaVectorStore.get_by_ids: collection_name={self._store._collection.name}")
		logger.info(f"ğŸ” ChromaVectorStore.get_by_ids: æŸ¥è¯¢çš„ ids={ids[:3]}..." if len(ids) > 3 else f"ğŸ” ChromaVectorStore.get_by_ids: æŸ¥è¯¢çš„ ids={ids}")
		
		# langchain Chroma çš„ get æ”¯æŒ ids å‚æ•°ï¼Œè¿”å› dict
		raw = self._store._collection.get(ids=ids, include=["metadatas", "documents"])  # type: ignore
		
		logger.info(f"ğŸ” ChromaVectorStore.get_by_ids: è¿”å›çš„ documents æ•°é‡={len(raw.get('documents', []) or [])}")
		logger.info(f"ğŸ” ChromaVectorStore.get_by_ids: è¿”å›çš„ metadatas æ•°é‡={len(raw.get('metadatas', []) or [])}")
		
		# è°ƒè¯•ï¼šæ˜¾ç¤ºcollectionä¸­å®é™…æœ‰å¤šå°‘æ–‡æ¡£
		try:
			count = self._store._collection.count()
			logger.info(f"ğŸ” ChromaVectorStore.get_by_ids: collectionæ€»æ–‡æ¡£æ•°={count}")
		except Exception as e:
			logger.warning(f"ğŸ” ChromaVectorStore.get_by_ids: æ— æ³•è·å–collectionæ€»æ•°: {e}")
		
		docs: List[Document] = []
		for text, meta in zip(raw.get("documents", []) or [], raw.get("metadatas", []) or []):
			docs.append(Document(page_content=text, metadata=meta))
		return docs

	async def get_by_ids(self, ids: List[str]) -> List[Document]:
		"""å¼‚æ­¥è·å–æ–‡æ¡£ï¼ˆä½¿ç”¨çº¿ç¨‹æ± åŒ…è£…ï¼‰
		
		è™½ç„¶ get_by_ids é€šå¸¸å¾ˆå¿«ï¼ˆç›´æ¥ä¸»é”®æŸ¥è¯¢ï¼‰ï¼Œä½†ä¸ºäº†é¿å…é˜»å¡
		äº‹ä»¶å¾ªç¯ï¼Œç»Ÿä¸€ä½¿ç”¨å¼‚æ­¥å®ç°ã€‚
		"""
		loop = asyncio.get_event_loop()
		executor = self._get_executor()
		
		# åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
		result = await loop.run_in_executor(
			executor,
			self._get_by_ids_sync,
			ids
		)
		return result


# âš¡ åå°çº¿ç¨‹é¢„åŠ è½½ FAISSï¼Œé¿å…å¯åŠ¨æ—¶é˜»å¡ä¸»çº¿ç¨‹
_FAISS_AVAILABLE = None
_FAISS = None
_faiss_loading = False
_faiss_lock = threading.Lock()
_faiss_loaded_event = threading.Event()

def _preload_faiss_in_background():
	"""åœ¨åå°çº¿ç¨‹é¢„åŠ è½½ FAISSï¼Œä¸é˜»å¡ä¸»å¯åŠ¨æµç¨‹"""
	global _FAISS_AVAILABLE, _FAISS, _faiss_loading
	
	with _faiss_lock:
		if _faiss_loading or _FAISS_AVAILABLE is not None:
			return  # å·²ç»åœ¨åŠ è½½æˆ–å·²åŠ è½½å®Œæˆ
		_faiss_loading = True
	
	def _load():
		global _FAISS_AVAILABLE, _FAISS
		try:
			logger.info("åå°é¢„åŠ è½½ FAISS å¼€å§‹...")
			try:
				from langchain_community.vectorstores import FAISS
				_FAISS_AVAILABLE = True
				_FAISS = FAISS
				logger.info("âœ“ FAISS é¢„åŠ è½½æˆåŠŸ (langchain_community)")
			except Exception as e:
				_FAISS_AVAILABLE = False
				_FAISS = None
				logger.warning(f"FAISS é¢„åŠ è½½å¤±è´¥: {e}")
		finally:
			_faiss_loaded_event.set()  # æ ‡è®°åŠ è½½å®Œæˆ
	
	thread = threading.Thread(target=_load, daemon=True, name="FAISSPreloader")
	thread.start()

def _get_faiss(timeout: float = 30.0):
	"""
	è·å– FAISS ç±»ï¼Œå¦‚æœæ­£åœ¨åå°åŠ è½½åˆ™ç­‰å¾…åŠ è½½å®Œæˆ
	
	Args:
		timeout: ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
	
	Returns:
		FAISS ç±»æˆ– None
	"""
	global _FAISS_AVAILABLE, _FAISS
	
	# å¦‚æœå·²ç»åŠ è½½å®Œæˆï¼Œç›´æ¥è¿”å›
	if _FAISS_AVAILABLE is not None:
		return _FAISS
	
	# ç­‰å¾…åå°åŠ è½½å®Œæˆ
	if _faiss_loading:
		logger.info(f"ç­‰å¾… FAISS åå°åŠ è½½å®Œæˆï¼ˆæœ€å¤šç­‰å¾… {timeout}ç§’ï¼‰...")
		if _faiss_loaded_event.wait(timeout=timeout):
			logger.info("FAISS åŠ è½½å®Œæˆ")
		else:
			logger.warning(f"ç­‰å¾… FAISS åŠ è½½è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰")
	
	return _FAISS


class FAISSVectorStore(VectorStoreLike):
	"""
	FAISS çš„è½»é‡å°è£…ã€‚é€šè¿‡ä¼ å…¥ Embeddings ä¸æŒä¹…åŒ–å‚æ•°è¿›è¡Œæ„é€ ï¼Œé¿å…åœ¨
	é¡¹ç›®å…¶ä»–æ¨¡å—ä¸­å‡ºç°å¯¹ FAISS çš„ç›´æ¥ä¾èµ–ã€‚
	
	æ‰€æœ‰æ£€ç´¢æ–¹æ³•å‡ä¸ºå¼‚æ­¥å®ç°ï¼Œä½¿ç”¨çº¿ç¨‹æ± åŒ…è£… FAISS çš„åŒæ­¥è°ƒç”¨ã€‚
	
	FAISS ç‰¹æ€§ï¼š
	- é«˜æ€§èƒ½çš„å‘é‡ç›¸ä¼¼åº¦æœç´¢
	- æ”¯æŒæœ¬åœ°æŒä¹…åŒ–ï¼ˆä¿å­˜/åŠ è½½ç´¢å¼•ï¼‰
	- å†…å­˜å‹å¥½çš„ç´¢å¼•ç»“æ„
	"""

	# å…±äº«çº¿ç¨‹æ± ï¼Œç”¨äºå¼‚æ­¥è°ƒç”¨ï¼ˆé¿å…åˆ›å»ºè¿‡å¤šçº¿ç¨‹ï¼‰
	_executor: Optional[ThreadPoolExecutor] = None
	_executor_lock = threading.Lock()

	@classmethod
	def _get_executor(cls) -> ThreadPoolExecutor:
		"""è·å–å…±äº«çš„çº¿ç¨‹æ± ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
		if cls._executor is None:
			with cls._executor_lock:
				if cls._executor is None:
					# åˆ›å»ºå›ºå®šå¤§å°çš„çº¿ç¨‹æ± ï¼Œé¿å…æ— é™åˆ¶åˆ›å»ºçº¿ç¨‹
					cls._executor = ThreadPoolExecutor(
						max_workers=4,  # æœ€å¤š4ä¸ªå¹¶å‘æ£€ç´¢
						thread_name_prefix="FAISSVectorStore"
					)
		return cls._executor

	def __init__(
		self,
		embedding_function: Embeddings,
		persist_directory: Optional[str] = None,
		collection_name: Optional[str] = None,
		distance_metric: str = "cosine",  # FAISSæ”¯æŒ: "cosine", "l2", "ip"
	):
		"""
		åˆå§‹åŒ– FAISSVectorStore
		
		Args:
			embedding_function: Embedding å‡½æ•°
			persist_directory: æŒä¹…åŒ–ç›®å½•
			collection_name: é›†åˆåç§°ï¼ˆç”¨äºæ–‡ä»¶åï¼‰
			distance_metric: è·ç¦»åº¦é‡æ–¹å¼ ("cosine", "l2", "ip")
		"""
		# âš¡ è·å– FAISSï¼ˆå¦‚æœæ­£åœ¨åå°åŠ è½½åˆ™ç­‰å¾…ï¼‰
		FAISS = _get_faiss(timeout=30.0)
		if FAISS is None:
			raise RuntimeError(
				"æœªæ£€æµ‹åˆ° FAISS é›†æˆï¼Œè¯·å®‰è£…: pip install faiss-cpu æˆ– pip install faiss-gpu"
			)

		# éªŒè¯è·ç¦»åº¦é‡å‚æ•°
		valid_metrics = ["cosine", "l2", "ip"]
		if distance_metric not in valid_metrics:
			logger.warning(f"æ— æ•ˆçš„è·ç¦»åº¦é‡ '{distance_metric}'ï¼Œä½¿ç”¨é»˜è®¤å€¼ 'cosine'")
			distance_metric = "cosine"
		
		self._distance_metric = distance_metric
		self._persist_directory = persist_directory
		self._collection_name = collection_name or "default"
		self._embedding_function = embedding_function
		logger.info(f"ğŸ¯ FAISSVectorStore ä½¿ç”¨è·ç¦»åº¦é‡: {distance_metric}")

		# æ„å»ºç´¢å¼•æ–‡ä»¶è·¯å¾„
		import os
		if persist_directory:
			os.makedirs(persist_directory, exist_ok=True)
			self._index_file = os.path.join(persist_directory, f"{self._collection_name}.faiss")
			self._pkl_file = os.path.join(persist_directory, f"{self._collection_name}.pkl")
		else:
			self._index_file = None
			self._pkl_file = None

		# å°è¯•åŠ è½½å·²æœ‰ç´¢å¼•
		if self._index_file and os.path.exists(self._index_file):
			try:
				logger.info(f"ğŸ“‚ åŠ è½½å·²æœ‰ FAISS ç´¢å¼•: {self._index_file}")
				self._store = FAISS.load_local(
					persist_directory,
					embedding_function,
					self._collection_name,
					allow_dangerous_deserialization=True  # å…è®¸åŠ è½½pickleæ–‡ä»¶
				)
				logger.info(f"âœ… FAISS ç´¢å¼•åŠ è½½æˆåŠŸï¼Œæ–‡æ¡£æ•°: {self._store.index.ntotal}")
			except Exception as e:
				logger.warning(f"âš ï¸ åŠ è½½ FAISS ç´¢å¼•å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°ç´¢å¼•")
				self._store = None
		else:
			self._store = None

	def add_documents(self, documents: List[Document], ids: Optional[List[str]] = None) -> None:
		"""
		ğŸ”’ å†…éƒ¨æ–¹æ³•ï¼šæ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
		
		âš ï¸ è­¦å‘Šï¼šæ­¤æ–¹æ³•ä»…ä¾› VectorStoreWithLock å†…éƒ¨ä½¿ç”¨ï¼ˆåœ¨æ–‡ä»¶é”ä¿æŠ¤ä¸‹è°ƒç”¨ï¼‰
		
		å¤–éƒ¨ä»£ç åº”ä½¿ç”¨:
		  vectorstore_mgr = get_vectorstore_manager()
		  vectorstore = vectorstore_mgr.get_or_create(...)
		  await vectorstore.add_documents_async(documents, ids)
		
		ç›´æ¥è°ƒç”¨æ­¤æ–¹æ³•ä¼šå¯¼è‡´å¤šè¿›ç¨‹å¹¶å‘å†™å…¥ï¼Œé€ æˆç´¢å¼•æŸåï¼
		"""
		# æ·»åŠ ç±»å‹æ£€æŸ¥å’Œè°ƒè¯•æ—¥å¿—
		if not documents:
			logger.warning("add_documents æ”¶åˆ°ç©ºæ–‡æ¡£åˆ—è¡¨")
			return
			
		# æ£€æŸ¥ documents æ˜¯å¦éƒ½æ˜¯ Document å¯¹è±¡
		for idx, doc in enumerate(documents):
			if not isinstance(doc, Document):
				logger.error(f"æ–‡æ¡£ {idx} ä¸æ˜¯ Document å¯¹è±¡ï¼Œç±»å‹: {type(doc)}, å€¼: {doc!r}")
				raise TypeError(f"æœŸæœ› Document å¯¹è±¡ï¼Œä½†æ”¶åˆ° {type(doc).__name__}")
		
		# FAISS å¤„ç†é€»è¾‘
		FAISS = _get_faiss()
		if FAISS is None:
			raise RuntimeError("FAISS æœªåŠ è½½")
		
		if self._store is None:
			# ğŸ¯ é¦–æ¬¡åˆ›å»ºç´¢å¼•ï¼Œæ ¹æ®è·ç¦»åº¦é‡é€‰æ‹©åˆé€‚çš„é…ç½®
			logger.info(f"ğŸ†• åˆ›å»ºæ–°çš„ FAISS ç´¢å¼•ï¼Œæ–‡æ¡£æ•°: {len(documents)}ï¼Œè·ç¦»åº¦é‡: {self._distance_metric}")
			
			# æ ¹æ®è·ç¦»åº¦é‡ç¡®å®šæ˜¯å¦éœ€è¦å½’ä¸€åŒ–
			# - cosine: éœ€è¦å½’ä¸€åŒ– + IPç´¢å¼•
			# - ip: ä¸å½’ä¸€åŒ– + IPç´¢å¼•
			# - l2: ä¸å½’ä¸€åŒ– + L2ç´¢å¼• (é»˜è®¤)
			normalize_L2 = (self._distance_metric == "cosine")
			
			# åˆ›å»ºç´¢å¼•ï¼Œlangchain FAISS ä¼šæ ¹æ® normalize_L2 è‡ªåŠ¨é€‰æ‹©ç´¢å¼•ç±»å‹
			self._store = FAISS.from_documents(
				documents,
				self._embedding_function,
				normalize_L2=normalize_L2  # ğŸ”¥ å…³é”®å‚æ•°ï¼šæ˜¯å¦å½’ä¸€åŒ–
			)
			
			# å¦‚æœæ˜¯ L2 è·ç¦»ä¸” FAISS é»˜è®¤åˆ›å»ºäº† IP ç´¢å¼•ï¼Œéœ€è¦æ‰‹åŠ¨æ›¿æ¢
			if self._distance_metric == "l2" and not normalize_L2:
				import faiss
				dim = self._store.index.d
				new_index = faiss.IndexFlatL2(dim)
				# å¤åˆ¶å‘é‡åˆ°æ–°ç´¢å¼•
				if self._store.index.ntotal > 0:
					vectors = self._store.index.reconstruct_n(0, self._store.index.ntotal)
					new_index.add(vectors)
				self._store.index = new_index
				logger.info(f"ğŸ”„ å·²åˆ‡æ¢åˆ° L2 ç´¢å¼•: {type(new_index).__name__}")
		else:
			# æ·»åŠ åˆ°å·²æœ‰ç´¢å¼•
			logger.info(f"â• æ·»åŠ æ–‡æ¡£åˆ° FAISS ç´¢å¼•ï¼Œæ–°å¢æ–‡æ¡£æ•°: {len(documents)}")
			# FAISS çš„ add_documents æ–¹æ³•
			self._store.add_documents(documents)
		
		# æŒä¹…åŒ–ç´¢å¼•
		if self._persist_directory:
			try:
				logger.info(f"ğŸ’¾ ä¿å­˜ FAISS ç´¢å¼•åˆ°: {self._index_file}")
				self._store.save_local(self._persist_directory, self._collection_name)
				logger.info(f"âœ… FAISS ç´¢å¼•ä¿å­˜æˆåŠŸï¼Œæ€»æ–‡æ¡£æ•°: {self._store.index.ntotal}")
			except Exception as e:
				logger.error(f"âŒ FAISS ç´¢å¼•ä¿å­˜å¤±è´¥: {e}", exc_info=True)

	async def similarity_search_with_score(self, query: str, k: int = 4) -> List[Tuple[Document, float]]:
		"""å¼‚æ­¥ç›¸ä¼¼åº¦æ£€ç´¢ï¼ˆä½¿ç”¨çº¿ç¨‹æ± åŒ…è£…ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
		
		å°†åŒæ­¥çš„ embedding + å‘é‡æ£€ç´¢æ“ä½œæ”¾åˆ°çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œ
		é¿å…é˜»å¡ asyncio äº‹ä»¶å¾ªç¯ã€‚
		"""
		if self._store is None:
			logger.warning("âš ï¸ FAISS ç´¢å¼•æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºç»“æœ")
			return []
		
		loop = asyncio.get_event_loop()
		executor = self._get_executor()
		
		# åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥æ“ä½œ
		result = await loop.run_in_executor(
			executor,
			self._store.similarity_search_with_score,
			query,
			k
		)
		return result

	def _get_by_ids_sync(self, ids: List[str]) -> List[Document]:
		"""å†…éƒ¨åŒæ­¥æ–¹æ³•ï¼šç”¨äºçº¿ç¨‹æ± æ‰§è¡Œ"""
		if not ids:
			return []
		
		if self._store is None:
			logger.warning("âš ï¸ FAISS ç´¢å¼•æœªåˆå§‹åŒ–")
			return []
		
		logger.info(f"ğŸ” FAISSVectorStore.get_by_ids: æŸ¥è¯¢ {len(ids)} ä¸ª ID")
		
		# FAISS é€šè¿‡ docstore è·å–æ–‡æ¡£
		# langchain FAISS å®ç°ä¸­ï¼Œdocstore æ˜¯ä¸€ä¸ªå­—å…¸æ˜ å°„
		docs: List[Document] = []
		
		try:
			if hasattr(self._store, 'docstore') and hasattr(self._store.docstore, '_dict'):
				# ä½¿ç”¨å†…éƒ¨å­—å…¸è·å–æ–‡æ¡£
				for doc_id in ids:
					if doc_id in self._store.docstore._dict:
						docs.append(self._store.docstore._dict[doc_id])
			else:
				logger.warning("âš ï¸ FAISS docstore ç»“æ„ä¸ç¬¦åˆé¢„æœŸ")
		except Exception as e:
			logger.error(f"âŒ FAISS get_by_ids å¤±è´¥: {e}", exc_info=True)
		
		logger.info(f"ğŸ” FAISSVectorStore.get_by_ids: è¿”å› {len(docs)} ä¸ªæ–‡æ¡£")
		return docs

	async def get_by_ids(self, ids: List[str]) -> List[Document]:
		"""å¼‚æ­¥è·å–æ–‡æ¡£ï¼ˆä½¿ç”¨çº¿ç¨‹æ± åŒ…è£…ï¼‰
		
		è™½ç„¶ get_by_ids é€šå¸¸å¾ˆå¿«ï¼ˆç›´æ¥ä¸»é”®æŸ¥è¯¢ï¼‰ï¼Œä½†ä¸ºäº†é¿å…é˜»å¡
		äº‹ä»¶å¾ªç¯ï¼Œç»Ÿä¸€ä½¿ç”¨å¼‚æ­¥å®ç°ã€‚
		"""
		loop = asyncio.get_event_loop()
		executor = self._get_executor()
		
		# åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
		result = await loop.run_in_executor(
			executor,
			self._get_by_ids_sync,
			ids
		)
		return result


__all__ = ["VectorStoreLike", "ChromaVectorStore", "FAISSVectorStore", "_preload_chroma_in_background", "_preload_faiss_in_background"] 