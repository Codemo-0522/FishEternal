import os
import time
import torch
from typing import List, Optional
from volcenginesdkarkruntime import Ark

from langchain_core.embeddings import Embeddings


class ArkEmbeddings(Embeddings):
    """
    åŸºäºç«å±±å¼•æ“ Ark çš„åµŒå…¥æ¨¡å‹å°è£…ï¼Œéµå¾ª LangChain Embeddings æ¥å£ã€‚

    æ¨¡å—åŒ–è®¾è®¡è¦ç‚¹ï¼š
    - ä¸åšæ–‡ä»¶è¯»å–ã€æ–‡æœ¬åˆ‡åˆ†ã€å‘é‡åº“å†™å…¥ç­‰ä»»ä½• I/O æˆ–ç­–ç•¥å†³ç­–ï¼›
      ä»…ä¸“æ³¨äºâ€œå­—ç¬¦ä¸² -> å‘é‡â€çš„è½¬æ¢ã€‚
    - æŸ¥è¯¢ä¸æ–‡æ¡£åµŒå…¥è¡Œä¸ºä¸€è‡´ï¼ŒæŸ¥è¯¢å¯é€‰æºå¸¦æŒ‡ä»¤å‰ç¼€ä»¥ä¼˜åŒ–æ£€ç´¢ã€‚
    - å¯é€‰ç»´åº¦æˆªæ–­ï¼ˆMRLï¼‰ä¸å‘é‡å½’ä¸€åŒ–ç”±æœ¬ç±»å†…éƒ¨å®Œæˆã€‚

    å‚æ•°:
    - api_key: å¿…å¡«ã€‚ç”±è°ƒç”¨æ–¹ä¼ å…¥ã€‚
    - model: Ark åµŒå…¥æ¨¡å‹åï¼Œé»˜è®¤ "doubao-embedding-large-text-250515"ã€‚
    - mrl_dim: å¯é€‰çš„å‘é‡ç»´åº¦æˆªæ–­ï¼ˆå¦‚ 2048 / 1024 / 512 / 256ï¼‰ã€‚None è¡¨ç¤ºä¸æˆªæ–­ã€‚
    - normalize: æ˜¯å¦å¯¹è¾“å‡ºå‘é‡è¿›è¡Œ L2 å½’ä¸€åŒ–ï¼Œé»˜è®¤ Trueã€‚
    - query_instruction: æŸ¥è¯¢æŒ‡ä»¤å‰ç¼€ï¼Œis_query=True æ—¶ç”Ÿæ•ˆï¼›None/ç©ºä¸²è¡¨ç¤ºä¸åŠ æŒ‡ä»¤ã€‚
    """

    def __init__(
        self,
        api_key: str,
        model: str = "doubao-embedding-large-text-250515",
        mrl_dim: Optional[int] = None,
        normalize: bool = True,
        query_instruction: Optional[str] = (
            "Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery: "
        ),
    ) -> None:
        if not api_key:
            raise ValueError("å¿…é¡»æä¾› api_keyï¼ˆç”±è°ƒç”¨æ–¹ä¼ å…¥ï¼‰ã€‚")

        # ç¦ç”¨ SDK å†…ç½®é‡è¯•ï¼Œé¿å…ä¸æˆ‘ä»¬çš„é‡è¯•æœºåˆ¶å†²çª
        self.client = Ark(api_key=api_key, max_retries=0)
        self.model = model
        self.mrl_dim = mrl_dim
        self.normalize = normalize
        self.query_instruction = query_instruction or ""

    def _prepare_inputs(self, inputs: List[str], is_query: bool) -> List[str]:
        if is_query and self.query_instruction:
            prefix = self.query_instruction
            return [f"{prefix}{text}" for text in inputs]
        return inputs

    def _encode(self, inputs: List[str], is_query: bool = False, max_retries: int = 8) -> List[List[float]]:
        processed_inputs = self._prepare_inputs(inputs, is_query=is_query)

        # å¸¦é‡è¯•çš„è¯·æ±‚é€»è¾‘ï¼ˆæŒ‡æ•°é€€é¿ + æŠ–åŠ¨ï¼‰
        for attempt in range(max_retries):
            try:
                resp = self.client.embeddings.create(
                    model=self.model,
                    input=processed_inputs,
                    encoding_format="float",
                )
                break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
            except Exception as e:
                # æ£€æµ‹é™æµé”™è¯¯ï¼ˆ429 æˆ– ServerOverloadedï¼‰
                is_rate_limit = (
                    "429" in str(e) or 
                    "TooManyRequests" in str(e) or 
                    "ServerOverloaded" in str(e) or
                    "RateLimitError" in str(type(e).__name__)
                )
                
                if is_rate_limit and attempt < max_retries - 1:
                    # æŒ‡æ•°é€€é¿ï¼š5s, 10s, 20s, 40s, 80s, 160s, 320s
                    wait_time = 5 * (2 ** attempt)
                    print(f"âš ï¸  é‡åˆ°é™æµï¼ˆ429 ServerOverloadedï¼‰ï¼Œç­‰å¾… {wait_time}s åé‡è¯•ï¼ˆç¬¬ {attempt + 1}/{max_retries} æ¬¡ï¼‰")
                    time.sleep(wait_time)
                    continue
                
                # å…¶ä»–é”™è¯¯æˆ–æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼Œç›´æ¥æŠ›å‡º
                raise

        embedding_tensor = torch.tensor(
            [d.embedding for d in resp.data], dtype=torch.bfloat16
        )

        # ç»´åº¦æˆªæ–­ï¼ˆè‹¥æŒ‡å®šï¼‰
        if self.mrl_dim is not None and self.mrl_dim > 0:
            max_dim = embedding_tensor.shape[1]
            slice_dim = min(self.mrl_dim, max_dim)
            embedding_tensor = embedding_tensor[:, :slice_dim]

        # L2 å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰
        if self.normalize:
            embedding_tensor = torch.nn.functional.normalize(embedding_tensor, dim=1, p=2)

        return embedding_tensor.float().tolist()

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        # é™ä½æ‰¹æ¬¡å¤§å°ä»¥å‡å°‘æœåŠ¡å™¨å‹åŠ›
        batch_size = 128  # ä» 256 é™ä½åˆ° 128
        all_embeddings = []
        total_batches = (len(texts) + batch_size - 1) // batch_size
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_num = i // batch_size + 1
            print(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}ï¼ˆ{len(batch)} æ¡æ–‡æœ¬ï¼‰")
            
            batch_embeddings = self._encode(batch, is_query=False)
            all_embeddings.extend(batch_embeddings)
            
            # åœ¨æ‰¹æ¬¡ä¹‹é—´æ·»åŠ æ›´é•¿å»¶è¿Ÿï¼Œé¿å…è§¦å‘é™æµ
            if i + batch_size < len(texts):
                wait_time = 5  # å¢åŠ åˆ° 5 ç§’
                print(f"â³ æ‰¹æ¬¡é—´å»¶è¿Ÿ {wait_time}sï¼Œé¿å…é™æµ...")
                time.sleep(wait_time)
        
        return all_embeddings

    def embed_query(self, text: str) -> List[float]:
        return self._encode([text], is_query=True)[0]


__all__ = ["ArkEmbeddings"]
