
import logging
from typing import Dict, Any, Optional, List, AsyncGenerator
from datetime import datetime, timezone, timedelta
# ç§»é™¤å‘é‡ç›¸å…³å¯¼å…¥
# import numpy as np
# from numpy.typing import NDArray
# from langchain_core.embeddings import Embeddings
# from sklearn.feature_extraction.text import TfidfVectorizer
# from ..vector_store.vector_store import VectorStore
# from ..content_filter import prepare_content_for_vector_storage, should_store_in_vector_db, prepare_content_for_context
from .deepseek import DeepSeekService
from .ollama import OllamaService
from .doubao import DouBaoService
from .bailian import BaiLianService
from .siliconflow import SiliconFlowService
from .zhipu import ZhipuService
from .hunyuan import HunyuanService
from .moonshot import MoonshotService
from .stepfun import StepfunService
from .modelscope import ModelScopeService
from ...mcp.manager import mcp_manager
from .streaming_manager import streaming_manager, StreamingState
from .tool_config import tool_config  # ğŸ‘ˆ å¯¼å…¥å…¨å±€é…ç½®

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# ç§»é™¤SimpleEmbeddingsç±»å’Œsimple_tokenizerå‡½æ•°

class LLMService:
    """LLMæœåŠ¡ç®¡ç†ç±»"""
    def __init__(self):
        # ç§»é™¤å‘é‡å­˜å‚¨åˆå§‹åŒ–
        # self.vector_store = VectorStore()
        self.last_response = None
        self.last_saved_images = []  # æ·»åŠ ä¿å­˜å›¾ç‰‡çš„å±æ€§
        # ç§»é™¤ current_service çš„ç¼“å­˜

    # ç§»é™¤ _get_relevant_history æ–¹æ³•

    async def generate_stream_universal(self,
                                      user_message: str,
                                      history: List[Dict[str, Any]],
                                      model_settings: Dict[str, Any],
                                      system_prompt: Optional[str] = None,
                                      session_id: Optional[str] = None,
                                      user_id: Optional[str] = None,
                                      enable_tools: bool = True,
                                      **kwargs) -> AsyncGenerator[str, None]:
        """
        é€šç”¨æµå¼ç”Ÿæˆæ–¹æ³•
        
        ä½¿ç”¨æ–°çš„æµå¼ç®¡ç†å™¨ï¼Œæ”¯æŒæ‰€æœ‰æ¨¡å‹ï¼Œè§£å†³å¹¶å‘å’Œå·¥å…·è°ƒç”¨é—®é¢˜
        """
        
        if not session_id:
            # å¦‚æœæ²¡æœ‰session_idï¼Œç”Ÿæˆä¸€ä¸ªä¸´æ—¶çš„
            import uuid
            session_id = f"temp_{uuid.uuid4().hex[:8]}"
        
        # æ³¨å†Œä¼šè¯åˆ°æµå¼ç®¡ç†å™¨ï¼ˆå¦‚æœè¿˜æ²¡æ³¨å†Œï¼‰
        if session_id not in streaming_manager.active_sessions:
            # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿwebsocketå¯¹è±¡ç”¨äºçŠ¶æ€ç®¡ç†
            class MockWebSocket:
                async def send_json(self, data):
                    logger.debug(f"çŠ¶æ€æ›´æ–°: {data}")
            
            await streaming_manager.register_session(
                session_id=session_id,
                user_id=user_id or "unknown",
                websocket=MockWebSocket()
            )
        
        try:
            # ä½¿ç”¨é€šç”¨æµå¼ç®¡ç†å™¨
            async for chunk in streaming_manager.generate_stream_universal(
                session_id=session_id,
                llm_service=self,
                user_message=user_message,
                history=history,
                model_settings=model_settings,
                system_prompt=system_prompt,
                enable_tools=enable_tools,
                user_id=user_id,
                **kwargs
            ):
                yield chunk
        finally:
            # æ¸…ç†ä¼šè¯
            await streaming_manager.unregister_session(session_id)

    async def generate_stream(self, 
                             user_message: str, 
                             history: List[Dict[str, Any]], 
                             model_settings: Dict[str, Any],
                             system_prompt: Optional[str] = None,
                             session_id: Optional[str] = None,
                             **kwargs) -> AsyncGenerator[str, None]:
        """
        ç”Ÿæˆæµå¼å›å¤
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            history: å†å²å¯¹è¯è®°å½•
            model_settings: æ¨¡å‹é…ç½®
            system_prompt: ç³»ç»Ÿæç¤º
            session_id: ä¼šè¯ID
            **kwargs: å…¶ä»–å‚æ•°
            
        Yields:
            str: ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        
        try:
            # è§£ææ¨¡å‹é…ç½®
            model_service = model_settings.get("modelService", "deepseek")
            base_url = model_settings.get("baseUrl", "")
            api_key = model_settings.get("apiKey", "")
            model_name = model_settings.get("modelName", "")
            model_params = model_settings.get("modelParams") if isinstance(model_settings, dict) else None
            
            logger.info(f"ç”Ÿæˆæµå¼å›å¤")
            logger.info(f"ç”¨æˆ·æ¶ˆæ¯: {user_message}")
            logger.info(f"ä¼šè¯ID: {session_id}")
            logger.info(f"ä½¿ç”¨æ¨¡å‹æœåŠ¡: {model_service}")
            # ğŸ› è°ƒè¯•ï¼šæ£€æŸ¥kwargsä¸­æ˜¯å¦åŒ…å«images_base64
            logger.info(f"ğŸ–¼ï¸ [llm_service.generate_stream] kwargsåŒ…å«: {list(kwargs.keys())}")
            if 'images_base64' in kwargs:
                images_data = kwargs.get('images_base64', [])
                logger.info(f"ğŸ–¼ï¸ [llm_service.generate_stream] æ¥æ”¶åˆ°images_base64: {len(images_data) if images_data else 0}å¼ å›¾ç‰‡")
            else:
                logger.warning(f"âš ï¸ [llm_service.generate_stream] kwargsä¸­æ²¡æœ‰images_base64ï¼")
            
            # æ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„æœåŠ¡å®ä¾‹ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°é…ç½®
            current_service = None
            if model_service == "deepseek":
                current_service = DeepSeekService(base_url, api_key, model_name)
            elif model_service == "ollama":
                current_service = OllamaService(base_url, api_key, model_name)
            elif model_service == "doubao":
                current_service = DouBaoService(base_url, api_key, model_name)
            elif model_service == "bailian":
                current_service = BaiLianService(base_url, api_key, model_name)
            elif model_service == "siliconflow":
                current_service = SiliconFlowService(base_url, api_key, model_name)
            elif model_service == "zhipu":
                current_service = ZhipuService(base_url, api_key, model_name)
            elif model_service == "hunyuan":
                current_service = HunyuanService(base_url, api_key, model_name)
            elif model_service == "moonshot":
                current_service = MoonshotService(base_url, api_key, model_name)
            elif model_service == "stepfun":
                current_service = StepfunService(base_url, api_key, model_name)
            elif model_service == "modelscope":
                current_service = ModelScopeService(base_url, api_key, model_name)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æœåŠ¡: {model_service}")
            
            # ç”Ÿæˆå›å¤ï¼ŒåŒæ—¶ä¼ é€’å†å²æ¶ˆæ¯
            response_text = ""
            error_occurred = False
            saved_images = []
            
            try:
                # ä¼ é€’å†å²æ¶ˆæ¯
                extra_kwargs = {"history": history}
                
                # å¦‚æœæœ‰å›¾ç‰‡ï¼Œä¼ é€’å¤šå¼ å›¾ç‰‡base64æ•°æ®
                if hasattr(current_service, 'generate_stream') and 'images_base64' in kwargs:
                    extra_kwargs["images_base64"] = kwargs.get("images_base64")
                    logger.info(f"ä¼ é€’å›¾ç‰‡æ•°æ®: {len(kwargs.get('images_base64', []))}å¼ å›¾ç‰‡")
                
                # ä¼ é€’session_idå’Œmessage_idå‚æ•°
                if session_id:
                    extra_kwargs["session_id"] = session_id
                    logger.info(f"ä¼ é€’session_id: {session_id}")
                elif 'session_id' in kwargs:
                    extra_kwargs["session_id"] = kwargs.get("session_id")
                    logger.info(f"ä¼ é€’session_id: {kwargs.get('session_id')}")
                
                if 'message_id' in kwargs:
                    extra_kwargs["message_id"] = kwargs.get("message_id")
                    logger.info(f"ä¼ é€’message_id: {kwargs.get('message_id')}")
                
                # ä¼ é€’user_idå‚æ•°ï¼ˆç”¨äºMinIOè·¯å¾„éš”ç¦»ï¼‰
                if 'user_id' in kwargs:
                    extra_kwargs["user_id"] = kwargs.get("user_id")
                    logger.info(f"ä¼ é€’user_id: {kwargs.get('user_id')}")
                
                # é€ä¼ æ¨¡å‹å‚æ•°
                if model_params and isinstance(model_params, dict):
                    extra_kwargs["model_params"] = model_params
                    logger.info(f"é€ä¼ æ¨¡å‹å‚æ•°: {list(model_params.keys())}")
                
                logger.info(f"æœ€ç»ˆä¼ é€’ç»™æ¨¡å‹æœåŠ¡çš„å‚æ•°: {list(extra_kwargs.keys())}")

                async for chunk in current_service.generate_stream(
                    user_message,  # ç›´æ¥ä½¿ç”¨åŸå§‹ç”¨æˆ·æ¶ˆæ¯
                    system_prompt or "",
                    **extra_kwargs
                ):
                    if isinstance(chunk, str):
                        response_text += chunk
                        yield chunk
                    else:
                        logger.warning(f"æ”¶åˆ°éå­—ç¬¦ä¸²çš„chunk: {type(chunk)}")
                
                self.last_response = {
                    "text": response_text,
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
                
                # è·å–å…·ä½“æœåŠ¡å®ä¾‹çš„ä¿å­˜å›¾ç‰‡ä¿¡æ¯
                if hasattr(current_service, 'last_saved_images'):
                    self.last_saved_images = current_service.last_saved_images
                    logger.info(f"âœ… ä»å…·ä½“æœåŠ¡å®ä¾‹è·å–åˆ°ä¿å­˜çš„å›¾ç‰‡: {self.last_saved_images}")
                else:
                    self.last_saved_images = []
                    logger.info("å…·ä½“æœåŠ¡å®ä¾‹æ²¡æœ‰last_saved_imageså±æ€§")
                    
            except Exception as e:
                error_occurred = True
                logger.error(f"ç”Ÿæˆå›å¤æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
                raise
            
        except Exception as e:
            logger.error(f"LLMService.generate_stream å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)
            raise

    def get_last_response(self) -> Optional[str]:
        """è·å–æœ€åä¸€æ¬¡çš„å›å¤"""
        return self.last_response
    
    async def generate_with_tools(
        self,
        user_message: str,
        history: List[Dict[str, Any]],
        model_settings: Dict[str, Any],
        system_prompt: Optional[str] = None,
        session_id: Optional[str] = None,
        user_id: Optional[str] = None,
        max_tool_iterations: Optional[int] = None,  # ğŸ‘ˆ æ”¹ä¸ºå¯é€‰ï¼Œè‡ªåŠ¨è¯»å–å…¨å±€é…ç½®
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        å¸¦å·¥å…·è°ƒç”¨çš„æµå¼ç”Ÿæˆï¼ˆæ”¯æŒ MCP å·¥å…·ï¼‰
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            history: å†å²å¯¹è¯è®°å½•
            model_settings: æ¨¡å‹é…ç½®
            system_prompt: ç³»ç»Ÿæç¤º
            session_id: ä¼šè¯IDï¼ˆå·¥å…·è°ƒç”¨éœ€è¦ï¼‰
            user_id: ç”¨æˆ·ID
            max_tool_iterations: æœ€å¤§å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•°ï¼ŒNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½® (tool_config.max_iterations)
            **kwargs: å…¶ä»–å‚æ•°
        
        Yields:
            str: ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        # ğŸ‘‡ ä½¿ç”¨å…¨å±€é…ç½®æˆ–ä¼ å…¥å‚æ•°
        max_iter = max_tool_iterations if max_tool_iterations is not None else tool_config.max_iterations
        logger.info(f"ğŸ”§ [generate_with_tools] æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iter} (å…¨å±€é…ç½®: {tool_config.max_iterations})")
        # æ£€æŸ¥ MCP æ˜¯å¦å¯ç”¨
        mcp_client = mcp_manager.get_client()
        if not mcp_client:
            logger.warning("âš ï¸ MCP Client æœªåˆå§‹åŒ–ï¼Œå›é€€åˆ°æ™®é€šå¯¹è¯æ¨¡å¼")
            async for chunk in self.generate_stream(
                user_message, history, model_settings, system_prompt, session_id, **kwargs
            ):
                yield chunk
            return
        
        # è·å–å·¥å…·åˆ—è¡¨ï¼ˆä¼ é€’ session_id ä»¥æ”¯æŒåŠ¨æ€å‚æ•°ï¼‰
        try:
            tools = await mcp_client.list_tools(
                session_id=session_id,
                user_id=user_id
            )
            if not tools:
                logger.info("â„¹ï¸ æ— å¯ç”¨å·¥å…·ï¼Œä½¿ç”¨æ™®é€šå¯¹è¯æ¨¡å¼")
                async for chunk in self.generate_stream(
                    user_message, history, model_settings, system_prompt, session_id, **kwargs
                ):
                    yield chunk
                return
            
            logger.info(f"ğŸ”§ å·²åŠ è½½ {len(tools)} ä¸ª MCP å·¥å…·")
            # æ‰“å°å·¥å…·æè¿°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            for tool in tools:
                logger.debug(f"  - {tool['function']['name']}: {tool['function']['description'][:100]}...")
        except Exception as e:
            logger.error(f"âŒ è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {e}")
            async for chunk in self.generate_stream(
                user_message, history, model_settings, system_prompt, session_id, **kwargs
            ):
                yield chunk
            return
        
        # è§£ææ¨¡å‹é…ç½®
        model_service = model_settings.get("modelService", "deepseek")
        base_url = model_settings.get("baseUrl", "")
        api_key = model_settings.get("apiKey", "")
        model_name = model_settings.get("modelName", "")
        
        # åˆ›å»ºæœåŠ¡å®ä¾‹
        current_service = self._create_service_instance(model_service, base_url, api_key, model_name)
        if not current_service:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æœåŠ¡: {model_service}")
        
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨ - å¦‚æœä¸æ”¯æŒï¼Œç›´æ¥ä½¿ç”¨æ™®é€šæ¨¡å¼
        if not hasattr(current_service, '_call_llm_with_tools_sync'):
            logger.warning(f"âš ï¸ {model_service} æœåŠ¡ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œä½¿ç”¨æ™®é€šå¯¹è¯æ¨¡å¼")
            async for chunk in self.generate_stream(
                user_message, history, model_settings, system_prompt, session_id, **kwargs
            ):
                yield chunk
            return
        
        # æ‰§è¡Œå·¥å…·è°ƒç”¨å¾ªç¯
        iteration = 0
        messages = self._build_messages(system_prompt, history, user_message)
        
        while iteration < max_iter:  # ğŸ‘ˆ ä½¿ç”¨å…¨å±€é…ç½®
            iteration += 1
            logger.info(f"ğŸ”„ å·¥å…·è°ƒç”¨è¿­ä»£ {iteration}/{max_iter}")
            
            # æ‰“å°å½“å‰æ¶ˆæ¯åˆ—è¡¨ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            logger.info(f"ğŸ“ å½“å‰æ¶ˆæ¯åˆ—è¡¨ï¼ˆå…± {len(messages)} æ¡ï¼‰:")
            for i, msg in enumerate(messages):
                role = msg.get("role", "unknown")
                content = msg.get("content", "")
                if role == "tool":
                    tool_name = msg.get("name", "unknown")
                    logger.info(f"   [{i+1}] {role} ({tool_name}): {content[:100]}...")
                elif role == "assistant" and "tool_calls" in msg:
                    tool_calls_info = msg.get("tool_calls", [])
                    logger.info(f"   [{i+1}] {role} (è¯·æ±‚å·¥å…·): {len(tool_calls_info)} ä¸ªå·¥å…·")
                else:
                    logger.info(f"   [{i+1}] {role}: {content[:100]}...")
            
            # è°ƒç”¨ LLMï¼ˆå¸¦å·¥å…·ï¼‰
            try:
                # ğŸ¯ æå–ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹å‚æ•°
                model_params = model_settings.get("modelParams", {})
                # ğŸ–¼ï¸ ä¼ é€’å›¾ç‰‡æ•°æ®åŠå…¶ä»–å‚æ•°ï¼ˆsession_id, message_id, user_idç­‰ï¼‰
                # æ³¨æ„ï¼šä½¿ç”¨getè€Œä¸æ˜¯pop,å› ä¸ºkwargså¯èƒ½è¿˜éœ€è¦ç”¨äºå…¶ä»–åœ°æ–¹
                images_base64 = kwargs.get('images_base64')
                # åˆ›å»ºä¸åŒ…å«images_base64çš„kwargså‰¯æœ¬,é¿å…é‡å¤ä¼ é€’
                other_kwargs = {k: v for k, v in kwargs.items() if k != 'images_base64'}
                if images_base64:
                    response = current_service._call_llm_with_tools_sync(messages, tools, model_params, images_base64, **other_kwargs)
                else:
                    # âš ï¸ å³ä½¿æ²¡æœ‰å›¾ç‰‡ï¼Œä¹Ÿè¦ä¼ é€’ other_kwargsï¼ˆåŒ…å« session_id, message_id, user_idï¼‰
                    response = current_service._call_llm_with_tools_sync(messages, tools, model_params, **other_kwargs)
            except NotImplementedError as e:
                # æ¨¡å‹ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œç›´æ¥ä½¿ç”¨æ™®é€šå¯¹è¯æ¨¡å¼
                # æ³¨æ„ï¼šhistory ä¸­å·²ç»åŒ…å«äº†ä¹‹å‰æ‰€æœ‰çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆåŒ…æ‹¬å·¥å…·è°ƒç”¨ç»“æœï¼‰
                logger.warning(f"âš ï¸ æ¨¡å‹ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œåˆ‡æ¢åˆ°æ™®é€šå¯¹è¯æ¨¡å¼: {e}")
                async for chunk in self.generate_stream(
                    user_message, history, model_settings, system_prompt, session_id, **kwargs
                ):
                    yield chunk
                return
            except Exception as e:
                logger.error(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
                yield f"\n[é”™è¯¯] LLM è°ƒç”¨å¤±è´¥: {str(e)}\n"
                break
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
            tool_calls = response.get("tool_calls", [])
            if not tool_calls:
                # æ— å·¥å…·è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆå›å¤
                final_content = response.get("content", "")
                logger.info("âœ… LLM è¿”å›æœ€ç»ˆå›å¤ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰")
                
                # æµå¼è¾“å‡º
                for char in final_content:
                    yield char
                break
            
            # æœ‰å·¥å…·è°ƒç”¨
            logger.info(f"ğŸ”§ LLM è¯·æ±‚è°ƒç”¨ {len(tool_calls)} ä¸ªå·¥å…·")
            
            # ğŸ¯ è·å–æ¨¡å‹åœ¨è°ƒç”¨å·¥å…·æ—¶è¾“å‡ºçš„æè¿°ï¼ˆå¦‚"ğŸ” æ­£åœ¨æ£€ç´¢..."ï¼‰
            tool_call_description = response.get("content") or ""
            
            # ğŸ“¤ å¦‚æœæ¨¡å‹è¾“å‡ºäº†æè¿°ï¼Œæµå¼è¾“å‡ºåˆ°å‰ç«¯çš„ <think> æ ‡ç­¾ä¸­
            if tool_call_description and tool_call_description.strip():
                logger.info(f"ğŸ’¬ æ¨¡å‹å·¥å…·è°ƒç”¨æè¿°: {tool_call_description[:100]}")
                # ğŸ¯ å…³é”®ï¼šå…ˆå‘é€å¼€å§‹æ ‡ç­¾ï¼Œç„¶åæµå¼å‘é€å†…å®¹ï¼Œæœ€åå‘é€ç»“æŸæ ‡ç­¾
                # è¿™æ ·å‰ç«¯ä¼šç´¯ç§¯æˆä¸€ä¸ªå®Œæ•´çš„ <think>...</think>ï¼Œåªæ¸²æŸ“ä¸€ä¸ªæŠ˜å æ 
                yield "<think>"  # å¼€å§‹æ ‡ç­¾
                # æµå¼è¾“å‡ºæè¿°å†…å®¹
                for char in tool_call_description:
                    yield char
                yield "</think>"  # ç»“æŸæ ‡ç­¾
            
            # æ·»åŠ  assistant æ¶ˆæ¯åˆ°å†å²
            messages.append({
                "role": "assistant",
                "content": response.get("content") or None,
                "tool_calls": tool_calls
            })
            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            for tool_call in tool_calls:
                import json
                tool_name = tool_call.get("function", {}).get("name")
                tool_args_str = tool_call.get("function", {}).get("arguments", "{}")
                tool_call_id = tool_call.get("id", "")
                
                try:
                    tool_args = json.loads(tool_args_str) if isinstance(tool_args_str, str) else tool_args_str
                except json.JSONDecodeError:
                    tool_args = {}
                
                logger.info(f"  ğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}")
                logger.info(f"     å‚æ•°: {json.dumps(tool_args, ensure_ascii=False)}")
                
                # ğŸ¯ ä¸å‘é€å·¥å…·çŠ¶æ€åˆ°å‰ç«¯ï¼ˆé¿å…æ˜¾ç¤ºå¤šä½™æ°”æ³¡ï¼‰
                import json as json_lib
                # yield f"__TOOL_STATUS__{json_lib.dumps({'tool': tool_name, 'status': 'calling', 'args': tool_args}, ensure_ascii=False)}__END__"
                
                # æ‰§è¡Œå·¥å…·
                try:
                    result = await mcp_client.call_tool(
                        tool_name=tool_name,
                        arguments=tool_args,
                        session_id=session_id,
                        user_id=user_id
                    )
                    
                    logger.info(f"  âœ… å·¥å…·æ‰§è¡ŒæˆåŠŸ: {tool_name}")
                    logger.info(f"     ç»“æœ: {result[:200]}...")
                    
                    # ğŸ¯ ç‰¹æ®Šå¤„ç†ï¼šæ‹¦æˆª search_knowledge_base çš„ç»“æœï¼Œæå–å¼•ç”¨ä¿¡æ¯
                    if tool_name == "search_knowledge_base":
                        try:
                            result_data = json.loads(result) if isinstance(result, str) else result
                            if result_data.get("success") and result_data.get("results"):
                                # æå–å¼•ç”¨ï¼ˆä¸æ—§ RAG æ¨¡å¼æ ¼å¼ä¿æŒä¸€è‡´ï¼‰
                                rich_refs = []
                                lean_refs = []
                                
                                for item in result_data["results"]:
                                    metadata = item.get("metadata", {})
                                    # ç²¾ç®€å¼•ç”¨ï¼ˆç”¨äºä¿å­˜åˆ°æ•°æ®åº“ï¼‰
                                    # ğŸ†• æ·»åŠ æŸ¥çœ‹åŸæ–‡æ‰€éœ€çš„å­—æ®µ
                                    lean = {
                                        "document_id": metadata.get("document_id") or metadata.get("source"),
                                        "chunk_id": metadata.get("chunk_id"),
                                        "score": item.get("score", 0.0),
                                        "doc_id": metadata.get("doc_id", ""),
                                        "kb_id": metadata.get("kb_id", ""),
                                        "filename": metadata.get("filename", "")
                                    }
                                    lean_refs.append(lean)
                                    
                                    # å®Œæ•´å¼•ç”¨ï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤ºï¼‰
                                    rich = {
                                        "document_id": lean["document_id"],
                                        "chunk_id": lean["chunk_id"],
                                        "score": lean["score"],
                                        "document_name": metadata.get("source"),
                                        "content": item.get("content", ""),
                                        "metadata": metadata,
                                        # ğŸ†• æ·»åŠ æŸ¥çœ‹åŸæ–‡æ‰€éœ€çš„å­—æ®µåˆ°é¡¶å±‚
                                        "doc_id": metadata.get("doc_id", ""),
                                        "kb_id": metadata.get("kb_id", ""),
                                        "filename": metadata.get("filename", "")
                                    }
                                    rich_refs.append(rich)
                                
                                # âŒ ä¸åœ¨è¿™é‡Œå‘é€å¼•ç”¨æ•°æ®ï¼å› ä¸ºè¿˜æ²¡å»é‡æ’åºï¼
                                # å¼•ç”¨æ•°æ®ä¼šåœ¨ streaming_manager çš„å»é‡æ’åºåç»Ÿä¸€å‘é€
                                if rich_refs:
                                    logger.info(f"  ğŸ“š æå–åˆ° {len(rich_refs)} æ¡çŸ¥è¯†åº“å¼•ç”¨ï¼ˆç­‰å¾…å»é‡æ’åºåå‘é€ï¼‰")
                                    # yield f"__REFERENCES__{json_lib.dumps({'rich': rich_refs, 'lean': lean_refs}, ensure_ascii=False)}__END__"
                        except Exception as ref_err:
                            logger.warning(f"  âš ï¸ æå–å¼•ç”¨ä¿¡æ¯å¤±è´¥: {ref_err}")
                    
                    # ğŸ¯ ä¸å‘é€å·¥å…·æ‰§è¡ŒæˆåŠŸçŠ¶æ€åˆ°å‰ç«¯ï¼ˆé¿å…æ˜¾ç¤ºå¤šä½™æ°”æ³¡ï¼‰
                    # yield f"__TOOL_STATUS__{json_lib.dumps({'tool': tool_name, 'status': 'success'}, ensure_ascii=False)}__END__"
                    
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call_id,
                        "name": tool_name,
                        "content": result
                    })
                
                except Exception as e:
                    logger.error(f"  âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {tool_name}, é”™è¯¯: {e}")
                    
                    # ğŸ¯ ä¸å‘é€å·¥å…·æ‰§è¡Œå¤±è´¥çŠ¶æ€åˆ°å‰ç«¯ï¼ˆé¿å…æ˜¾ç¤ºå¤šä½™æ°”æ³¡ï¼‰
                    # yield f"__TOOL_STATUS__{json_lib.dumps({'tool': tool_name, 'status': 'error', 'error': str(e)}, ensure_ascii=False)}__END__"
                    
                    import json
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call_id,
                        "name": tool_name,
                        "content": json.dumps({"error": str(e)}, ensure_ascii=False)
                    })
        
        if iteration >= max_iter:  # ğŸ‘ˆ ä½¿ç”¨å…¨å±€é…ç½®
            logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•° ({max_iter})ï¼Œå¼ºåˆ¶ç”Ÿæˆæœ€ç»ˆå›å¤")
            
            # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼Œæç¤ºæ¨¡å‹å·¥å…·è°ƒç”¨æ¬¡æ•°å·²è¾¾ä¸Šé™
            messages.append({
                "role": "system",
                "content": "âš ï¸ å·¥å…·è°ƒç”¨æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼Œè¯·æ ¹æ®å·²è·å–çš„ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆå›å¤ã€‚"
            })
            
            # å¼ºåˆ¶è°ƒç”¨ä¸€æ¬¡ LLM ç”Ÿæˆæœ€ç»ˆå›å¤ï¼ˆä¸å¸¦å·¥å…·ï¼‰
            try:
                # ğŸ¯ æå–ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹å‚æ•°
                model_params = model_settings.get("modelParams", {})
                # ğŸ–¼ï¸ ä¼ é€’å›¾ç‰‡æ•°æ®åŠå…¶ä»–å‚æ•°ï¼ˆsession_id, message_id, user_idç­‰ï¼‰
                images_base64 = kwargs.get('images_base64')
                # åˆ›å»ºä¸åŒ…å«images_base64çš„kwargså‰¯æœ¬,é¿å…é‡å¤ä¼ é€’
                other_kwargs = {k: v for k, v in kwargs.items() if k != 'images_base64'}
                if images_base64:
                    response = current_service._call_llm_with_tools_sync(messages, [], model_params, images_base64, **other_kwargs)
                else:
                    # âš ï¸ å³ä½¿æ²¡æœ‰å›¾ç‰‡ï¼Œä¹Ÿè¦ä¼ é€’ other_kwargsï¼ˆåŒ…å« session_id, message_id, user_idï¼‰
                    response = current_service._call_llm_with_tools_sync(messages, [], model_params, **other_kwargs)
                final_content = response.get("content", "")
                
                if final_content:
                    logger.info(f"âœ… ç”Ÿæˆæœ€ç»ˆå›å¤ï¼ˆå·¥å…·è°ƒç”¨ä¸Šé™åï¼‰: {final_content[:100]}...")
                    # æµå¼è¾“å‡º
                    for char in final_content:
                        yield char
                else:
                    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œè¿”å›æç¤ºä¿¡æ¯
                    fallback_msg = "\n\nâš ï¸ å·²è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œä½†æˆ‘å·²ä¸ºæ‚¨æ”¶é›†äº†ç›¸å…³ä¿¡æ¯ã€‚å¦‚éœ€æ›´å¤šå¸®åŠ©ï¼Œè¯·å°è¯•ç®€åŒ–é—®é¢˜æˆ–åˆ†æ‰¹æ¬¡è¯¢é—®ã€‚"
                    for char in fallback_msg:
                        yield char
                        
            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆæœ€ç»ˆå›å¤å¤±è´¥: {e}")
                error_msg = f"\n\nâš ï¸ ç³»ç»Ÿé”™è¯¯ï¼šå·¥å…·è°ƒç”¨æ¬¡æ•°è¾¾åˆ°ä¸Šé™åç”Ÿæˆå›å¤å¤±è´¥ã€‚é”™è¯¯ä¿¡æ¯ï¼š{str(e)}"
                for char in error_msg:
                    yield char
    
    def _create_service_instance(self, model_service: str, base_url: str, api_key: str, model_name: str):
        """åˆ›å»ºæœåŠ¡å®ä¾‹"""
        if model_service == "deepseek":
            return DeepSeekService(base_url, api_key, model_name)
        elif model_service == "ollama":
            return OllamaService(base_url, api_key, model_name)
        elif model_service == "doubao":
            return DouBaoService(base_url, api_key, model_name)
        elif model_service == "bailian":
            return BaiLianService(base_url, api_key, model_name)
        elif model_service == "siliconflow":
            return SiliconFlowService(base_url, api_key, model_name)
        elif model_service == "zhipu":
            return ZhipuService(base_url, api_key, model_name)
        elif model_service == "hunyuan":
            return HunyuanService(base_url, api_key, model_name)
        elif model_service == "moonshot":
            return MoonshotService(base_url, api_key, model_name)
        elif model_service == "stepfun":
            return StepfunService(base_url, api_key, model_name)
        elif model_service == "modelscope":
            return ModelScopeService(base_url, api_key, model_name)
        return None
    
    def _build_messages(self, system_prompt: str, history: List[Dict[str, Any]], user_message: str) -> List[Dict[str, Any]]:
        """æ„å»ºæ¶ˆæ¯åˆ—è¡¨"""
        messages = []
        
        if system_prompt and system_prompt.strip():
            messages.append({"role": "system", "content": system_prompt.strip()})
        
        if history:
            for msg in history:
                # ğŸ–¼ï¸ ã€å…³é”®ä¿®å¤ã€‘ä¿ç•™å†å²æ¶ˆæ¯ä¸­çš„å›¾ç‰‡ä¿¡æ¯
                message = {
                    "role": msg.get("role", "user"),
                    "content": msg.get("content", "")
                }
                
                # å¦‚æœå†å²æ¶ˆæ¯åŒ…å«å›¾ç‰‡ï¼Œä¿ç•™ images å­—æ®µï¼ˆMinIO URLï¼‰
                if 'images' in msg and msg['images']:
                    message['images'] = msg['images']
                    logger.info(f"ğŸ“¸ å†å²æ¶ˆæ¯åŒ…å« {len(msg['images'])} å¼ å›¾ç‰‡ï¼ˆå°†ç”±_process_request_dataè½¬æ¢ä¸ºbase64ï¼‰")
                
                messages.append(message)
        
        messages.append({"role": "user", "content": user_message})
        
        return messages
    
    async def generate_with_tools_non_streaming(
        self,
        user_message: str,
        history: List[Dict[str, Any]],
        model_settings: Dict[str, Any],
        system_prompt: Optional[str] = None,
        session_id: Optional[str] = None,
        user_id: Optional[str] = None,
        max_tool_iterations: Optional[int] = None,  # ğŸ‘ˆ æ”¹ä¸ºå¯é€‰ï¼Œè‡ªåŠ¨è¯»å–å…¨å±€é…ç½®
        extra_tools: Optional[List[Dict[str, Any]]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¸¦å·¥å…·è°ƒç”¨çš„éæµå¼ç”Ÿæˆï¼ˆç”¨äºç¾¤èŠç­‰åœºæ™¯ï¼‰
        
        Args:
            max_tool_iterations: æœ€å¤§å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•°ï¼ŒNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½® (tool_config.max_iterations)
            extra_tools: é¢å¤–æ³¨å…¥çš„å·¥å…·åˆ—è¡¨ï¼ˆä¾‹å¦‚ç¾¤èŠä¸“ç”¨å·¥å…·ï¼‰
        
        Returns:
            dict: {
                "content": "æœ€ç»ˆå›å¤å†…å®¹",
                "tool_calls_made": ["tool1", "tool2"],  # è°ƒç”¨è¿‡çš„å·¥å…·åˆ—è¡¨
                "skip_reply": bool,  # æ˜¯å¦è°ƒç”¨äº† skip_reply å·¥å…·
                "references": []  # çŸ¥è¯†åº“å¼•ç”¨åˆ—è¡¨ï¼ˆç²¾ç®€æ ¼å¼ï¼Œå·²å»é‡æ’åºï¼‰
            }
        """
        # ğŸ‘‡ ä½¿ç”¨å…¨å±€é…ç½®æˆ–ä¼ å…¥å‚æ•°
        max_iter = max_tool_iterations if max_tool_iterations is not None else tool_config.max_iterations
        logger.info(f"ğŸ”§ [generate_with_tools_non_streaming] æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iter} (å…¨å±€é…ç½®: {tool_config.max_iterations})")
        from app.mcp.manager import mcp_manager
        
        # æ£€æŸ¥ MCP æ˜¯å¦å¯ç”¨
        mcp_client = mcp_manager.get_client()
        if not mcp_client:
            logger.warning("âš ï¸ MCP Client æœªåˆå§‹åŒ–ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼")
            # ä½¿ç”¨æ™®é€šæ¨¡å¼ç”Ÿæˆ
            full_response = ""
            async for chunk in self.generate_stream(
                user_message, history, model_settings, system_prompt, session_id, **kwargs
            ):
                full_response += chunk
            return {
                "content": full_response,
                "tool_calls_made": [],
                "skip_reply": False,
                "references": []
            }
        
        # è·å–å·¥å…·åˆ—è¡¨
        try:
            tools = await mcp_client.list_tools(session_id=session_id, user_id=user_id)
            if not tools:
                tools = []
            
            # åˆå¹¶é¢å¤–å·¥å…·
            if extra_tools:
                tools.extend(extra_tools)
                logger.info(f"âœ¨ æ³¨å…¥äº† {len(extra_tools)} ä¸ªé¢å¤–å·¥å…·")
            
            if not tools:
                logger.info("â„¹ï¸ æ— å¯ç”¨å·¥å…·ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼")
                full_response = ""
                async for chunk in self.generate_stream(
                    user_message, history, model_settings, system_prompt, session_id, **kwargs
                ):
                    full_response += chunk
                return {
                    "content": full_response,
                    "tool_calls_made": [],
                    "skip_reply": False,
                    "references": []
                }
            
            logger.info(f"ğŸ”§ å·²åŠ è½½ {len(tools)} ä¸ª MCP å·¥å…·")
        except Exception as e:
            logger.error(f"âŒ è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {e}")
            full_response = ""
            async for chunk in self.generate_stream(
                user_message, history, model_settings, system_prompt, session_id, **kwargs
            ):
                full_response += chunk
            return {
                "content": full_response,
                "tool_calls_made": [],
                "skip_reply": False,
                "references": []
            }
        
        # åˆ›å»ºæœåŠ¡å®ä¾‹
        model_service = model_settings.get("modelService", "deepseek")
        base_url = model_settings.get("baseUrl", "")
        api_key = model_settings.get("apiKey", "")
        model_name = model_settings.get("modelName", "")
        
        current_service = self._create_service_instance(model_service, base_url, api_key, model_name)
        if not current_service:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æœåŠ¡: {model_service}")
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨
        if not hasattr(current_service, '_call_llm_with_tools_sync'):
            logger.warning(f"âš ï¸ {model_service} æœåŠ¡ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼")
            full_response = ""
            async for chunk in self.generate_stream(
                user_message, history, model_settings, system_prompt, session_id, **kwargs
            ):
                full_response += chunk
            return {
                "content": full_response,
                "tool_calls_made": [],
                "skip_reply": False,
                "references": []
            }
        
        # ğŸ†• åˆ›å»º streaming_manager å®ä¾‹æ¥å¤„ç†å¼•ç”¨ï¼ˆå¤ç”¨å»é‡æ’åºé€»è¾‘ï¼‰
        from app.utils.llm.streaming_manager import UniversalStreamingManager
        streaming_manager = UniversalStreamingManager()
        
        # æ‰§è¡Œå·¥å…·è°ƒç”¨å¾ªç¯
        iteration = 0
        messages = self._build_messages(system_prompt, history, user_message)
        tool_calls_made = []
        skip_reply_called = False
        
        while iteration < max_iter:  # ğŸ‘ˆ ä½¿ç”¨å…¨å±€é…ç½®
            iteration += 1
            logger.info(f"ğŸ”„ å·¥å…·è°ƒç”¨è¿­ä»£ {iteration}/{max_iter}")
            
            # è°ƒç”¨ LLM
            try:
                model_params = model_settings.get("modelParams", {})
                images_base64 = kwargs.get('images_base64')
                other_kwargs = {k: v for k, v in kwargs.items() if k != 'images_base64'}
                
                if images_base64:
                    response = current_service._call_llm_with_tools_sync(
                        messages, tools, model_params, images_base64, **other_kwargs
                    )
                else:
                    response = current_service._call_llm_with_tools_sync(
                        messages, tools, model_params, **other_kwargs
                    )
            except Exception as e:
                logger.error(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
                return {
                    "content": f"[é”™è¯¯] LLM è°ƒç”¨å¤±è´¥: {str(e)}",
                    "tool_calls_made": tool_calls_made,
                    "skip_reply": skip_reply_called,
                    "references": []
                }
            
            # æ£€æŸ¥å·¥å…·è°ƒç”¨
            tool_calls = response.get("tool_calls", [])
            if not tool_calls:
                # æ— å·¥å…·è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆå›å¤
                final_content = response.get("content", "")
                logger.info("âœ… LLM è¿”å›æœ€ç»ˆå›å¤ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰")
                
                # ğŸ†• æå–æœ€ç»ˆçš„å¼•ç”¨æ•°æ®
                final_references = []
                if hasattr(streaming_manager, '_pending_references') and session_id in streaming_manager._pending_references:
                    refs_data = streaming_manager._pending_references[session_id]
                    final_references = refs_data.get("lean", [])
                    logger.info(f"ğŸ“š è¿”å› {len(final_references)} æ¡çŸ¥è¯†åº“å¼•ç”¨")
                
                return {
                    "content": final_content,
                    "tool_calls_made": tool_calls_made,
                    "skip_reply": skip_reply_called,
                    "references": final_references
                }
            
            # æœ‰å·¥å…·è°ƒç”¨
            logger.info(f"ğŸ”§ LLM è¯·æ±‚è°ƒç”¨ {len(tool_calls)} ä¸ªå·¥å…·")
            
            # æ·»åŠ  assistant æ¶ˆæ¯
            messages.append({
                "role": "assistant",
                "content": response.get("content") or None,
                "tool_calls": tool_calls
            })
            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            for tool_call in tool_calls:
                import json
                tool_name = tool_call.get("function", {}).get("name")
                tool_args_str = tool_call.get("function", {}).get("arguments", "{}")
                tool_call_id = tool_call.get("id", "")
                
                try:
                    tool_args = json.loads(tool_args_str) if isinstance(tool_args_str, str) else tool_args_str
                except json.JSONDecodeError:
                    tool_args = {}
                
                logger.info(f"  ğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}")
                logger.info(f"     å‚æ•°: {json.dumps(tool_args, ensure_ascii=False)}")
                
                # è®°å½•å·¥å…·è°ƒç”¨
                tool_calls_made.append(tool_name)
                
                # ğŸ¯ æ£€æµ‹ skip_reply å·¥å…·
                if tool_name == "skip_reply":
                    skip_reply_called = True
                    logger.info(f"  ğŸ¤ æ£€æµ‹åˆ° skip_reply å·¥å…·è°ƒç”¨")
                
                # æ‰§è¡Œå·¥å…·
                try:
                    result = await mcp_client.call_tool(
                        tool_name=tool_name,
                        arguments=tool_args,
                        session_id=session_id,
                        user_id=user_id
                    )
                    
                    logger.info(f"  âœ… å·¥å…·æ‰§è¡ŒæˆåŠŸ: {tool_name}")
                    # ğŸ”§ ä¿®å¤ï¼šresult å¯èƒ½æ˜¯å­—å…¸ï¼Œä¸èƒ½ç›´æ¥åˆ‡ç‰‡
                    result_str = str(result) if result else 'None'
                    logger.info(f"     ç»“æœ: {result_str[:200]}...")
                    
                    # ğŸ†• ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯çŸ¥è¯†åº“æ£€ç´¢å·¥å…·ï¼Œæ”¶é›†å¼•ç”¨æ•°æ®ï¼ˆå¤ç”¨ streaming_manager çš„é€»è¾‘ï¼‰
                    if tool_name == "search_knowledge_base" and isinstance(result, str):
                        try:
                            result_data = json.loads(result)
                            if result_data.get("success") and result_data.get("results"):
                                # åˆå§‹åŒ–å¼•ç”¨å­˜å‚¨
                                if not hasattr(streaming_manager, '_pending_references'):
                                    streaming_manager._pending_references = {}
                                if session_id not in streaming_manager._pending_references:
                                    streaming_manager._pending_references[session_id] = {"rich": [], "lean": []}
                                
                                # ğŸ”¥ ä¿®å¤ï¼šä¸streaming_manager._execute_tools_parallel()ä¿æŒå®Œå…¨ä¸€è‡´çš„å¼•ç”¨æ„å»ºé€»è¾‘
                                rich_refs = []
                                lean_refs = []
                                
                                for item in result_data.get("results", []):
                                    meta = item.get("metadata", {})
                                    
                                    # ğŸ†• ä½¿ç”¨å·¥å…·è¿”å›çš„å…¨å±€åºå·ï¼ˆå·²åœ¨knowledge_retrieval.pyä¸­åˆ†é…ï¼‰
                                    global_marker = item.get("ref_marker")
                                    if not global_marker:
                                        logger.warning(f"âš ï¸ æ£€ç´¢ç»“æœç¼ºå°‘ref_markerå­—æ®µï¼item: {item.keys()}")
                                        continue
                                    
                                    # è·å–chunk_idï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
                                    chunk_id = meta.get("chunk_id", "")
                                    
                                    # ğŸ¯ ã€å…³é”®ä¿®å¤ã€‘ç”Ÿæˆå”¯ä¸€çš„ref_idç”¨äºå»é‡
                                    # ä½¿ç”¨ chunk_id ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼ˆå¦‚æœæ²¡æœ‰åˆ™ç”¨å†…å®¹å“ˆå¸Œï¼‰
                                    import hashlib
                                    if chunk_id:
                                        ref_id = chunk_id
                                    else:
                                        # å¦‚æœæ²¡æœ‰chunk_idï¼Œç”¨å†…å®¹å“ˆå¸Œä½œä¸ºå”¯ä¸€æ ‡è¯†
                                        content = item.get("content", "")
                                        ref_id = hashlib.md5(content.encode('utf-8')).hexdigest()
                                    
                                    # Richæ ¼å¼ï¼šåŒ…å«å®Œæ•´å†…å®¹å’Œå…ƒæ•°æ®ï¼ˆå‘é€åˆ°å‰ç«¯ï¼‰
                                    rich_refs.append({
                                        "ref_id": ref_id,  # ğŸ¯ å”¯ä¸€æ ‡è¯†ï¼ˆç”¨äºå»é‡ï¼‰
                                        "ref_marker": global_marker,  # ğŸ†• å…¨å±€åºå·ï¼ˆç”¨äº##æ•°å­—$$å¼•ç”¨ï¼‰
                                        "document_id": meta.get("document_id") or meta.get("source"),
                                        "chunk_id": chunk_id,
                                        "score": item.get("score", 0.0),
                                        "document_name": meta.get("source"),
                                        "content": item.get("content", ""),
                                        "metadata": meta,
                                        "doc_id": meta.get("doc_id", ""),
                                        "kb_id": meta.get("kb_id", ""),
                                        "filename": meta.get("filename", "")
                                    })
                                    
                                    # Leanæ ¼å¼ï¼šä»…ä¿å­˜ç´¢å¼•ä¿¡æ¯ï¼ˆä¿å­˜åˆ°æ•°æ®åº“ï¼‰
                                    lean_refs.append({
                                        "ref_id": ref_id,  # ğŸ¯ å”¯ä¸€æ ‡è¯†ï¼ˆç”¨äºå»é‡ï¼‰
                                        "ref_marker": global_marker,  # ğŸ†• å…¨å±€åºå·
                                        "document_id": meta.get("document_id") or meta.get("source"),
                                        "chunk_id": chunk_id,
                                        "score": item.get("score", 0.0),
                                        "doc_id": meta.get("doc_id", ""),
                                        "kb_id": meta.get("kb_id", ""),
                                        "filename": meta.get("filename", "")
                                    })
                                
                                # è¿½åŠ åˆ°å¾…å¤„ç†å¼•ç”¨
                                streaming_manager._pending_references[session_id]["rich"].extend(rich_refs)
                                streaming_manager._pending_references[session_id]["lean"].extend(lean_refs)
                                
                                logger.info(f"  ğŸ“š æ”¶é›†åˆ° {len(rich_refs)} æ¡çŸ¥è¯†åº“å¼•ç”¨ï¼ˆref_markerèŒƒå›´: {rich_refs[0].get('ref_marker') if rich_refs else '?'} - {rich_refs[-1].get('ref_marker') if rich_refs else '?'}ï¼‰")
                        except json.JSONDecodeError:
                            logger.warning(f"  âš ï¸ æ— æ³•è§£æçŸ¥è¯†åº“æ£€ç´¢ç»“æœ")
                        except Exception as ref_err:
                            logger.error(f"  âŒ æå–å¼•ç”¨æ•°æ®å¤±è´¥: {ref_err}")
                    
                except Exception as tool_err:
                    logger.error(f"  âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {tool_name} | é”™è¯¯: {tool_err}")
                    result = f"å·¥å…·è°ƒç”¨å¤±è´¥: {str(tool_err)}"
                
                # æ·»åŠ å·¥å…·ç»“æœ
                # ğŸ”§ ç¡®ä¿ content æ˜¯å­—ç¬¦ä¸²ï¼ˆLLM API è¦æ±‚ï¼‰
                import json
                content_str = json.dumps(result, ensure_ascii=False) if isinstance(result, dict) else str(result)
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call_id,
                    "name": tool_name,
                    "content": content_str
                })
            
            # ğŸ†• æ¯è½®å·¥å…·è°ƒç”¨åè¿›è¡Œå»é‡ï¼ˆå¤ç”¨ streaming_manager çš„å»é‡é€»è¾‘ï¼‰
            await streaming_manager._deduplicate_knowledge_base_results(
                session_id=session_id,
                tool_calls=tool_calls
            )
        
        # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
        logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•° ({max_iter})")
        
        # ğŸ†• æå–æœ€ç»ˆçš„å¼•ç”¨æ•°æ®
        final_references = []
        if hasattr(streaming_manager, '_pending_references') and session_id in streaming_manager._pending_references:
            refs_data = streaming_manager._pending_references[session_id]
            final_references = refs_data.get("lean", [])
        
        return {
            "content": "[æç¤º] å·²è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œè¯·ç¨åé‡è¯•ã€‚",
            "tool_calls_made": tool_calls_made,
            "skip_reply": skip_reply_called,
            "references": final_references
        } 