"""
é€šç”¨æµå¼è¾“å‡ºç®¡ç†å™¨

è§£å†³æ‰€æœ‰æ¨¡å‹çš„æµå¼è¾“å‡ºé—®é¢˜ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨ï¼Œç¡®ä¿çœŸæ­£çš„å¼‚æ­¥å¹¶å‘å¤„ç†
"""

import asyncio
import json
import logging
import time
import concurrent.futures
from typing import AsyncGenerator, Dict, Any, List, Optional, Callable
from dataclasses import dataclass
from enum import Enum
import uuid
from .streaming_config import streaming_config
from .tool_config import tool_config  # ğŸ‘ˆ å¯¼å…¥å…¨å±€é…ç½®

logger = logging.getLogger(__name__)


class StreamingState(Enum):
    """æµå¼è¾“å‡ºçŠ¶æ€"""
    IDLE = "idle"
    THINKING = "thinking"
    TOOL_CALLING = "tool_calling"
    GENERATING = "generating"
    COMPLETED = "completed"
    ERROR = "error"


@dataclass
class StreamingSession:
    """æµå¼ä¼šè¯ä¿¡æ¯"""
    session_id: str
    user_id: str
    websocket: Any
    state: StreamingState = StreamingState.IDLE
    current_task: Optional[asyncio.Task] = None
    start_time: float = 0
    last_activity: float = 0
    
    def __post_init__(self):
        self.start_time = time.time()
        self.last_activity = time.time()


class UniversalStreamingManager:
    """
    é€šç”¨æµå¼è¾“å‡ºç®¡ç†å™¨
    
    ç‰¹æ€§ï¼š
    1. æ”¯æŒæ‰€æœ‰æ¨¡å‹çš„çœŸæ­£æµå¼è¾“å‡º
    2. å·¥å…·è°ƒç”¨æœŸé—´çš„å®æ—¶è¿›åº¦åé¦ˆ
    3. çœŸæ­£çš„å¼‚æ­¥å¹¶å‘ï¼Œç”¨æˆ·ä¹‹é—´ä¸ä¼šç›¸äº’é˜»å¡
    4. æ™ºèƒ½çš„æµå¼è¾“å‡ºä¼˜åŒ–
    5. å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
    """
    
    def __init__(self):
        self.active_sessions: Dict[str, StreamingSession] = {}
        self.session_lock = asyncio.Lock()
        self.thread_pool = None
        self._cleanup_task = None
        
        # åˆå§‹åŒ–çº¿ç¨‹æ± ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if streaming_config.use_thread_pool_for_sync_calls:
            self.thread_pool = concurrent.futures.ThreadPoolExecutor(
                max_workers=streaming_config.thread_pool_max_workers
            )
        
        # å»¶è¿Ÿå¯åŠ¨æ¸…ç†ä»»åŠ¡ï¼ˆåœ¨æœ‰äº‹ä»¶å¾ªç¯æ—¶å¯åŠ¨ï¼‰
        self._cleanup_task = None
        self._cleanup_started = False
        
        # ğŸ†• å¹¶å‘æ§åˆ¶ä¿¡å·é‡ï¼ˆæ¯ä¸ªä¼šè¯ç‹¬ç«‹ï¼‰
        self._tool_semaphores: Dict[str, asyncio.Semaphore] = {}
        
        # ğŸ†• å·¥å…·è°ƒç”¨ç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self._tool_cache: Dict[str, Any] = {}  # {cache_key: result}
        
        # ğŸ†• å·¥å…·è°ƒç”¨ç»Ÿè®¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self._tool_stats: Dict[str, Dict[str, Any]] = {}  # {session_id: stats}
    
    def _start_cleanup_task(self):
        """å¯åŠ¨æ¸…ç†ä»»åŠ¡ï¼ˆå¦‚æœæœ‰äº‹ä»¶å¾ªç¯ï¼‰"""
        if self._cleanup_started:
            return
            
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯
            loop = asyncio.get_running_loop()
            
            async def cleanup_loop():
                while True:
                    try:
                        await asyncio.sleep(streaming_config.cleanup_interval)
                        await self._cleanup_expired_sessions()
                    except asyncio.CancelledError:
                        break
                    except Exception as e:
                        logger.error(f"æ¸…ç†ä»»åŠ¡å‡ºé”™: {e}")
            
            self._cleanup_task = asyncio.create_task(cleanup_loop())
            self._cleanup_started = True
            logger.info("âœ… æ¸…ç†ä»»åŠ¡å·²å¯åŠ¨")
        except RuntimeError:
            # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œç¨åå¯åŠ¨
            logger.debug("æš‚æ— äº‹ä»¶å¾ªç¯ï¼Œæ¸…ç†ä»»åŠ¡å°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶å¯åŠ¨")
    
    async def _cleanup_expired_sessions(self):
        """æ¸…ç†è¿‡æœŸä¼šè¯"""
        current_time = time.time()
        expired_sessions = []
        
        async with self.session_lock:
            for session_id, session in self.active_sessions.items():
                if current_time - session.last_activity > streaming_config.session_timeout:
                    expired_sessions.append(session_id)
        
        for session_id in expired_sessions:
            logger.info(f"æ¸…ç†è¿‡æœŸä¼šè¯: {session_id}")
            await self.unregister_session(session_id)
    
    async def shutdown(self):
        """å…³é—­ç®¡ç†å™¨"""
        if self._cleanup_task:
            self._cleanup_task.cancel()
        
        if self.thread_pool:
            self.thread_pool.shutdown(wait=True)
        
        # æ¸…ç†æ‰€æœ‰ä¼šè¯
        session_ids = list(self.active_sessions.keys())
        for session_id in session_ids:
            await self.unregister_session(session_id)
        
    async def register_session(self, session_id: str, user_id: str, websocket: Any) -> StreamingSession:
        """æ³¨å†Œæ–°çš„æµå¼ä¼šè¯"""
        # ç¡®ä¿æ¸…ç†ä»»åŠ¡å·²å¯åŠ¨
        self._start_cleanup_task()
        
        async with self.session_lock:
            session = StreamingSession(
                session_id=session_id,
                user_id=user_id,
                websocket=websocket
            )
            self.active_sessions[session_id] = session
            logger.info(f"ğŸ”— æ³¨å†Œæµå¼ä¼šè¯: {session_id} (ç”¨æˆ·: {user_id})")
            return session
    
    async def unregister_session(self, session_id: str):
        """æ³¨é”€æµå¼ä¼šè¯"""
        async with self.session_lock:
            if session_id in self.active_sessions:
                session = self.active_sessions[session_id]
                if session.current_task and not session.current_task.done():
                    session.current_task.cancel()
                del self.active_sessions[session_id]
                logger.info(f"ğŸ”Œ æ³¨é”€æµå¼ä¼šè¯: {session_id}")
    
    async def update_session_state(self, session_id: str, state: StreamingState, 
                                 message: Optional[str] = None):
        """æ›´æ–°ä¼šè¯çŠ¶æ€å¹¶é€šçŸ¥å‰ç«¯"""
        if session_id not in self.active_sessions:
            return
            
        session = self.active_sessions[session_id]
        session.state = state
        session.last_activity = time.time()
        
        # å‘é€çŠ¶æ€æ›´æ–°åˆ°å‰ç«¯
        try:
            status_data = {
                "type": "status_update",
                "state": state.value,
                "message": message,
                "timestamp": time.time()
            }
            await session.websocket.send_json(status_data)
        except Exception as e:
            logger.warning(f"å‘é€çŠ¶æ€æ›´æ–°å¤±è´¥: {e}")
    
    async def send_tool_status(self, session_id: str, tool_name: str, status: str, 
                             args: Optional[Dict] = None, error: Optional[str] = None):
        """å‘é€å·¥å…·çŠ¶æ€åˆ°å‰ç«¯ï¼ˆç”¨äºçŠ¶æ€æ°”æ³¡æ˜¾ç¤ºï¼‰"""
        if session_id not in self.active_sessions:
            return
            
        session = self.active_sessions[session_id]
        
        try:
            tool_status_data = {
                "type": "tool_status",
                "tool": tool_name,
                "status": status,  # calling, success, error
                "args": args,
                "error": error
            }
            await session.websocket.send_json(tool_status_data)
            logger.debug(f"ğŸ”§ å‘é€å·¥å…·çŠ¶æ€: {tool_name} - {status}")
        except Exception as e:
            logger.warning(f"å‘é€å·¥å…·çŠ¶æ€å¤±è´¥: {e}")
    
    async def generate_stream_universal(
        self,
        session_id: str,
        llm_service: Any,
        user_message: str,
        history: List[Dict[str, Any]],
        model_settings: Dict[str, Any],
        system_prompt: Optional[str] = None,
        enable_tools: bool = True,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        é€šç”¨æµå¼ç”Ÿæˆæ–¹æ³•
        
        æ”¯æŒæ‰€æœ‰æ¨¡å‹ï¼Œè‡ªåŠ¨å¤„ç†å·¥å…·è°ƒç”¨ï¼Œç¡®ä¿çœŸæ­£çš„æµå¼è¾“å‡º
        """
        
        if session_id not in self.active_sessions:
            raise ValueError(f"ä¼šè¯æœªæ³¨å†Œ: {session_id}")
        
        session = self.active_sessions[session_id]
        
        try:
            # âš ï¸ ã€å…³é”®ä¿®å¤ã€‘æ¯æ¬¡æ–°å¯¹è¯å¼€å§‹æ—¶æ¸…ç©ºä¸Šæ¬¡ä¿å­˜çš„å›¾ç‰‡URL
            if hasattr(llm_service, 'last_saved_images'):
                llm_service.last_saved_images = []
                logger.debug("ğŸ§¹ å·²æ¸…ç©ºä¸Šæ¬¡ä¿å­˜çš„å›¾ç‰‡URL")
            
            # æ›´æ–°çŠ¶æ€ä¸ºæ€è€ƒä¸­
            await self.update_session_state(session_id, StreamingState.THINKING, "æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜...")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨ï¼ˆä¼ é€’model_settingsç”¨äºæ¨¡å‹èƒ½åŠ›æ£€æŸ¥ï¼‰
            should_use_tools = await self._should_use_tools(user_message, llm_service, model_settings)
            
            if enable_tools and should_use_tools:
                # ä½¿ç”¨å·¥å…·è°ƒç”¨æµå¼ç”Ÿæˆ
                async for chunk in self._generate_with_tools_streaming(
                    session_id, llm_service, user_message, history, 
                    model_settings, system_prompt, **kwargs
                ):
                    yield chunk
            else:
                # ç›´æ¥æµå¼ç”Ÿæˆ
                await self.update_session_state(session_id, StreamingState.GENERATING, "æ­£åœ¨ç”Ÿæˆå›å¤...")
                async for chunk in self._generate_direct_streaming(
                    session_id, llm_service, user_message, history,
                    model_settings, system_prompt, **kwargs
                ):
                    yield chunk
            
            # å®Œæˆ
            await self.update_session_state(session_id, StreamingState.COMPLETED, "å›å¤å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆé”™è¯¯: {e}")
            await self.update_session_state(session_id, StreamingState.ERROR, f"ç”Ÿæˆå¤±è´¥: {str(e)}")
            yield f"\n[é”™è¯¯] ç”Ÿæˆå¤±è´¥: {str(e)}\n"
    
    async def _should_use_tools(
        self, 
        user_message: str, 
        llm_service: Any,
        model_settings: Optional[Dict[str, Any]] = None
    ) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·è°ƒç”¨
        
        æ£€æŸ¥é¡ºåºï¼š
        1. æ£€æŸ¥æœåŠ¡æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨æ–¹æ³•
        2. æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨é»‘åå•ä¸­ï¼ˆå·²çŸ¥ä¸æ”¯æŒï¼‰
        """
        
        # 1. æ£€æŸ¥LLMæœåŠ¡æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨
        if not hasattr(llm_service, 'generate_with_tools') and \
           not hasattr(llm_service, '_call_llm_with_tools_sync'):
            logger.debug("LLMæœåŠ¡ä¸æ”¯æŒå·¥å…·è°ƒç”¨æ–¹æ³•")
            return False
        
        # 2. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²çŸ¥ä¸æ”¯æŒå·¥å…·è°ƒç”¨
        if model_settings:
            model_name = model_settings.get("modelName", "")
            if model_name:
                try:
                    from .model_capability_manager import model_capability_manager
                    
                    # æŸ¥è¯¢æ¨¡å‹èƒ½åŠ›ï¼ˆä¸‰å±‚ç¼“å­˜ï¼šæœ¬åœ° â†’ Redis â†’ MongoDBï¼‰
                    supports = await model_capability_manager.check_supports_tools(model_name)
                    
                    if not supports:
                        logger.info(f"ğŸš« æ¨¡å‹ {model_name} å·²çŸ¥ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œè·³è¿‡MCP")
                        return False
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ æŸ¥è¯¢æ¨¡å‹èƒ½åŠ›å¤±è´¥ï¼Œé»˜è®¤å…è®¸å°è¯•: {e}")
        
        return True
    
    async def _generate_direct_streaming(
        self,
        session_id: str,
        llm_service: Any,
        user_message: str,
        history: List[Dict[str, Any]],
        model_settings: Dict[str, Any],
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """ç›´æ¥æµå¼ç”Ÿæˆï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰"""
        
        # ğŸ› è°ƒè¯•ï¼šæ£€æŸ¥images_base64æ˜¯å¦å­˜åœ¨
        images_base64 = kwargs.get('images_base64', [])
        logger.info(f"ğŸ–¼ï¸ [streaming_manager._generate_direct_streaming] æ¥æ”¶åˆ°images_base64: {len(images_base64) if images_base64 else 0}å¼ å›¾ç‰‡")
        logger.info(f"ğŸ–¼ï¸ [streaming_manager._generate_direct_streaming] kwargsåŒ…å«: {list(kwargs.keys())}")
        
        try:
            async for chunk in llm_service.generate_stream(
                user_message=user_message,
                history=history,
                model_settings=model_settings,
                system_prompt=system_prompt,
                session_id=session_id,
                **kwargs
            ):
                # æ›´æ–°ä¼šè¯æ´»åŠ¨æ—¶é—´
                if session_id in self.active_sessions:
                    self.active_sessions[session_id].last_activity = time.time()
                yield chunk
                
        except Exception as e:
            logger.error(f"ç›´æ¥æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    async def _generate_with_tools_streaming(
        self,
        session_id: str,
        llm_service: Any,
        user_message: str,
        history: List[Dict[str, Any]],
        model_settings: Dict[str, Any],
        system_prompt: Optional[str] = None,
        max_iterations: Optional[int] = None,  # ğŸ‘ˆ æ”¹ä¸ºå¯é€‰ï¼Œè‡ªåŠ¨è¯»å–å…¨å±€é…ç½®
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        å¸¦å·¥å…·è°ƒç”¨çš„æµå¼ç”Ÿæˆ
        
        æ ¸å¿ƒä¼˜åŒ–ï¼š
        1. å¹¶è¡Œå¤„ç†å·¥å…·è°ƒç”¨å’ŒçŠ¶æ€æ›´æ–°
        2. æ™ºèƒ½çš„æµå¼è¾“å‡ºç¼“å†²
        3. å®æ—¶è¿›åº¦åé¦ˆ
        
        å‚æ•°:
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ŒNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½® (tool_config.max_iterations)
        """
        
        # ğŸ‘‡ ä½¿ç”¨å…¨å±€é…ç½®æˆ–ä¼ å…¥å‚æ•°
        max_iter = max_iterations if max_iterations is not None else tool_config.max_iterations
        logger.info(f"ğŸ”§ å·¥å…·è°ƒç”¨æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iter} (å…¨å±€é…ç½®: {tool_config.max_iterations})")
        
        from ...mcp.manager import mcp_manager
        
        # è·å–MCPå®¢æˆ·ç«¯
        mcp_client = mcp_manager.get_client()
        if not mcp_client:
            logger.warning("MCPå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œå›é€€åˆ°ç›´æ¥æµå¼ç”Ÿæˆ")
            async for chunk in self._generate_direct_streaming(
                session_id, llm_service, user_message, history, 
                model_settings, system_prompt, **kwargs
            ):
                yield chunk
            return
        
        # è·å–ç”¨æˆ·IDï¼ˆä»å·²æ³¨å†Œçš„ä¼šè¯ä¸­ï¼‰
        session = self.active_sessions.get(session_id)
        user_id = session.user_id if session else None
        
        # è·å–å¯ç”¨å·¥å…·ï¼ˆä¼ é€’session_idå’Œuser_idä»¥æ”¯æŒç”¨æˆ·å·¥å…·é…ç½®è¿‡æ»¤ï¼‰
        tools = await mcp_client.list_tools(session_id=session_id, user_id=user_id)
        if not tools:
            logger.info("æ— å¯ç”¨å·¥å…·ï¼Œä½¿ç”¨ç›´æ¥æµå¼ç”Ÿæˆ")
            async for chunk in self._generate_direct_streaming(
                session_id, llm_service, user_message, history,
                model_settings, system_prompt, **kwargs
            ):
                yield chunk
            return
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = llm_service._build_messages(system_prompt or "", history, user_message)
        iteration = 0
        has_output_started = False  # ğŸ¯ è¿½è¸ªæ˜¯å¦å·²ç»å¼€å§‹è¾“å‡ºå†…å®¹
        reached_limit = False  # ğŸ¯ è¿½è¸ªæ˜¯å¦çœŸçš„è¾¾åˆ°äº†è¿­ä»£ä¸Šé™
        last_iteration_had_tool_calls = False  # ğŸ¯ è¿½è¸ªä¸Šä¸€æ¬¡è¿­ä»£æ˜¯å¦åŒæ—¶è¾“å‡ºäº† content å’Œ tool_calls
        
        # ğŸ†• æ€»è¶…æ—¶æ§åˆ¶
        workflow_start_time = time.time()
        
        while iteration < max_iter:  # ğŸ‘ˆ ä½¿ç”¨å…¨å±€é…ç½®
            iteration += 1
            
            # ğŸ†• æ£€æŸ¥æ€»è¶…æ—¶
            elapsed_time = time.time() - workflow_start_time
            if elapsed_time > tool_config.total_timeout:
                logger.warning(f"âš ï¸ å·¥å…·è°ƒç”¨æµç¨‹æ€»è¶…æ—¶ï¼ˆ{elapsed_time:.1f}ç§’ > {tool_config.total_timeout}ç§’ï¼‰")
                if tool_config.force_reply_on_max_iterations:
                    yield "\n\nâš ï¸ å·¥å…·è°ƒç”¨è¶…æ—¶ï¼Œæ­£åœ¨ç”Ÿæˆæœ€ç»ˆå›å¤...\n\n"
                    # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼Œæç¤ºæ¨¡å‹è¶…æ—¶
                    messages.append({
                        "role": "system",
                        "content": f"âš ï¸ å·¥å…·è°ƒç”¨å·²è¶…æ—¶ï¼ˆ{elapsed_time:.1f}ç§’ï¼‰ï¼Œè¯·æ ¹æ®å·²è·å–çš„ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆå›å¤ã€‚"
                    })
                    # å¼ºåˆ¶ç”Ÿæˆæœ€ç»ˆå›å¤
                    async for chunk in self._generate_direct_streaming(
                        session_id, llm_service, "", [], model_settings, None, 
                        messages=messages, **kwargs
                    ):
                        yield chunk
                break
            
            logger.info(f"ğŸ”„ å·¥å…·è°ƒç”¨è¿­ä»£ {iteration}/{max_iter} (å·²ç”¨æ—¶: {elapsed_time:.1f}ç§’/{tool_config.total_timeout}ç§’)")
            
            # ğŸ¯ ç´¯ç§¯æ¨¡å‹åœ¨å·¥å…·è°ƒç”¨å‰è¾“å‡ºçš„æè¿°æ–‡å­—
            accumulated_content = ""
            is_first_content_in_iteration = True  # ğŸ¯ è¿½è¸ªå½“å‰è¿­ä»£æ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡è¾“å‡ºå†…å®¹
            logger.info(f"ğŸ” è¿­ä»£ {iteration} å¼€å§‹ï¼Œlast_iteration_had_tool_calls={last_iteration_had_tool_calls}")
            
            try:
                # ğŸš€ çœŸæµå¼ï¼šç›´æ¥ä½¿ç”¨å¼‚æ­¥æµå¼ç”Ÿæˆå™¨
                async for event in self._call_llm_streaming_with_tools(
                    llm_service, messages, tools, session_id, model_settings, **kwargs
                ):
                    # å¤„ç†ä¸åŒç±»å‹çš„äº‹ä»¶
                    if event["type"] == "content_delta":
                        # ğŸ¯ å†…å®¹ç‰‡æ®µç›´æ¥è¾“å‡ºï¼ˆçœŸæµå¼ï¼ï¼‰
                        if not has_output_started:
                            await self.update_session_state(
                                session_id,
                                StreamingState.GENERATING,
                                None
                            )
                            has_output_started = True
                            logger.info("âœ… é¦–æ¬¡è¾“å‡ºï¼ˆçœŸæµå¼å†…å®¹ï¼‰ï¼Œå·²é€šçŸ¥å‰ç«¯éšè—çŠ¶æ€æ°”æ³¡")
                        
                        # ğŸ¯ å¦‚æœä¸Šä¸€æ¬¡è¿­ä»£åŒæ—¶è¾“å‡ºäº† content å’Œ tool_callsï¼Œä¸”å½“å‰æ˜¯ç¬¬ä¸€æ¬¡è¾“å‡ºå†…å®¹ï¼Œæ’å…¥åˆ†éš”ç¬¦
                        # ä½œç”¨ï¼šåˆ†éš”å·¥å…·è°ƒç”¨æ—¶çš„æè¿°æ–‡å­—ï¼ˆå¦‚"ğŸ” æ­£åœ¨æ£€ç´¢..."ï¼‰å’Œæœ€ç»ˆå›å¤å†…å®¹
                        if last_iteration_had_tool_calls and is_first_content_in_iteration and event["content"].strip():
                            yield "\n\n---\n\n"
                            is_first_content_in_iteration = False
                            logger.info(f"âœ… ç¬¬ {iteration} æ¬¡è¿­ä»£ï¼Œæ’å…¥åˆ†éš”çº¿åˆ†éš”å·¥å…·è°ƒç”¨æè¿°å’Œæœ€ç»ˆå›å¤")
                        
                        # ğŸ¯ æ ‡è®°å½“å‰è¿­ä»£å·²ç»è¾“å‡ºè¿‡ content
                        if event["content"].strip() and is_first_content_in_iteration:
                            is_first_content_in_iteration = False
                        
                        # ğŸ¯ ç´¯ç§¯å†…å®¹ï¼ˆç”¨äºä¿å­˜åˆ°æ¶ˆæ¯å†å²ï¼‰
                        accumulated_content += event["content"]
                        
                        # ç›´æ¥é€ä¼ å†…å®¹ï¼ˆä¸ç´¯ç§¯ã€ä¸æ¨¡æ‹Ÿï¼‰
                        yield event["content"]
                        if session_id in self.active_sessions:
                            self.active_sessions[session_id].last_activity = time.time()
                    
                    elif event["type"] == "tool_calls":
                        # ğŸ¯ æ”¶åˆ°å·¥å…·è°ƒç”¨è¯·æ±‚
                        tool_calls = event["tool_calls"]
                        logger.info(f"ğŸ”§ éœ€è¦è°ƒç”¨ {len(tool_calls)} ä¸ªå·¥å…·ï¼ˆçœŸæµå¼ï¼‰")
                        
                        # ğŸ¯ å¦‚æœæ¨¡å‹è¾“å‡ºäº†æè¿°æ–‡å­—ï¼Œè®°å½•æ—¥å¿—
                        if accumulated_content.strip():
                            logger.info(f"ğŸ’¬ æ¨¡å‹å·¥å…·è°ƒç”¨æè¿°: {accumulated_content[:100]}")
                        
                        # ğŸ¯ æ ‡è®°ï¼šå½“å‰è¿­ä»£åŒæ—¶è¾“å‡ºäº† content å’Œ tool_calls
                        # åªæœ‰å½“ accumulated_content éç©ºæ—¶æ‰æ ‡è®°ï¼ˆå¦‚æœ content ä¸º null åˆ™ä¸æ ‡è®°ï¼‰
                        last_iteration_had_tool_calls = bool(accumulated_content.strip())
                        logger.info(f"ğŸ” è®¾ç½® last_iteration_had_tool_calls={last_iteration_had_tool_calls}, accumulated_content={accumulated_content[:50]}")
                        
                        # æ·»åŠ åˆ°æ¶ˆæ¯å†å²ï¼ˆä¿ç•™æ¨¡å‹è¾“å‡ºçš„æè¿°ï¼‰
                        messages.append({
                            "role": "assistant",
                            "content": accumulated_content if accumulated_content.strip() else None,
                            "tool_calls": tool_calls
                        })
                        
                        # è·³å‡ºäº‹ä»¶å¾ªç¯ï¼Œå‡†å¤‡æ‰§è¡Œå·¥å…·
                        break
                    
                    elif event["type"] == "done":
                        # ğŸ¯ æµå¼å®Œæˆï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰
                        logger.info("âœ… çœŸæµå¼å®Œæˆï¼Œæ— å·¥å…·è°ƒç”¨")
                        
                        # ğŸ–¼ï¸ ä¿å­˜å›¾ç‰‡
                        await self._save_pending_images_after_tools(llm_service)
                        
                        # é€€å‡ºè¿­ä»£å¾ªç¯ï¼ˆæ­£å¸¸å®Œæˆï¼Œä¸æ˜¯è¾¾åˆ°ä¸Šé™ï¼‰
                        iteration = max_iter  # ğŸ‘ˆ ä½¿ç”¨å…¨å±€é…ç½®
                        reached_limit = False  # æ˜ç¡®æ ‡è®°ï¼šè¿™æ˜¯æ­£å¸¸å®Œæˆ
                        break
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨éœ€è¦æ‰§è¡Œ
                if event.get("type") != "tool_calls":
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸ
                    break
                
                # æœ‰å·¥å…·è°ƒç”¨ï¼Œç»§ç»­æ‰§è¡Œ
                tool_calls = event["tool_calls"]
                
            except NotImplementedError as e:
                # æ¨¡å‹ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œæ ‡è®°å¹¶é™çº§åˆ°ç›´æ¥æµå¼ç”Ÿæˆ
                logger.warning(f"âš ï¸ æ¨¡å‹ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œåˆ‡æ¢åˆ°æ™®é€šå¯¹è¯æ¨¡å¼: {e}")
                
                # âœ… æ ‡è®°æ¨¡å‹ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆå†™å…¥MongoDB + Redis + æœ¬åœ°ç¼“å­˜ï¼‰
                model_name = model_settings.get("modelName", "")
                if model_name:
                    try:
                        from .model_capability_manager import model_capability_manager
                        await model_capability_manager.mark_unsupported(
                            model_name,
                            error_message=str(e),
                            notes="è‡ªåŠ¨æ£€æµ‹ï¼šå·¥å…·è°ƒç”¨è¿”å›NotImplementedError"
                        )
                    except Exception as mark_error:
                        logger.error(f"æ ‡è®°æ¨¡å‹èƒ½åŠ›å¤±è´¥: {mark_error}")
                
                # é™çº§åˆ°æ™®é€šæµå¼ç”Ÿæˆ
                async for chunk in self._generate_direct_streaming(
                    session_id, llm_service, user_message, history,
                    model_settings, system_prompt, **kwargs
                ):
                    yield chunk
                return
            except Exception as e:
                logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
                yield f"\n[é”™è¯¯] åˆ†æå¤±è´¥: {str(e)}\n"
                break
            
            # ğŸ¯ å¦‚æœè¿˜æ²¡å¼€å§‹è¾“å‡ºï¼Œç«‹å³éšè—çŠ¶æ€æ°”æ³¡
            if not has_output_started:
                await self.update_session_state(
                    session_id,
                    StreamingState.GENERATING,
                    None
                )
                has_output_started = True
                logger.info("âœ… å·¥å…·è°ƒç”¨å¼€å§‹ï¼Œå·²é€šçŸ¥å‰ç«¯éšè—çŠ¶æ€æ°”æ³¡")
            
            # ğŸ¯ ç§»é™¤äº†"æ­£åœ¨è°ƒç”¨Xä¸ªå·¥å…·..."çš„çŠ¶æ€æ°”æ³¡
            # å› ä¸ºå·¥å…·è°ƒç”¨æ€è€ƒè¿‡ç¨‹å·²ç»åœ¨ <think> æ ‡ç­¾ä¸­æ˜¾ç¤º
            
            tool_results = await self._execute_tools_parallel(
                tool_calls, mcp_client, session_id, kwargs.get('user_id')
            )
            
            # ğŸ¯ æ£€æŸ¥æ˜¯å¦æœ‰å¾…å‘é€çš„å¼•ç”¨æ•°æ®ï¼ˆå¢é‡æ•°æ®ï¼‰
            if hasattr(self, '_pending_references') and session_id in self._pending_references:
                refs_data = self._pending_references.get(session_id)
                if refs_data and refs_data.get('rich'):
                    # ğŸ” è°ƒè¯•ï¼šæ‰“å°å³å°†å‘é€çš„ref_idå’Œåºå·
                    sending_info = [(r.get("ref_marker", "?"), r.get("ref_id", "")[:8] + "..." if r.get("ref_id") else "EMPTY") for r in refs_data.get('rich', [])]
                    logger.info(f"ğŸ“¤ å‡†å¤‡å‘é€ {len(refs_data.get('rich', []))} æ¡MCPå·¥å…·å¼•ç”¨æ•°æ®åˆ°chat routerï¼ˆè¿­ä»£ {iteration}/{max_iter}ï¼‰")  # ğŸ‘ˆ ä½¿ç”¨å…¨å±€é…ç½®
                    logger.info(f"ğŸ” å‘é€çš„åºå·å’Œref_id: {sending_info}")
                # é€šè¿‡ç‰¹æ®Šæ ‡è®°å‘é€å¼•ç”¨æ•°æ®
                refs_json = json.dumps(refs_data, ensure_ascii=False)
                yield f"__REFERENCES__{refs_json}__END__"
                logger.info(f"âœ… å·²æˆåŠŸå‘é€ {len(refs_data.get('rich', []))} æ¡å¢é‡å¼•ç”¨æ•°æ®åˆ°chat router")
                
                # ğŸ¯ ã€å…³é”®ä¿®å¤ã€‘å‘é€åæ¸…ç©º _pending_referencesï¼ˆä½†ä¿ç•™ _sent_ref_idsï¼‰
                # _pending_references åªå­˜å‚¨å¾…å‘é€çš„å¢é‡æ•°æ®
                # _sent_ref_ids è®°å½•æ‰€æœ‰å·²å‘é€çš„ref_idï¼Œç”¨äºå»é‡
                self._pending_references[session_id] = {"rich": [], "lean": []}
                logger.info(f"ğŸ§¹ å·²æ¸…ç©ºå¾…å‘é€ç¼“å­˜ï¼ˆå·²å‘é€çš„ref_idä»ä¿ç•™ç”¨äºå»é‡ï¼‰")
            
            # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯åˆ—è¡¨
            for result in tool_results:
                messages.append(result)
            
            # ğŸ¯ æ£€æŸ¥æ˜¯å¦çœŸçš„è¾¾åˆ°äº†è¿­ä»£ä¸Šé™ï¼ˆè€Œä¸æ˜¯æ­£å¸¸å®Œæˆï¼‰
            if iteration >= max_iter:  # ğŸ‘ˆ ä½¿ç”¨å…¨å±€é…ç½®
                reached_limit = True
        
        # ğŸ†• å¦‚æœçœŸçš„è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆè€Œä¸æ˜¯æ­£å¸¸å®Œæˆï¼‰
        if reached_limit:
            if tool_config.force_reply_on_max_iterations:
                logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•° ({max_iter})ï¼Œå¼ºåˆ¶ç”Ÿæˆæœ€ç»ˆå›å¤")
                yield "\n\nâš ï¸ å·²è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œæ­£åœ¨ç”Ÿæˆæœ€ç»ˆå›å¤...\n\n"
                
                # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼Œæç¤ºæ¨¡å‹å·¥å…·è°ƒç”¨æ¬¡æ•°å·²è¾¾ä¸Šé™
                messages.append({
                    "role": "system",
                    "content": "âš ï¸ å·¥å…·è°ƒç”¨æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼Œè¯·æ ¹æ®å·²è·å–çš„ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆå›å¤ã€‚"
                })
                
                # å¼ºåˆ¶è°ƒç”¨ä¸€æ¬¡ LLM ç”Ÿæˆæœ€ç»ˆå›å¤ï¼ˆä¸å¸¦å·¥å…·ï¼‰
                try:
                    async for chunk in self._generate_direct_streaming(
                        session_id, llm_service, "", [], model_settings, None,
                        messages=messages, **kwargs
                    ):
                        yield chunk
                except Exception as e:
                    logger.error(f"âŒ å¼ºåˆ¶ç”Ÿæˆæœ€ç»ˆå›å¤å¤±è´¥: {e}")
                    yield f"\n\n[é”™è¯¯] ç”Ÿæˆæœ€ç»ˆå›å¤å¤±è´¥: {str(e)}\n"
            else:
                yield "\n[æç¤º] å·²è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œè¯·å°è¯•é‡æ–°æé—®ã€‚\n"
        
        # ğŸ¯ ã€å…³é”®ã€‘å·¥å…·è°ƒç”¨å¾ªç¯ç»“æŸåï¼Œæ¸…ç†è¯¥ä¼šè¯çš„æ‰€æœ‰å¼•ç”¨ç›¸å…³ç¼“å­˜
        # æ— è®ºæ˜¯æ­£å¸¸å®Œæˆè¿˜æ˜¯è¾¾åˆ°ä¸Šé™ï¼Œéƒ½åº”è¯¥æ¸…ç†
        if hasattr(self, '_pending_references') and session_id in self._pending_references:
            del self._pending_references[session_id]
        if hasattr(self, '_sent_ref_ids') and session_id in self._sent_ref_ids:
            del self._sent_ref_ids[session_id]
        if hasattr(self, '_last_ref_marker') and session_id in self._last_ref_marker:
            del self._last_ref_marker[session_id]
        
        logger.info(f"ğŸ§¹ å·¥å…·è°ƒç”¨æµç¨‹ç»“æŸï¼Œå·²æ¸…ç†ä¼šè¯ {session_id} çš„æ‰€æœ‰å¼•ç”¨æ•°æ®ç¼“å­˜")
        
        # ğŸ†• è¾“å‡ºå·¥å…·è°ƒç”¨ç»Ÿè®¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if tool_config.enable_tool_stats and session_id in self._tool_stats:
            stats = self._tool_stats[session_id]
            logger.info(f"ğŸ“Š å·¥å…·è°ƒç”¨ç»Ÿè®¡ [ä¼šè¯ {session_id}]:")
            logger.info(f"   æ€»è°ƒç”¨: {stats['total_calls']}, æˆåŠŸ: {stats['successful_calls']}, å¤±è´¥: {stats['failed_calls']}, ç¼“å­˜: {stats['cached_calls']}")
            logger.info(f"   æ€»è€—æ—¶: {stats['total_time']:.2f}ç§’")
            if stats['by_tool']:
                logger.info(f"   æŒ‰å·¥å…·ç»Ÿè®¡:")
                for tool_name, tool_stats in stats['by_tool'].items():
                    avg_time = tool_stats['total_time'] / tool_stats['calls'] if tool_stats['calls'] > 0 else 0
                    logger.info(f"     - {tool_name}: {tool_stats['calls']}æ¬¡ (æˆåŠŸ:{tool_stats['success']}, å¤±è´¥:{tool_stats['failed']}, ç¼“å­˜:{tool_stats['cached']}, å¹³å‡:{avg_time:.2f}ç§’)")
    
    def get_tool_stats(self, session_id: str) -> Optional[Dict[str, Any]]:
        """è·å–å·¥å…·è°ƒç”¨ç»Ÿè®¡"""
        if tool_config.enable_tool_stats and session_id in self._tool_stats:
            return self._tool_stats[session_id].copy()
        return None
    
    def clear_tool_cache(self):
        """æ¸…ç©ºå·¥å…·ç¼“å­˜"""
        if tool_config.enable_tool_cache:
            cache_size = len(self._tool_cache)
            self._tool_cache.clear()
            logger.info(f"ğŸ§¹ å·²æ¸…ç©ºå·¥å…·ç¼“å­˜ ({cache_size} æ¡è®°å½•)")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        return {
            "enabled": tool_config.enable_tool_cache,
            "size": len(self._tool_cache) if tool_config.enable_tool_cache else 0
        }
    
    async def _call_llm_async_with_tools(
        self, 
        llm_service: Any, 
        messages: List[Dict], 
        tools: List[Dict],
        session_id: str,
        model_settings: Dict[str, Any],
        **kwargs  # ğŸ–¼ï¸ æ¥æ”¶å›¾ç‰‡ç­‰é¢å¤–å‚æ•°
    ) -> Dict[str, Any]:
        """
        å¼‚æ­¥è°ƒç”¨LLMï¼ˆå¸¦å·¥å…·ï¼‰
        
        å…³é”®ä¼˜åŒ–ï¼šå°†åŒæ­¥è°ƒç”¨åŒ…è£…ä¸ºå¼‚æ­¥ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
        """
        
        def sync_call():
            # è·å–å…·ä½“çš„æ¨¡å‹æœåŠ¡å®ä¾‹
            if hasattr(llm_service, '_create_service_instance'):
                # å¯¹äºLLMServiceï¼Œä½¿ç”¨_create_service_instanceæ–¹æ³•
                model_service = model_settings.get("modelService", "deepseek")
                base_url = model_settings.get("baseUrl", "")
                api_key = model_settings.get("apiKey", "")
                model_name = model_settings.get("modelName", "")
                current_service = llm_service._create_service_instance(model_service, base_url, api_key, model_name)
            elif hasattr(llm_service, '_get_service'):
                # å¯¹äºå…¶ä»–æœåŠ¡ï¼Œä½¿ç”¨_get_serviceæ–¹æ³•
                current_service = llm_service._get_service(model_settings)
            else:
                # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„æœåŠ¡
                current_service = llm_service
            
            if not current_service:
                raise ValueError("æ— æ³•åˆ›å»ºæ¨¡å‹æœåŠ¡å®ä¾‹")
            
            if not hasattr(current_service, '_call_llm_with_tools_sync'):
                raise NotImplementedError(f"æ¨¡å‹æœåŠ¡ {current_service.__class__.__name__} ä¸æ”¯æŒå·¥å…·è°ƒç”¨")
            
            # ğŸ–¼ï¸ ä¿å­˜æœåŠ¡å®ä¾‹å¼•ç”¨ï¼Œä¾›å›¾ç‰‡ä¿å­˜ä½¿ç”¨
            llm_service._last_service_instance = current_service
            
            # ğŸ¯ æå–ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹å‚æ•°
            model_params = model_settings.get("modelParams", {})
            logger.info(f"ğŸ”§ å·¥å…·è°ƒç”¨ä¼ é€’ç”¨æˆ·æ¨¡å‹å‚æ•°: {json.dumps(model_params, ensure_ascii=False) if model_params else 'æ— '}")
            
            # ğŸ–¼ï¸ æå–å¹¶ä¼ é€’å›¾ç‰‡æ•°æ®åŠå…¶ä»–å‚æ•°ï¼ˆsession_id, message_id, user_idç­‰ï¼‰
            images_base64 = kwargs.pop('images_base64', None)  # ä½¿ç”¨popç§»é™¤,é¿å…é‡å¤ä¼ é€’
            if images_base64:
                logger.info(f"ğŸ–¼ï¸ å·¥å…·è°ƒç”¨ä¼ é€’ {len(images_base64)} å¼ å›¾ç‰‡")
            
            # âš ï¸ ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿session_idåœ¨kwargsä¸­ï¼ˆå› ä¸ºå®ƒæ˜¯æ˜¾å¼å‚æ•°ï¼Œéœ€è¦æ‰‹åŠ¨åŠ å›å»ï¼‰
            if 'session_id' not in kwargs:
                kwargs['session_id'] = session_id
            
            # âš ï¸ ã€å…³é”®ä¿®å¤ã€‘å§‹ç»ˆä½¿ç”¨å…³é”®å­—å‚æ•°ä¼ é€’ï¼Œé¿å…å‚æ•°ä½ç½®é”™ä¹±
            # ğŸ¯ ä½¿ç”¨æµå¼å·¥å…·è°ƒç”¨ï¼ˆé»˜è®¤å¯ç”¨ï¼Œè‡ªåŠ¨å…¼å®¹æ‰€æœ‰æ¨¡å‹ï¼‰
            return current_service._call_llm_with_tools_sync(
                messages=messages,
                tools=tools,
                model_params=model_params,
                images_base64=images_base64,  # å³ä½¿ä¸ºNoneä¹Ÿæ˜¾å¼ä¼ é€’
                use_streaming=streaming_config.use_streaming_tool_calls,  # ğŸ¯ æµå¼å·¥å…·è°ƒç”¨ï¼ˆé»˜è®¤Trueï¼‰
                **kwargs
            )
        
        # ä½¿ç”¨é…ç½®çš„çº¿ç¨‹æ± æˆ–é»˜è®¤æ‰§è¡Œå™¨
        loop = asyncio.get_event_loop()
        executor = self.thread_pool if streaming_config.use_thread_pool_for_sync_calls else None
        
        try:
            # æ·»åŠ è¶…æ—¶æ§åˆ¶ï¼ˆä½¿ç”¨ tool_config çš„ llm_call_timeoutï¼‰
            return await asyncio.wait_for(
                loop.run_in_executor(executor, sync_call),
                timeout=tool_config.llm_call_timeout
            )
        except asyncio.TimeoutError:
            logger.error(f"LLMè°ƒç”¨è¶…æ—¶ (ä¼šè¯: {session_id}, è¶…æ—¶æ—¶é—´: {tool_config.llm_call_timeout}ç§’)")
            raise Exception(f"LLMè°ƒç”¨è¶…æ—¶ï¼ˆè¶…è¿‡{tool_config.llm_call_timeout}ç§’ï¼‰")
        except NotImplementedError:
            # é‡æ–°æŠ›å‡ºNotImplementedErrorï¼Œè®©ä¸Šå±‚å¤„ç†æ¨¡å‹ä¸æ”¯æŒå·¥å…·è°ƒç”¨çš„æƒ…å†µ
            raise
    
    async def _call_llm_streaming_with_tools(
        self,
        llm_service: Any,
        messages: List[Dict[str, Any]],
        tools: List[Dict[str, Any]],
        session_id: str,
        model_settings: Dict[str, Any],
        **kwargs
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        çœŸæµå¼è°ƒç”¨ LLMï¼ˆå¸¦å·¥å…·æ”¯æŒï¼‰
        
        ç›´æ¥ä½¿ç”¨åº•å±‚æœåŠ¡çš„æµå¼ç”Ÿæˆå™¨ï¼Œä¸ç´¯ç§¯å†…å®¹
        
        Yields:
            dict: äº‹ä»¶å¯¹è±¡
                - {"type": "content_delta", "content": "..."}
                - {"type": "tool_calls", "tool_calls": [...]}
                - {"type": "done", "finish_reason": "stop"}
        """
        # ğŸ¯ è·å–æ¨¡å‹å‚æ•°ï¼ˆä½¿ç”¨æ­£ç¡®çš„é”®å modelParamsï¼‰
        model_params = model_settings.get("modelParams", {})
        logger.info(f"ğŸ”§ çœŸæµå¼å·¥å…·è°ƒç”¨ä¼ é€’ç”¨æˆ·æ¨¡å‹å‚æ•°: {json.dumps(model_params, ensure_ascii=False) if model_params else 'æ— '}")
        
        # ä» kwargs ä¸­æå– images_base64ï¼Œé¿å…é‡å¤ä¼ é€’
        images_base64 = kwargs.pop('images_base64', None)
        
        # ğŸ”§ è·å–å…·ä½“çš„æœåŠ¡å®ä¾‹ï¼ˆæ ¹æ® model_settings åŠ¨æ€åˆ›å»ºï¼Œé¿å…ä½¿ç”¨è¿‡æœŸçš„ç¼“å­˜ï¼‰
        if hasattr(llm_service, '_create_service_instance'):
            # å¯¹äºLLMServiceï¼Œä½¿ç”¨_create_service_instanceæ–¹æ³•
            model_service = model_settings.get("modelService", "deepseek")
            base_url = model_settings.get("baseUrl", "")
            api_key = model_settings.get("apiKey", "")
            model_name = model_settings.get("modelName", "")
            current_service = llm_service._create_service_instance(model_service, base_url, api_key, model_name)
            
            if not current_service:
                raise ValueError(f"æ— æ³•åˆ›å»ºæ¨¡å‹æœåŠ¡å®ä¾‹: {model_service}")
            
            # ä¿å­˜æœåŠ¡å®ä¾‹å¼•ç”¨ï¼ˆä¾›å›¾ç‰‡ä¿å­˜ä½¿ç”¨ï¼‰
            llm_service._last_service_instance = current_service
        elif hasattr(llm_service, '_get_service'):
            # å¯¹äºå…¶ä»–æœåŠ¡ï¼Œä½¿ç”¨_get_serviceæ–¹æ³•
            current_service = llm_service._get_service(model_settings)
        else:
            # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„æœåŠ¡
            current_service = llm_service
        
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦æ”¯æŒçœŸæµå¼
        if not hasattr(current_service, '_call_llm_with_tools_streaming'):
            logger.warning("âš ï¸ æœåŠ¡ä¸æ”¯æŒçœŸæµå¼ï¼Œé™çº§åˆ°åŒæ­¥æ¨¡å¼")
            # é™çº§ï¼šä½¿ç”¨åŒæ­¥æ–¹æ³•å¹¶æ¨¡æ‹Ÿæµå¼
            response = await self._call_llm_async_with_tools(
                llm_service, messages, tools, session_id, model_settings, 
                images_base64=images_base64, **kwargs
            )
            
            # æ£€æŸ¥å·¥å…·è°ƒç”¨
            if response.get("tool_calls"):
                yield {
                    "type": "tool_calls",
                    "tool_calls": response["tool_calls"]
                }
            else:
                # æ¨¡æ‹Ÿæµå¼è¾“å‡ºå†…å®¹
                content = response.get("content", "")
                async for chunk in self._smart_streaming_output(content):
                    yield {
                        "type": "content_delta",
                        "content": chunk
                    }
            
            yield {
                "type": "done",
                "finish_reason": "stop"
            }
            return
        
        # ğŸš€ ä½¿ç”¨çœŸæµå¼
        async for event in current_service._call_llm_with_tools_streaming(
            messages=messages,
            tools=tools,
            model_params=model_params,
            images_base64=images_base64,
            **kwargs
        ):
            yield event
    
    async def _save_pending_images_after_tools(self, llm_service: Any):
        """
        åœ¨å·¥å…·è°ƒç”¨å®Œæˆåï¼Œä¿å­˜ç¼“å­˜çš„å›¾ç‰‡åˆ°MinIO
        
        ç”¨äºå·¥å…·è°ƒç”¨æ¨¡å¼ä¸‹çš„å›¾ç‰‡ä¿å­˜ï¼Œå› ä¸ºå·¥å…·è°ƒç”¨æ˜¯åŒæ­¥+éæµå¼çš„ï¼Œ
        éœ€è¦åœ¨æœ€ç»ˆå›å¤æ—¶æ‰è§¦å‘å›¾ç‰‡ä¿å­˜é€»è¾‘ã€‚
        """
        try:
            # è·å–å…·ä½“çš„æœåŠ¡å®ä¾‹ï¼ˆå¤„ç†LLMServiceåŒ…è£…çš„æƒ…å†µï¼‰
            current_service = llm_service
            if hasattr(llm_service, '_last_service_instance'):
                current_service = llm_service._last_service_instance
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„å›¾ç‰‡æ•°æ®
            if not hasattr(current_service, '_pending_images'):
                logger.debug("å·¥å…·è°ƒç”¨æ¨¡å¼ï¼šæ— ç¼“å­˜çš„å›¾ç‰‡æ•°æ®")
                return
            
            pending = current_service._pending_images
            images_base64 = pending.get('images_base64')
            session_id = pending.get('session_id')
            message_id = pending.get('message_id')
            user_id = pending.get('user_id')
            
            if not images_base64 or not session_id or not message_id:
                logger.warning(f"âš ï¸ ç¼“å­˜çš„å›¾ç‰‡æ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡ä¿å­˜")
                return
            
            logger.info(f"ğŸ–¼ï¸ å·¥å…·è°ƒç”¨æ¨¡å¼ï¼šå¼€å§‹ä¿å­˜ {len(images_base64)} å¼ ç¼“å­˜å›¾ç‰‡åˆ°MinIO...")
            
            # è°ƒç”¨å…·ä½“æœåŠ¡çš„å›¾ç‰‡ä¿å­˜æ–¹æ³•
            if hasattr(current_service, '_save_images_to_minio'):
                saved_images = await current_service._save_images_to_minio(
                    images_base64=images_base64,
                    session_id=session_id,
                    message_id=message_id,
                    user_id=user_id
                )
                
                # ä¿å­˜åˆ°å®ä¾‹å˜é‡ä¾›å¤–éƒ¨è®¿é—®
                current_service.last_saved_images = saved_images
                # âš ï¸ ã€å…³é”®ä¿®å¤ã€‘åŒæ—¶ä¿å­˜åˆ° llm_serviceï¼Œä¾› chat.py è·å–
                if hasattr(llm_service, 'last_saved_images'):
                    llm_service.last_saved_images = saved_images
                    logger.info(f"âœ… å·¥å…·è°ƒç”¨æ¨¡å¼ï¼šæˆåŠŸä¿å­˜ {len(saved_images)} å¼ å›¾ç‰‡å¹¶åŒæ­¥åˆ° llm_service")
                else:
                    logger.info(f"âœ… å·¥å…·è°ƒç”¨æ¨¡å¼ï¼šæˆåŠŸä¿å­˜ {len(saved_images)} å¼ å›¾ç‰‡")
                
                # æ¸…ç†ç¼“å­˜
                delattr(current_service, '_pending_images')
            else:
                logger.warning(f"âš ï¸ æœåŠ¡ {current_service.__class__.__name__} ä¸æ”¯æŒå›¾ç‰‡ä¿å­˜")
                
        except Exception as e:
            logger.error(f"âŒ å·¥å…·è°ƒç”¨æ¨¡å¼ï¼šä¿å­˜å›¾ç‰‡å¤±è´¥: {e}", exc_info=True)
    
    async def _deduplicate_knowledge_base_results(
        self,
        session_id: str,
        tool_calls: List[Dict]
    ):
        """
        å¯¹çŸ¥è¯†åº“æ£€ç´¢ç»“æœè¿›è¡Œå…¨å±€å»é‡
        
        åœºæ™¯ï¼š
        1. å¹¶è¡Œè°ƒç”¨ï¼šæ¨¡å‹ä¸€æ¬¡è¿”å›å¤šä¸ª search_knowledge_base
        2. ä¸²è¡Œè°ƒç”¨ï¼šæ¨¡å‹å¤šè½®è¿­ä»£ï¼Œæ¯è½®éƒ½è°ƒç”¨ search_knowledge_base
        3. æ··åˆè°ƒç”¨ï¼šç¬¬ä¸€è½®å¹¶è¡Œ3æ¬¡ï¼Œç¬¬äºŒè½®åˆå¹¶è¡Œ2æ¬¡...
        
        ç­–ç•¥ï¼š
        - ğŸ¯ ã€å…³é”®ã€‘å…¨å±€å»é‡ï¼šè·¨æ‰€æœ‰è½®æ¬¡ç´¯ç§¯çš„æ‰€æœ‰æ•°æ®è¿›è¡Œå»é‡
        - ä½¿ç”¨å†…å®¹å“ˆå¸Œä½œä¸ºå»é‡é”®
        - ä¿ç•™åˆ†æ•°æœ€é«˜çš„ç»“æœ
        - å»é‡åé‡æ–°åˆ†é…å…¨å±€åºå·ï¼ˆä»1å¼€å§‹è¿ç»­ç¼–å·ï¼‰
        
        æ³¨æ„ï¼š
        - æ¯è½®å·¥å…·è°ƒç”¨åéƒ½ä¼šè¿½åŠ æ–°æ•°æ®åˆ° _pending_references
        - æœ¬å‡½æ•°è´Ÿè´£å¯¹ç´¯ç§¯çš„æ‰€æœ‰æ•°æ®è¿›è¡Œå»é‡ï¼ˆåŒ…æ‹¬å†å²è½®æ¬¡çš„æ•°æ®ï¼‰
        """
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„å¼•ç”¨æ•°æ®
        if not hasattr(self, '_pending_references') or session_id not in self._pending_references:
            return
        
        # æ£€æŸ¥æœ¬è½®æ˜¯å¦æœ‰ search_knowledge_base è°ƒç”¨
        has_kb_search = any(
            tc.get("function", {}).get("name") == "search_knowledge_base"
            for tc in tool_calls
        )
        
        if not has_kb_search:
            return
        
        refs_data = self._pending_references[session_id]
        rich_refs = refs_data.get("rich", [])
        lean_refs = refs_data.get("lean", [])
        
        if not rich_refs:
            return
        
        logger.info(f"ğŸ”„ å¼€å§‹å…¨å±€å»é‡çŸ¥è¯†åº“æ£€ç´¢ç»“æœï¼ˆç´¯ç§¯æ€»æ•°: {len(rich_refs)} æ¡ï¼‰")
        
        # ğŸ¯ ä½¿ç”¨å†…å®¹å“ˆå¸Œè¿›è¡Œå…¨å±€å»é‡
        import hashlib
        
        def get_content_hash(content: str) -> str:
            """è®¡ç®—å†…å®¹å“ˆå¸Œ"""
            return hashlib.md5(content.encode('utf-8')).hexdigest()
        
        # æ„å»ºå»é‡å­—å…¸ï¼š{content_hash: (max_score, best_rich_ref, best_lean_ref)}
        dedup_map = {}
        
        # ç¡®ä¿ rich_refs å’Œ lean_refs é•¿åº¦ä¸€è‡´
        if len(rich_refs) != len(lean_refs):
            logger.warning(f"âš ï¸ rich_refs å’Œ lean_refs é•¿åº¦ä¸ä¸€è‡´: {len(rich_refs)} vs {len(lean_refs)}")
            # æˆªæ–­åˆ°è¾ƒçŸ­çš„é•¿åº¦
            min_len = min(len(rich_refs), len(lean_refs))
            rich_refs = rich_refs[:min_len]
            lean_refs = lean_refs[:min_len]
        
        # ğŸ¯ éå†æ‰€æœ‰ç´¯ç§¯çš„å¼•ç”¨ï¼ˆè·¨æ‰€æœ‰è½®æ¬¡ï¼‰
        for rich_ref, lean_ref in zip(rich_refs, lean_refs):
            content = rich_ref.get("content", "")
            if not content:
                continue
            
            content_hash = get_content_hash(content)
            score = rich_ref.get("score", 0.0)
            
            # ä¿ç•™åˆ†æ•°æœ€é«˜çš„ç‰ˆæœ¬
            if content_hash not in dedup_map or score > dedup_map[content_hash][0]:
                dedup_map[content_hash] = (score, rich_ref, lean_ref)
        
        # æå–å»é‡åçš„ç»“æœï¼ˆæŒ‰åˆ†æ•°é™åºï¼‰
        deduped_items = sorted(dedup_map.values(), key=lambda x: x[0], reverse=True)
        
        # ğŸ¯ ã€å…³é”®ä¿®å¤ã€‘ä¿ç•™å·²å‘é€çš„æ—§æ•°æ®ï¼Œåªæ ‡è®°æ–°å¢çš„æ•°æ®
        # ç­–ç•¥ï¼š
        # 1. è®°å½•ä¸Šä¸€è½®å·²ç»å»é‡å¹¶å‘é€çš„æ•°æ®ï¼ˆé€šè¿‡ref_idï¼‰
        # 2. æœ¬è½®å»é‡åï¼Œæ ‡è®°å“ªäº›æ˜¯æ–°å¢çš„
        # 3. åªç»™æ–°å¢çš„æ•°æ®åˆ†é…æ–°åºå·
        
        # è·å–å·²å‘é€çš„ref_idé›†åˆ
        if not hasattr(self, '_sent_ref_ids'):
            self._sent_ref_ids = {}
        if session_id not in self._sent_ref_ids:
            self._sent_ref_ids[session_id] = set()
        
        # è·å–å½“å‰çš„æœ€å¤§åºå·
        if not hasattr(self, '_last_ref_marker'):
            self._last_ref_marker = {}
        current_max_marker = self._last_ref_marker.get(session_id, 0)
        
        new_rich_refs = []
        new_lean_refs = []
        
        for score, rich_ref, lean_ref in deduped_items:
            ref_id = rich_ref.get("ref_id", "")
            
            if ref_id in self._sent_ref_ids[session_id]:
                # è¿™æ˜¯å·²å‘é€çš„æ—§æ•°æ®ï¼Œè·³è¿‡ï¼ˆä¸æ·»åŠ åˆ°å¾…å‘é€åˆ—è¡¨ï¼‰
                continue
            else:
                # è¿™æ˜¯æ–°æ•°æ®ï¼Œåˆ†é…æ–°åºå·
                current_max_marker += 1
                rich_ref["ref_marker"] = current_max_marker
                lean_ref["ref_marker"] = current_max_marker
                
                new_rich_refs.append(rich_ref)
                new_lean_refs.append(lean_ref)
                
                # æ ‡è®°ä¸ºå·²å‘é€
                self._sent_ref_ids[session_id].add(ref_id)
        
        # æ›´æ–°æœ€å¤§åºå·
        if current_max_marker > 0:
            self._last_ref_marker[session_id] = current_max_marker
        
        # ğŸ¯ æ›´æ–°ä¸ºå»é‡åçš„å…¨å±€æ•°æ®
        self._pending_references[session_id]["rich"] = new_rich_refs
        self._pending_references[session_id]["lean"] = new_lean_refs
        
        removed_count = len(rich_refs) - len(new_rich_refs)
        logger.info(f"âœ… å…¨å±€å»é‡å®Œæˆï¼ˆå»é‡å‰: {len(rich_refs)} æ¡ â†’ å»é‡å: {len(new_rich_refs)} æ¡ï¼Œå»é™¤: {removed_count} æ¡é‡å¤ï¼‰")
        
        # æ‰“å°å…¨å±€åºå·èŒƒå›´
        if new_rich_refs:
            markers = [r.get("ref_marker") for r in new_rich_refs]
            logger.info(f"ğŸ“š å…¨å±€åºå·èŒƒå›´: {markers[0]} - {markers[-1]}")
    
    def _get_tool_semaphore(self, session_id: str) -> asyncio.Semaphore:
        """è·å–ä¼šè¯çš„å·¥å…·å¹¶å‘æ§åˆ¶ä¿¡å·é‡"""
        if session_id not in self._tool_semaphores:
            self._tool_semaphores[session_id] = asyncio.Semaphore(tool_config.max_concurrent_tools)
        return self._tool_semaphores[session_id]
    
    def _get_cache_key(self, tool_name: str, tool_args: Dict) -> str:
        """ç”Ÿæˆå·¥å…·è°ƒç”¨çš„ç¼“å­˜é”®"""
        import hashlib
        args_str = json.dumps(tool_args, sort_keys=True, ensure_ascii=False)
        key = f"{tool_name}:{args_str}"
        return hashlib.md5(key.encode('utf-8')).hexdigest()
    
    def _init_tool_stats(self, session_id: str):
        """åˆå§‹åŒ–å·¥å…·è°ƒç”¨ç»Ÿè®¡"""
        if tool_config.enable_tool_stats and session_id not in self._tool_stats:
            self._tool_stats[session_id] = {
                "total_calls": 0,
                "successful_calls": 0,
                "failed_calls": 0,
                "cached_calls": 0,
                "total_time": 0.0,
                "by_tool": {}
            }
    
    def _record_tool_call(self, session_id: str, tool_name: str, success: bool, duration: float, cached: bool = False):
        """è®°å½•å·¥å…·è°ƒç”¨ç»Ÿè®¡"""
        if not tool_config.enable_tool_stats:
            return
        
        self._init_tool_stats(session_id)
        stats = self._tool_stats[session_id]
        
        stats["total_calls"] += 1
        stats["total_time"] += duration
        
        if cached:
            stats["cached_calls"] += 1
        elif success:
            stats["successful_calls"] += 1
        else:
            stats["failed_calls"] += 1
        
        # æŒ‰å·¥å…·ç»Ÿè®¡
        if tool_name not in stats["by_tool"]:
            stats["by_tool"][tool_name] = {
                "calls": 0,
                "success": 0,
                "failed": 0,
                "cached": 0,
                "total_time": 0.0
            }
        
        tool_stats = stats["by_tool"][tool_name]
        tool_stats["calls"] += 1
        tool_stats["total_time"] += duration
        
        if cached:
            tool_stats["cached"] += 1
        elif success:
            tool_stats["success"] += 1
        else:
            tool_stats["failed"] += 1
        
        if tool_config.verbose_logging:
            logger.info(f"ğŸ“Š å·¥å…·ç»Ÿè®¡ [{tool_name}]: æ€»è°ƒç”¨={stats['total_calls']}, æˆåŠŸ={stats['successful_calls']}, å¤±è´¥={stats['failed_calls']}, ç¼“å­˜={stats['cached_calls']}")
    
    def _truncate_tool_result(self, result: str, tool_name: str) -> str:
        """æˆªæ–­è¿‡å¤§çš„å·¥å…·è¿”å›ç»“æœ"""
        result_bytes = len(result.encode('utf-8'))
        
        if result_bytes > tool_config.max_tool_result_size:
            if tool_config.verbose_logging:
                logger.warning(
                    f"âš ï¸ å·¥å…· {tool_name} è¿”å›ç»“æœè¿‡å¤§ "
                    f"({result_bytes} å­—èŠ‚)ï¼Œæˆªæ–­åˆ° {tool_config.max_tool_result_size} å­—èŠ‚"
                )
            
            # æˆªæ–­åˆ°æŒ‡å®šå¤§å°
            truncated = result.encode('utf-8')[:tool_config.max_tool_result_size].decode('utf-8', errors='ignore')
            return truncated + f"\n\nâš ï¸ [ç»“æœè¿‡å¤§ï¼Œå·²æˆªæ–­ã€‚åŸå§‹å¤§å°: {result_bytes} å­—èŠ‚ï¼Œæˆªæ–­å: {tool_config.max_tool_result_size} å­—èŠ‚]"
        
        return result
    
    async def _execute_tools_parallel(
        self,
        tool_calls: List[Dict],
        mcp_client: Any,
        session_id: str,
        user_id: Optional[str]
    ) -> List[Dict]:
        """
        å¹¶è¡Œæ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆå¸¦å¹¶å‘æ§åˆ¶ã€ç¼“å­˜ã€ç»Ÿè®¡ã€è¶…æ—¶ç­‰åŠŸèƒ½ï¼‰
        
        æ–°å¢åŠŸèƒ½ï¼š
        1. å¹¶å‘æ§åˆ¶ï¼ˆmax_concurrent_toolsï¼‰
        2. å·¥å…·ç»“æœç¼“å­˜ï¼ˆenable_tool_cacheï¼‰
        3. å·¥å…·è°ƒç”¨ç»Ÿè®¡ï¼ˆenable_tool_statsï¼‰
        4. ç»“æœå¤§å°æˆªæ–­ï¼ˆmax_tool_result_sizeï¼‰
        5. å•ä¸ªå·¥å…·æ‰§è¡Œè¶…æ—¶ï¼ˆtool_execution_timeoutï¼‰
        6. é”™è¯¯ç»§ç»­æ§åˆ¶ï¼ˆallow_continue_on_errorï¼‰
        7. è¯¦ç»†æ—¥å¿—æ§åˆ¶ï¼ˆverbose_loggingï¼‰
        """
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        self._init_tool_stats(session_id)
        
        # è·å–å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        semaphore = self._get_tool_semaphore(session_id)
        
        async def execute_single_tool(tool_call):
            tool_name = tool_call.get("function", {}).get("name")
            tool_args_str = tool_call.get("function", {}).get("arguments", "{}")
            tool_call_id = tool_call.get("id", "")
            
            start_time = time.time()
            
            # ğŸ”’ å¹¶å‘æ§åˆ¶ï¼šè·å–ä¿¡å·é‡
            async with semaphore:
                try:
                    # è§£æå‚æ•°
                    if isinstance(tool_args_str, str):
                        tool_args = json.loads(tool_args_str)
                    else:
                        tool_args = tool_args_str
                    
                    if tool_config.verbose_logging:
                        logger.info(f"ğŸ”§ æ‰§è¡Œå·¥å…·: {tool_name}, å‚æ•°: {tool_args}")
                    else:
                        logger.info(f"ğŸ”§ æ‰§è¡Œå·¥å…·: {tool_name}")
                    
                    # ğŸ¯ æ£€æŸ¥ç¼“å­˜
                    cache_key = None
                    if tool_config.enable_tool_cache:
                        cache_key = self._get_cache_key(tool_name, tool_args)
                        if cache_key in self._tool_cache:
                            cached_result = self._tool_cache[cache_key]
                            duration = time.time() - start_time
                            self._record_tool_call(session_id, tool_name, True, duration, cached=True)
                            
                            if tool_config.verbose_logging:
                                logger.info(f"ğŸ’¾ ä½¿ç”¨ç¼“å­˜ç»“æœ: {tool_name}")
                            
                            return {
                                "role": "tool",
                                "tool_call_id": tool_call_id,
                                "name": tool_name,
                                "content": str(cached_result)
                            }
                    
                    # ğŸ¯ å‘é€å·¥å…·çŠ¶æ€åˆ°å‰ç«¯çŠ¶æ€æ°”æ³¡ï¼ˆä¸æ˜¯æ¶ˆæ¯æ°”æ³¡ï¼‰
                    await self.send_tool_status(
                        session_id=session_id,
                        tool_name=tool_name,
                        status="calling",
                        args=tool_args
                    )
                    
                    # ğŸ• æ‰§è¡Œå·¥å…·ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
                    try:
                        result = await asyncio.wait_for(
                            mcp_client.call_tool(
                                tool_name=tool_name,
                                arguments=tool_args,
                                session_id=session_id,
                                user_id=user_id
                            ),
                            timeout=tool_config.tool_execution_timeout
                        )
                    except asyncio.TimeoutError:
                        raise Exception(f"å·¥å…·æ‰§è¡Œè¶…æ—¶ï¼ˆè¶…è¿‡{tool_config.tool_execution_timeout}ç§’ï¼‰")
                    
                    duration = time.time() - start_time
                    
                    if tool_config.verbose_logging:
                        logger.info(f"âœ… å·¥å…·æ‰§è¡ŒæˆåŠŸ: {tool_name} (è€—æ—¶: {duration:.2f}ç§’)")
                    else:
                        logger.info(f"âœ… å·¥å…·æ‰§è¡ŒæˆåŠŸ: {tool_name}")
                    
                    # ğŸ¯ æˆªæ–­è¿‡å¤§çš„ç»“æœ
                    result_str = str(result)
                    result_str = self._truncate_tool_result(result_str, tool_name)
                    
                    # ğŸ¯ ç¼“å­˜ç»“æœ
                    if tool_config.enable_tool_cache and cache_key:
                        self._tool_cache[cache_key] = result_str
                        if tool_config.verbose_logging:
                            logger.info(f"ğŸ’¾ å·²ç¼“å­˜å·¥å…·ç»“æœ: {tool_name}")
                    
                    # ğŸ¯ è®°å½•ç»Ÿè®¡
                    self._record_tool_call(session_id, tool_name, True, duration)
                    
                    # ğŸ¯ ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯çŸ¥è¯†åº“æ£€ç´¢å·¥å…·ï¼Œæå–å¹¶å‘é€å¼•ç”¨æ•°æ®
                    if tool_name == "search_knowledge_base" and isinstance(result_str, str):
                        try:
                            result_data = json.loads(result_str)
                            if result_data.get("success") and result_data.get("results"):
                                # åˆå§‹åŒ–å¼•ç”¨å­˜å‚¨
                                if not hasattr(self, '_pending_references'):
                                    self._pending_references = {}
                                if session_id not in self._pending_references:
                                    self._pending_references[session_id] = {"rich": [], "lean": []}
                                
                                # æ„å»ºå¼•ç”¨æ•°æ®ï¼ˆrichå’Œleanæ ¼å¼ï¼‰
                                rich_refs = []
                                lean_refs = []
                                
                                for item in result_data.get("results", []):
                                    meta = item.get("metadata", {})
                                    
                                    # ğŸ†• ä½¿ç”¨å·¥å…·è¿”å›çš„å…¨å±€åºå·ï¼ˆå·²åœ¨knowledge_retrieval.pyä¸­åˆ†é…ï¼‰
                                    global_marker = item.get("ref_marker")
                                    if not global_marker:
                                        logger.warning(f"âš ï¸ æ£€ç´¢ç»“æœç¼ºå°‘ref_markerå­—æ®µï¼item: {item.keys()}")
                                        continue
                                    
                                    # è·å–chunk_idï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
                                    chunk_id = meta.get("chunk_id", "")
                                    
                                    # ğŸ¯ ã€å…³é”®ä¿®å¤ã€‘ç”Ÿæˆå”¯ä¸€çš„ref_idç”¨äºå»é‡
                                    # ä½¿ç”¨ chunk_id ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼ˆå¦‚æœæ²¡æœ‰åˆ™ç”¨å†…å®¹å“ˆå¸Œï¼‰
                                    import hashlib
                                    if chunk_id:
                                        ref_id = chunk_id
                                    else:
                                        # å¦‚æœæ²¡æœ‰chunk_idï¼Œç”¨å†…å®¹å“ˆå¸Œä½œä¸ºå”¯ä¸€æ ‡è¯†
                                        content = item.get("content", "")
                                        ref_id = hashlib.md5(content.encode('utf-8')).hexdigest()
                                    
                                    # ğŸ” è°ƒè¯•ï¼šè®°å½•å…¨å±€åºå·
                                    logger.info(f"âœ… æå–å…¨å±€åºå· {global_marker}: ref_id={ref_id[:12]}..., source={meta.get('source', 'Unknown')}")
                                    
                                    # Richæ ¼å¼ï¼šåŒ…å«å®Œæ•´å†…å®¹å’Œå…ƒæ•°æ®ï¼ˆå‘é€åˆ°å‰ç«¯ï¼‰
                                    rich_refs.append({
                                        "ref_id": ref_id,  # ğŸ¯ å”¯ä¸€æ ‡è¯†ï¼ˆç”¨äºå»é‡ï¼‰
                                        "ref_marker": global_marker,  # ğŸ†• å…¨å±€åºå·ï¼ˆç”¨äº##æ•°å­—$$å¼•ç”¨ï¼‰
                                        "document_id": meta.get("document_id") or meta.get("source"),
                                        "chunk_id": chunk_id,
                                        "score": item.get("score", 0.0),
                                        "document_name": meta.get("source"),
                                        "content": item.get("content", ""),
                                        "metadata": meta,
                                        # ğŸ†• æ·»åŠ æŸ¥çœ‹åŸæ–‡æ‰€éœ€çš„å­—æ®µåˆ°é¡¶å±‚
                                        "doc_id": meta.get("doc_id", ""),
                                        "kb_id": meta.get("kb_id", ""),
                                        "filename": meta.get("filename", "")
                                    })
                                    
                                    # Leanæ ¼å¼ï¼šä»…ä¿å­˜ç´¢å¼•ä¿¡æ¯ï¼ˆä¿å­˜åˆ°æ•°æ®åº“ï¼‰
                                    # ğŸ†• æ·»åŠ æŸ¥çœ‹åŸæ–‡æ‰€éœ€çš„å­—æ®µ
                                    lean_refs.append({
                                        "ref_id": ref_id,  # ğŸ¯ å”¯ä¸€æ ‡è¯†ï¼ˆç”¨äºå»é‡ï¼‰
                                        "ref_marker": global_marker,  # ğŸ†• å…¨å±€åºå·
                                        "document_id": meta.get("document_id") or meta.get("source"),
                                        "chunk_id": chunk_id,
                                        "score": item.get("score", 0.0),
                                        "doc_id": meta.get("doc_id", ""),
                                        "kb_id": meta.get("kb_id", ""),
                                        "filename": meta.get("filename", "")
                                    })
                                
                                # è¿½åŠ æ–°çš„å¼•ç”¨æ•°æ®
                                self._pending_references[session_id]["rich"].extend(rich_refs)
                                self._pending_references[session_id]["lean"].extend(lean_refs)
                                
                                # ğŸ” è°ƒè¯•ï¼šæ‰“å°ç´¯ç§¯çš„å…¨å±€åºå·
                                all_markers = [r.get("ref_marker", "?") for r in self._pending_references[session_id]["rich"]]
                                logger.info(f"ğŸ“š å·²æå– {len(rich_refs)} æ¡MCPå·¥å…·å¼•ç”¨æ•°æ®ï¼ˆç´¯è®¡: {len(self._pending_references[session_id]['rich'])} æ¡ï¼‰ï¼Œå…¨å±€åºå·èŒƒå›´: {all_markers[0] if all_markers else '?'} - {all_markers[-1] if all_markers else '?'}")
                                logger.debug(f"ğŸ” ç´¯ç§¯å…¨å±€åºå·åˆ—è¡¨: {all_markers}")
                        except json.JSONDecodeError:
                            logger.warning(f"âš ï¸ æ— æ³•è§£æå·¥å…·ç»“æœä¸ºJSON: {result_str[:100]}")
                        except Exception as e:
                            logger.error(f"âŒ æå–å¼•ç”¨æ•°æ®å¤±è´¥: {e}", exc_info=True)
                    
                    # ğŸ†• ç‰¹æ®Šæ ‡è®°ï¼šå¦‚æœæ˜¯å›¾è°±æ£€ç´¢å·¥å…·ï¼Œè®°å½•ä¼šè¯IDï¼ˆç¨åä»Redisæå–å¯è§†åŒ–æ•°æ®ï¼‰
                    if tool_name in ["graph_search_knowledge", "flexible_graph_query"]:
                        # åˆå§‹åŒ–å›¾è°±æ£€ç´¢æ ‡è®°å­˜å‚¨
                        if not hasattr(self, '_pending_graph_sessions'):
                            self._pending_graph_sessions = set()
                        self._pending_graph_sessions.add(session_id)
                        logger.info(f"ğŸ¨ å›¾è°±æ£€ç´¢å·¥å…· [{tool_name}] å·²æ‰§è¡Œï¼Œæ ‡è®°ä¼šè¯: {session_id}ï¼ˆå¯è§†åŒ–æ•°æ®å°†ä»Redisæå–ï¼‰")
                    
                    # ğŸ¯ å‘é€å·¥å…·æˆåŠŸçŠ¶æ€
                    await self.send_tool_status(
                        session_id=session_id,
                        tool_name=tool_name,
                        status="success"
                    )
                    
                    return {
                        "role": "tool",
                        "tool_call_id": tool_call_id,
                        "name": tool_name,
                        "content": result_str
                    }
                    
                except Exception as e:
                    duration = time.time() - start_time
                    
                    if tool_config.verbose_logging:
                        logger.error(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥ {tool_name}: {e} (è€—æ—¶: {duration:.2f}ç§’)")
                    else:
                        logger.error(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥ {tool_name}: {e}")
                    
                    # ğŸ¯ è®°å½•å¤±è´¥ç»Ÿè®¡
                    self._record_tool_call(session_id, tool_name, False, duration)
                    
                    # ğŸ¯ å‘é€å·¥å…·å¤±è´¥çŠ¶æ€
                    await self.send_tool_status(
                        session_id=session_id,
                        tool_name=tool_name,
                        status="error",
                        error=str(e)
                    )
                    
                    # ğŸ¯ æ£€æŸ¥æ˜¯å¦å…è®¸å¤±è´¥åç»§ç»­
                    if not tool_config.allow_continue_on_error:
                        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä¸­æ–­æ•´ä¸ªå·¥å…·è°ƒç”¨æµç¨‹
                    
                    return {
                        "role": "tool",
                        "tool_call_id": tool_call_id,
                        "name": tool_name,
                        "content": f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
                    }
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·
        tasks = [execute_single_tool(tool_call) for tool_call in tool_calls]
        results = await asyncio.gather(*tasks)
        
        # ğŸ¯ å»é‡ï¼šå¦‚æœæœ¬è½®æœ‰å¤šä¸ª search_knowledge_base è°ƒç”¨ï¼Œå¯¹ç»“æœè¿›è¡Œå»é‡
        await self._deduplicate_knowledge_base_results(session_id, tool_calls)
        
        # ğŸ¯ ã€å…³é”®ä¿®å¤ã€‘å»é‡åï¼Œéœ€è¦æ›´æ–°æ‰€æœ‰ search_knowledge_base çš„ tool æ¶ˆæ¯å†…å®¹
        # ç­–ç•¥ï¼šæ¯ä¸ª tool æ¶ˆæ¯éƒ½è¿”å›ç›¸åŒçš„å»é‡æ•°æ®ï¼ˆæ»¡è¶³ OpenAI API è¦æ±‚æ¯ä¸ª tool_call éƒ½æœ‰å“åº”ï¼‰
        if hasattr(self, '_pending_references') and session_id in self._pending_references:
            refs_data = self._pending_references[session_id]
            rich_refs = refs_data.get("rich", [])
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ search_knowledge_base è°ƒç”¨
            kb_search_indices = [i for i, r in enumerate(results) if r.get("name") == "search_knowledge_base"]
            
            # ğŸ¯ ã€å…³é”®ä¿®å¤ã€‘æ— è®ºæœ‰å‡ ä¸ªè°ƒç”¨ï¼Œéƒ½è¦æ›´æ–°toolæ¶ˆæ¯ä¸ºå»é‡åçš„ç»“æœï¼
            if len(kb_search_indices) >= 1 and rich_refs:
                logger.info(f"ğŸ”„ æ£€æµ‹åˆ° {len(kb_search_indices)} ä¸ª search_knowledge_base è°ƒç”¨ï¼Œæ›´æ–°toolæ¶ˆæ¯ä¸ºå»é‡æ’åºåçš„ç»“æœï¼ˆå»é‡å: {len(rich_refs)} æ¡ï¼‰")
                
                # ğŸ¯ æ„å»ºåˆå¹¶åçš„å»é‡ç»“æœ
                deduped_results = []
                all_queries = []
                
                # ğŸ” è°ƒè¯•ï¼šæ‰“å°å»é‡æ’åºåçš„åºå·
                logger.info(f"ğŸ” å»é‡æ’åºåçš„ref_markeré¡ºåº: {[r.get('ref_marker') for r in rich_refs]}")
                
                for idx, rich_ref in enumerate(rich_refs, start=1):
                    deduped_results.append({
                        "index": idx,
                        "ref_marker": rich_ref.get("ref_marker", idx),
                        "content": rich_ref.get("content", ""),
                        "score": rich_ref.get("score", 0.0),
                        "metadata": {
                            "source": rich_ref.get("source", ""),
                            "chunk_index": rich_ref.get("chunk_index", 0),
                            "chunk_id": rich_ref.get("chunk_id", ""),
                            "document_id": rich_ref.get("doc_id", ""),
                            "doc_id": rich_ref.get("doc_id", ""),
                            "kb_id": rich_ref.get("kb_id", ""),
                            "filename": rich_ref.get("filename", "")
                        }
                    })
                
                # æ”¶é›†æ‰€æœ‰æŸ¥è¯¢
                for i in kb_search_indices:
                    try:
                        original_data = json.loads(results[i]["content"])
                        query = original_data.get("query", "")
                        if query:
                            all_queries.append(query)
                    except:
                        pass
                
                # æ„å»ºåˆå¹¶åçš„ tool æ¶ˆæ¯å†…å®¹ï¼ˆæ‰€æœ‰ tool æ¶ˆæ¯éƒ½è¿”å›ç›¸åŒçš„å»é‡æ•°æ®ï¼‰
                merged_content = json.dumps({
                    "success": True,
                    "query": " | ".join(all_queries) if all_queries else "å¤šæ¬¡æ£€ç´¢ï¼ˆå·²åˆå¹¶å»é‡ï¼‰",
                    "total": len(deduped_results),
                    "results": deduped_results
                }, ensure_ascii=False)
                
                # ğŸ¯ æ›´æ–°æ‰€æœ‰ search_knowledge_base æ¶ˆæ¯ä¸ºå»é‡æ’åºåçš„å†…å®¹
                # æ³¨æ„ï¼šä¸èƒ½åˆ é™¤æ¶ˆæ¯ï¼Œå› ä¸º OpenAI API è¦æ±‚æ¯ä¸ª tool_call éƒ½æœ‰å¯¹åº”çš„å“åº”
                # ã€å…³é”®ã€‘è¿™æ ·å¯ä»¥ç¡®ä¿æ¨¡å‹çœ‹åˆ°çš„å¼•ç”¨åºå·å’Œå‰ç«¯æ”¶åˆ°çš„ä¸€è‡´ï¼
                for i in kb_search_indices:
                    results[i]["content"] = merged_content
                
                logger.info(f"âœ… å·²æ›´æ–° {len(kb_search_indices)} ä¸ª search_knowledge_base å·¥å…·æ¶ˆæ¯ä¸ºå»é‡æ’åºåçš„ç»“æœï¼ˆå»é‡å: {len(deduped_results)} æ¡ï¼Œæ¨¡å‹çœ‹åˆ°çš„åºå·å°†ä¸å‰ç«¯ä¸€è‡´ï¼‰")
        
        return results
    
    async def _smart_streaming_output(self, content: str, chunk_size: Optional[int] = None) -> AsyncGenerator[str, None]:
        """
        æ™ºèƒ½æµå¼è¾“å‡º
        
        æŒ‰è¯æ±‡å•ä½è¾“å‡ºï¼Œè€Œä¸æ˜¯é€å­—ç¬¦ï¼Œæå‡ç”¨æˆ·ä½“éªŒ
        """
        
        if not content:
            return
        
        # ä½¿ç”¨é…ç½®çš„åˆ†å—å¤§å°
        if chunk_size is None:
            chunk_size = streaming_config.chunk_size
        
        # å¦‚æœç¦ç”¨æ™ºèƒ½åˆ†å—ï¼Œç›´æ¥è¾“å‡º
        if not streaming_config.enable_smart_chunking:
            yield content
            return
        
        # æŒ‰è¯æ±‡åˆ†å‰²ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
        import re
        
        # åˆ†å‰²ç­–ç•¥ï¼šä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å•è¯ã€æ ‡ç‚¹ç¬¦å·
        tokens = re.findall(r'[\u4e00-\u9fff]|[a-zA-Z]+|\d+|[^\w\s]|\s+', content)
        
        current_chunk = ""
        for token in tokens:
            current_chunk += token
            
            # è®°å½•åˆ†å—å†…å®¹ï¼ˆå¦‚æœå¯ç”¨è°ƒè¯•ï¼‰
            if streaming_config.log_chunk_content:
                logger.debug(f"åˆ†å—ç´¯ç§¯: '{current_chunk}'")
            
            # å½“ç´¯ç§¯åˆ°è¶³å¤Ÿçš„å†…å®¹æ—¶è¾“å‡º
            if len(current_chunk) >= chunk_size or token in ['\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']:
                yield current_chunk
                current_chunk = ""
                # ä½¿ç”¨é…ç½®çš„å»¶è¿Ÿ
                if streaming_config.chunk_delay > 0:
                    await asyncio.sleep(streaming_config.chunk_delay)
        
        # è¾“å‡ºå‰©ä½™å†…å®¹
        if current_chunk:
            yield current_chunk
    
    async def get_session_stats(self) -> Dict[str, Any]:
        """è·å–ä¼šè¯ç»Ÿè®¡ä¿¡æ¯"""
        async with self.session_lock:
            active_count = len(self.active_sessions)
            states = {}
            for session in self.active_sessions.values():
                state = session.state.value
                states[state] = states.get(state, 0) + 1
            
            return {
                "active_sessions": active_count,
                "states": states,
                "timestamp": time.time()
            }


# å…¨å±€æµå¼ç®¡ç†å™¨å®ä¾‹
streaming_manager = UniversalStreamingManager()
