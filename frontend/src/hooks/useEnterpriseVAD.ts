import { useState, useRef, useCallback, useEffect } from 'react';

/**
 * ä¼ä¸šçº§ VAD é…ç½®
 */
interface EnterpriseVADOptions {
  /** è¯­éŸ³å¼€å§‹éŸ³é‡é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œé»˜è®¤ 0.03 */
  speechStartThreshold?: number;
  /** è¯­éŸ³æŒç»­éŸ³é‡é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œé»˜è®¤ 0.02 */
  speechContinueThreshold?: number;
  /** è¯­éŸ³å¼€å§‹å‰éœ€è¦çš„æœ€å°æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œé˜²æ­¢è¯¯è§¦å‘ï¼Œé»˜è®¤ 200ms */
  minSpeechDuration?: number;
  /** é™éŸ³æŒç»­æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰ï¼Œè¶…è¿‡æ­¤æ—¶é•¿è‡ªåŠ¨åœæ­¢ï¼Œé»˜è®¤ 1500ms */
  silenceDuration?: number;
  /** æœ€å¤§å½•éŸ³æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰ï¼Œè¶…è¿‡è‡ªåŠ¨åœæ­¢ï¼Œé»˜è®¤ 60 ç§’ */
  maxRecordingDuration?: number;
  /** æ˜¯å¦å¯ç”¨è°ƒè¯•æ—¥å¿— */
  debug?: boolean;
  /** è¯­éŸ³å¼€å§‹å›è°ƒ */
  onSpeechStart?: () => void;
  /** è¯­éŸ³ç»“æŸå›è°ƒ */
  onSpeechEnd?: () => void;
  /** æœ€å¤§å½•éŸ³æ—¶é•¿åˆ°è¾¾å›è°ƒ */
  onMaxDurationReached?: () => void;
}

interface UseEnterpriseVADReturn {
  /** æ˜¯å¦æ­£åœ¨è¯´è¯ */
  isSpeaking: boolean;
  /** å¯åŠ¨ VAD */
  startVAD: (stream: MediaStream) => void;
  /** åœæ­¢ VAD */
  stopVAD: () => void;
  /** å½“å‰éŸ³é‡ï¼ˆ0-1ï¼‰ */
  currentVolume: number;
  /** å½•éŸ³æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ */
  recordingDuration: number;
}

/**
 * ä¼ä¸šçº§ VAD Hook
 * 
 * ç‰¹æ€§ï¼š
 * - åŒé˜ˆå€¼æ£€æµ‹ï¼šå¯åŠ¨é˜ˆå€¼ + æŒç»­é˜ˆå€¼
 * - é˜²è¯¯è§¦å‘ï¼šéœ€è¦æŒç»­ä¸€å®šæ—¶é—´æ‰è®¤ä¸ºæ˜¯è¯­éŸ³
 * - æ™ºèƒ½é™éŸ³æ£€æµ‹ï¼šæŒç»­é™éŸ³æ‰åœæ­¢
 * - æœ€å¤§æ—¶é•¿é™åˆ¶ï¼šé˜²æ­¢å½•éŸ³è¿‡é•¿
 * - å®Œå–„çš„çŠ¶æ€ç®¡ç†ï¼šä½¿ç”¨ ref é¿å…é—­åŒ…é™·é˜±
 */
export const useEnterpriseVAD = (options: EnterpriseVADOptions = {}): UseEnterpriseVADReturn => {
  const {
    speechStartThreshold = 0.03,
    speechContinueThreshold = 0.02,
    minSpeechDuration = 200,
    silenceDuration = 1500,
    maxRecordingDuration = Infinity, // é»˜è®¤ä¸é™åˆ¶å½•éŸ³æ—¶é•¿
    debug = false,
    onSpeechStart,
    onSpeechEnd,
    onMaxDurationReached,
  } = options;

  const [isSpeaking, setIsSpeaking] = useState(false);
  const [currentVolume, setCurrentVolume] = useState(0);
  const [recordingDuration, setRecordingDuration] = useState(0);

  // ä½¿ç”¨ ref å­˜å‚¨çŠ¶æ€ï¼Œé¿å…é—­åŒ…é—®é¢˜
  const isSpeakingRef = useRef(false);
  const currentVolumeRef = useRef(0);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const streamRef = useRef<MediaStream | null>(null);

  // è®¡æ—¶å™¨
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const speechStartTimerRef = useRef<NodeJS.Timeout | null>(null);
  const maxDurationTimerRef = useRef<NodeJS.Timeout | null>(null);
  const recordingStartTimeRef = useRef<number | null>(null);
  const durationIntervalRef = useRef<NodeJS.Timeout | null>(null);

  // éŸ³é‡å†å²è®°å½•ï¼ˆç”¨äºæ›´å‡†ç¡®çš„åˆ¤æ–­ï¼‰
  const volumeHistoryRef = useRef<number[]>([]);
  const HISTORY_SIZE = 5;

  const log = (...args: any[]) => {
    if (debug) {
      console.log('[EnterpriseVAD]', ...args);
    }
  };

  /**
   * è®¡ç®—éŸ³é‡çš„ç§»åŠ¨å¹³å‡
   */
  const getAverageVolume = useCallback((history: number[]): number => {
    if (history.length === 0) return 0;
    return history.reduce((sum, v) => sum + v, 0) / history.length;
  }, []);

  /**
   * å¯åŠ¨ VAD
   */
  const startVAD = useCallback((stream: MediaStream) => {
    log('ğŸš€ å¯åŠ¨ä¼ä¸šçº§ VAD');
    log('ğŸ“Š é…ç½®:', {
      speechStartThreshold,
      speechContinueThreshold,
      minSpeechDuration,
      silenceDuration,
      maxRecordingDuration,
    });

    // ä¿å­˜ stream å¼•ç”¨
    streamRef.current = stream;
    recordingStartTimeRef.current = Date.now();

    // é‡ç½®çŠ¶æ€
    isSpeakingRef.current = false;
    setIsSpeaking(false);
    volumeHistoryRef.current = [];
    setRecordingDuration(0);

    // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡
    const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
    audioContextRef.current = audioContext;

    // åˆ›å»ºåˆ†æå™¨èŠ‚ç‚¹
    const analyser = audioContext.createAnalyser();
    analyser.fftSize = 2048;
    analyser.smoothingTimeConstant = 0.8;
    analyserRef.current = analyser;

    // è¿æ¥éŸ³é¢‘æº
    const source = audioContext.createMediaStreamSource(stream);
    source.connect(analyser);

    // å¼€å§‹æ›´æ–°å½•éŸ³æ—¶é•¿
    durationIntervalRef.current = setInterval(() => {
      if (recordingStartTimeRef.current) {
        const duration = Date.now() - recordingStartTimeRef.current;
        setRecordingDuration(duration);
      }
    }, 100);

    // è®¾ç½®æœ€å¤§æ—¶é•¿è®¡æ—¶å™¨ï¼ˆä»…åœ¨æœ‰é™æ—¶é•¿æ—¶è®¾ç½®ï¼‰
    if (maxRecordingDuration !== Infinity && maxRecordingDuration > 0) {
      maxDurationTimerRef.current = setTimeout(() => {
        log('â° è¾¾åˆ°æœ€å¤§å½•éŸ³æ—¶é•¿:', maxRecordingDuration, 'ms');
        onMaxDurationReached?.();
        onSpeechEnd?.();
      }, maxRecordingDuration);
    }

    // å¼€å§‹æ£€æµ‹éŸ³é‡
    const dataArray = new Uint8Array(analyser.frequencyBinCount);

    const detectVolume = () => {
      if (!analyserRef.current) return;

      analyser.getByteFrequencyData(dataArray);

      // è®¡ç®—å¹³å‡éŸ³é‡
      const average = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length;
      const volume = average / 255; // å½’ä¸€åŒ–åˆ° 0-1

      // æ›´æ–°éŸ³é‡çŠ¶æ€
      currentVolumeRef.current = volume;
      setCurrentVolume(volume);

      // æ›´æ–°éŸ³é‡å†å²
      volumeHistoryRef.current.push(volume);
      if (volumeHistoryRef.current.length > HISTORY_SIZE) {
        volumeHistoryRef.current.shift();
      }

      const avgVolume = getAverageVolume(volumeHistoryRef.current);

      // VAD é€»è¾‘
      if (!isSpeakingRef.current) {
        // å½“å‰æœªåœ¨è¯´è¯çŠ¶æ€
        if (avgVolume > speechStartThreshold) {
          // éŸ³é‡è¶…è¿‡å¯åŠ¨é˜ˆå€¼
          if (!speechStartTimerRef.current) {
            log('ğŸ¤ æ£€æµ‹åˆ°å¯èƒ½çš„è¯­éŸ³ï¼Œå¼€å§‹éªŒè¯... (éŸ³é‡:', avgVolume.toFixed(3), ')');

            // å¯åŠ¨è¯­éŸ³å¼€å§‹éªŒè¯è®¡æ—¶å™¨
            speechStartTimerRef.current = setTimeout(() => {
              // å†æ¬¡æ£€æŸ¥éŸ³é‡ï¼Œç¡®ä¿ä¸æ˜¯ç¬é—´å™ªéŸ³
              const currentAvgVolume = getAverageVolume(volumeHistoryRef.current);
              if (currentAvgVolume > speechStartThreshold) {
                log('âœ… ç¡®è®¤è¯­éŸ³å¼€å§‹ (å¹³å‡éŸ³é‡:', currentAvgVolume.toFixed(3), ')');
                isSpeakingRef.current = true;
                setIsSpeaking(true);
                onSpeechStart?.();
              } else {
                log('âŒ è¯¯åˆ¤ä¸ºå™ªéŸ³ï¼Œå–æ¶ˆè¯­éŸ³å¼€å§‹ (å¹³å‡éŸ³é‡:', currentAvgVolume.toFixed(3), ')');
              }
              speechStartTimerRef.current = null;
            }, minSpeechDuration);
          }
        } else {
          // éŸ³é‡ä½äºå¯åŠ¨é˜ˆå€¼ï¼Œæ¸…é™¤éªŒè¯è®¡æ—¶å™¨
          if (speechStartTimerRef.current) {
            log('ğŸ”‡ éŸ³é‡ä¸‹é™ï¼Œå–æ¶ˆè¯­éŸ³éªŒè¯ (éŸ³é‡:', avgVolume.toFixed(3), ')');
            clearTimeout(speechStartTimerRef.current);
            speechStartTimerRef.current = null;
          }
        }
      } else {
        // å½“å‰æ­£åœ¨è¯´è¯çŠ¶æ€
        if (avgVolume > speechContinueThreshold) {
          // éŸ³é‡è¶…è¿‡æŒç»­é˜ˆå€¼ï¼Œç»§ç»­è¯´è¯
          if (silenceTimerRef.current) {
            log('ğŸ”Š æ£€æµ‹åˆ°è¯­éŸ³ï¼Œå–æ¶ˆé™éŸ³è®¡æ—¶å™¨ (éŸ³é‡:', avgVolume.toFixed(3), ')');
            clearTimeout(silenceTimerRef.current);
            silenceTimerRef.current = null;
          }
        } else {
          // éŸ³é‡ä½äºæŒç»­é˜ˆå€¼ï¼Œæ£€æµ‹é™éŸ³
          if (!silenceTimerRef.current) {
            log('ğŸ”‡ æ£€æµ‹åˆ°é™éŸ³ï¼Œå¼€å§‹è®¡æ—¶... (éŸ³é‡:', avgVolume.toFixed(3), ')');

            // å¯åŠ¨é™éŸ³è®¡æ—¶å™¨
            silenceTimerRef.current = setTimeout(() => {
              log('â±ï¸ é™éŸ³è¶…è¿‡', silenceDuration, 'msï¼Œè§¦å‘è¯­éŸ³ç»“æŸ');
              isSpeakingRef.current = false;
              setIsSpeaking(false);
              onSpeechEnd?.();
              silenceTimerRef.current = null;
            }, silenceDuration);
          }
        }
      }

      // ç»§ç»­æ£€æµ‹
      animationFrameRef.current = requestAnimationFrame(detectVolume);
    };

    detectVolume();
    log('âœ… VAD å·²å¯åŠ¨');
  }, [
    speechStartThreshold,
    speechContinueThreshold,
    minSpeechDuration,
    silenceDuration,
    maxRecordingDuration,
    onSpeechStart,
    onSpeechEnd,
    onMaxDurationReached,
    getAverageVolume,
  ]);

  /**
   * åœæ­¢ VAD
   */
  const stopVAD = useCallback(() => {
    log('ğŸ›‘ åœæ­¢ VAD');

    // åœæ­¢éŸ³é‡æ£€æµ‹
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }

    // æ¸…é™¤æ‰€æœ‰è®¡æ—¶å™¨
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }

    if (speechStartTimerRef.current) {
      clearTimeout(speechStartTimerRef.current);
      speechStartTimerRef.current = null;
    }

    if (maxDurationTimerRef.current) {
      clearTimeout(maxDurationTimerRef.current);
      maxDurationTimerRef.current = null;
    }

    if (durationIntervalRef.current) {
      clearInterval(durationIntervalRef.current);
      durationIntervalRef.current = null;
    }

    // å…³é—­éŸ³é¢‘ä¸Šä¸‹æ–‡
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    analyserRef.current = null;
    streamRef.current = null;
    isSpeakingRef.current = false;
    recordingStartTimeRef.current = null;
    volumeHistoryRef.current = [];
    
    setIsSpeaking(false);
    setCurrentVolume(0);
    setRecordingDuration(0);

    log('âœ… VAD å·²åœæ­¢');
  }, []);

  // ç»„ä»¶å¸è½½æ—¶æ¸…ç†
  useEffect(() => {
    return () => {
      stopVAD();
    };
  }, [stopVAD]);

  return {
    isSpeaking,
    startVAD,
    stopVAD,
    currentVolume,
    recordingDuration,
  };
};

